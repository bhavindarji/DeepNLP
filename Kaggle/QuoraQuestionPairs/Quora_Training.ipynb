{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras._impl.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras._impl.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras._impl.keras.utils.data_utils import get_file\n",
    "\n",
    "# from tensorflow.python.keras._impl.keras.preprocessing import sequence\n",
    "# from tensorflow.python.keras._impl.keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, BatchNormalization\n",
    "# from tensorflow.python.keras._impl.keras.layers import Reshape, Flatten, Dropout, Concatenate, dot, add\n",
    "\n",
    "# from tensorflow.python.keras._impl.keras.optimizers import Adam\n",
    "# from tensorflow.python.keras._impl.keras.models import Model\n",
    "# from tensorflow.python.keras._impl.keras.layers import LSTM\n",
    "# from tensorflow.python import keras\n",
    "# from tensorflow.python.keras._impl.keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial global var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
    "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
    "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
    "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'\n",
    "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
    "MODEL_WEIGHTS_FILE = 'question_pairs_weights.h5'\n",
    "SENT_EMB_DIM = 300\n",
    "MAX_SEQ_LEN = 25\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447\n",
    "NB_EPOCHS = 25\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 32\n",
    "DATA_PATH = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_data = np.load(open(DATA_PATH + Q1_TRAINING_DATA_FILE, 'rb'))\n",
    "q2_data = np.load(open(DATA_PATH + Q2_TRAINING_DATA_FILE, 'rb'))\n",
    "labels = np.load(open(DATA_PATH + LABEL_TRAINING_DATA_FILE, 'rb'))\n",
    "word_embedding_matrix = np.load(open(DATA_PATH+WORD_EMBEDDING_MATRIX_FILE, 'rb'))\n",
    "with open(DATA_PATH+NB_WORDS_DATA_FILE, 'r') as f:\n",
    "    nb_words = json.load(f)['nb_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange(base, hypothesis, label):\n",
    "    features = {\"base\": base, \"hypothesis\": hypothesis}\n",
    "    return features, label\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((Q1_train, Q2_train, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.batch(16)\n",
    "    dataset = dataset.map(rearrange)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def test_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((Q1_test, Q2_test, y_test))\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.batch(16)\n",
    "    dataset = dataset.map(rearrange)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363858, 25)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def gen_dataset():\n",
    "    up = [i for i in range(10)]\n",
    "    down = [9-i for i in range(10)]\n",
    "    \n",
    "    with open(\"./data/test.csv\", 'w') as f:\n",
    "        writer = csv.writer(f, delimiter=\",\")\n",
    "        for i in range(1000):\n",
    "            writer.writerow([1] + up)\n",
    "            writer.writerow([0] + down)\n",
    "            \n",
    "gen_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator 받기\n",
    "def gen():\n",
    "    for i, j in zip(range(10, 1100), range(1000)):\n",
    "        yield (i, j)\n",
    "\n",
    "dataset=tf.data.Dataset.from_generator(gen, (tf.float32, tf.float32))\\\n",
    "        .shuffle(100)\\\n",
    "        .batch(20)\\\n",
    "        .make_one_shot_iterator()\\\n",
    "        .get_next() #얼마만큼 buffer에 넣고, 섞을 것인가: 통 사이즈\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(3):\n",
    "        _label, _feat = sess.run(dataset)\n",
    "        print(_label, _feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.TFRecordDataset()\n",
    "dataset=tf.data.TextLineDataset('./data/test.csv')\\\n",
    "        .batch(20)\\\n",
    "        .make_one_shot_iterator()\\\n",
    "        .get_next() #얼마만큼 buffer에 넣고, 섞을 것인가: 통 사이즈, batch는 링크 증가\n",
    "\n",
    "lines = tf.decode_csv(dataset, record_defaults=[[0]]*11) #record_defaults는 missing value가 있을때\n",
    "features = tf.stack(lines[1:], axis = 1) #column방향으로 묶어야함\n",
    "label = lines[0]\n",
    "# feature = tf.stack(lines[1:], axis=1) #column에 대해서 합쳐주는 기능\n",
    "# label = lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10) (20,)\n",
      "(20, 10) (20,)\n",
      "(20, 10) (20,)\n",
      "(20, 10) (20,)\n",
      "(20, 10) (20,)\n",
      "(20, 10) (20,)\n",
      "(20, 10) (20,)\n",
      "(20, 10) (20,)\n",
      "(20, 10) (20,)\n",
      "(20, 10) (20,)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    for i in range(10):\n",
    "        _feat, _lab = sess.run([features, label])\n",
    "        print(_feat.shape, _lab.shape)\n",
    "#     for f, l in zip(_feat, _lab):\n",
    "#         print(l,f)    \n",
    "#     print(_feat, _lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.TFRecordDataset()\n",
    "dataset=tf.data.TextLineDataset('./data/test.csv')\\\n",
    "        .repeat()\\\n",
    "        .batch(20)\\\n",
    "        .make_one_shot_iterator()\\\n",
    "        .get_next() #얼마만큼 buffer에 넣고, 섞을 것인가: 통 사이즈, batch는 링크 증가\n",
    "\n",
    "lines = tf.decode_csv(dataset, record_defaults=[[0]]*11) #record_defaults는 missing value가 있을때\n",
    "features = tf.stack(lines[1:], axis = 1) #column방향으로 묶어야함\n",
    "label = tf.expand_dims(lines[0], axis = -1)\n",
    "\n",
    "label = tf.cast(label, tf.float32)\n",
    "features = tf.cast(features, tf.float32)\n",
    "\n",
    "# feature = tf.stack(lines[1:], xis=1) #column에 대해서 합쳐주는 기능\n",
    "# label = lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = tf.layers.dense(features, units=10, activation=tf.nn.relu)\n",
    "layer2 = tf.layers.dense(layer1, units=10, activation=tf.nn.relu)\n",
    "layer3 = tf.layers.dense(layer2, units=10, activation=tf.nn.relu)\n",
    "out = tf.layers.dense(layer3, units=1)\n",
    "\n",
    "loss = tf.losses.sigmoid_cross_entropy(label, out)\n",
    "train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)\n",
    "\n",
    "pred = tf.nn.sigmoid(out)\n",
    "accuracy = tf.metrics.accuracy(label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    for i in range(5000):\n",
    "        _, _loss, _acc = sess.run([train_op, loss, accuracy])\n",
    "        print('step: {}, loss: {}, accuracy: {}'.format(i, _loss, _acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  5417,    29, 67434],\n",
       "       [    0,     0,     0, ...,   100,   138,   140],\n",
       "       [    0,     0,     0, ...,    66,    68,   631],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,    72,    47,   692],\n",
       "       [    0,     0,     0, ...,   152,    10,    35],\n",
       "       [    0,     0,     0, ...,    42,   102,    39]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def RNN_attention(features, label, mode):\n",
    "    \n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    \n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        loss = tf.losses.mean_squared_error(labels, logit_layer)\n",
    "        train_op = tf.train.AdamOptimizer(1e-4).minimize(loss, global_step)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op)\n",
    "    \n",
    "      \n",
    "    \n",
    "    elif EVAL:\n",
    "        pred = tf.nn.sigmoid(logit_layer)\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc': accuracy})\n",
    "        \n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    est = tf.estimator.Estimator(RNN_attention)\n",
    "    est.train(input_fn, steps=None)\n",
    "    est.evalate(input_fn, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.MAX_SEQ_LEN = MAX_SEQ_LEN\n",
    "        \n",
    "        self.q1_input = Input(shape=(MAX_SEQ_LEN, ))\n",
    "        self.q2_input = Input(shape=(MAX_SEQ_LEN, ))\n",
    "        \n",
    "        self.embedding = Embedding(nb_words + 1,\n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix],\n",
    "                 input_length=MAX_SEQ_LEN, \n",
    "                 trainable=False)        \n",
    "        \n",
    "        self.Bidirectional = Bidirectional(LSTM(SENT_EMB_DIM, return_sequences=True), merge_mode='sum')\n",
    "                \n",
    "    def predict(self):\n",
    "        \n",
    "        q1_input = self.q1_input      \n",
    "        q1_encoded = self.embedding(q1_input)\n",
    "        q1_bi = self.Bidirectional(q1_encoded)\n",
    "        \n",
    "        q2_input = self.q2_input\n",
    "        q2_encoded = self.embedding(q2_input)\n",
    "        q2_bi = self.Bidirectional(q2_encoded)\n",
    "                \n",
    "        attention = dot([q1_encoded, q2_encoded], [1,1])\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Dense((MAX_SEQ_LEN * SENT_EMB_DIM))(attention)\n",
    "        attention = Reshape((MAX_SEQUENCE_LENGTH, SENT_EMB_DIM))(attention)\n",
    "        \n",
    "        print(attention)\n",
    "        \n",
    "        merged = add([q1_encoded,attention])\n",
    "        merged = Flatten()(merged)\n",
    "        merged = Dense(200, activation='relu')(merged)\n",
    "        merged = Dropout(DROPOUT)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "        \n",
    "        is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "        \n",
    "        model = Model(inputs=[self.q1_input,self.q2_input], outputs=is_duplicate)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = Attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"reshape_4/Reshape:0\", shape=(?, 25, 300), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_30 (InputLayer)           (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_31 (InputLayer)           (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 25, 300)      28678800    input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 300, 300)     0           embedding_14[0][0]               \n",
      "                                                                 embedding_14[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 90000)        0           dot_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 7500)         675007500   flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 25, 300)      0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 25, 300)      0           embedding_14[0][0]               \n",
      "                                                                 reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 7500)         0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 200)          1500200     flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200)          800         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            201         batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 705,187,501\n",
      "Trainable params: 676,508,301\n",
      "Non-trainable params: 28,679,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = att.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363858, 25)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-def1f9d00720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mq1_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mq1_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "q1_input = Input(shape=(MAX_SEQ_LEN, ))\n",
    "q1_input(Q1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "printprint((\"Starting training at\"\"Starti , datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=NB_EPOCHS,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    verbose=2,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/bradleypallen/keras-quora-question-pairs/blob/master/quora-question-pairs-data-prep.ipynb\n",
    "https://github.com/Smerity/keras_snli\n",
    "https://github.com/bradleypallen/keras-quora-question-pairs/blob/master/quora-question-pairs-training-attention.ipynb\n",
    "https://github.com/bradleypallen/keras-quora-question-pairs/blob/master/quora-question-pairs-training.ipynb\n",
    "http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
