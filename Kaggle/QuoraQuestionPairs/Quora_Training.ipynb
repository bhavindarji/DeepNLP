{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras._impl.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras._impl.keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, BatchNormalization\n",
    "from tensorflow.python.keras._impl.keras.layers import Reshape, Flatten, Dropout, Concatenate, dot, add\n",
    "\n",
    "from tensorflow.python.keras._impl.keras.optimizers import Adam\n",
    "from tensorflow.python.keras._impl.keras.models import Model\n",
    "from tensorflow.python.keras._impl.keras.layers import LSTM\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras._impl.keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras._impl.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras._impl.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras._impl.keras.utils.data_utils import get_file\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial global var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
    "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
    "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
    "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'\n",
    "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
    "MODEL_WEIGHTS_FILE = 'question_pairs_weights.h5'\n",
    "SENT_EMB_DIM = 300\n",
    "MAX_SEQ_LEN = 25\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447\n",
    "NB_EPOCHS = 25\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 32\n",
    "DATA_PATH = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_data = np.load(open(DATA_PATH + Q1_TRAINING_DATA_FILE, 'rb'))\n",
    "q2_data = np.load(open(DATA_PATH + Q2_TRAINING_DATA_FILE, 'rb'))\n",
    "labels = np.load(open(DATA_PATH + LABEL_TRAINING_DATA_FILE, 'rb'))\n",
    "word_embedding_matrix = np.load(open(DATA_PATH+WORD_EMBEDDING_MATRIX_FILE, 'rb'))\n",
    "with open(DATA_PATH+NB_WORDS_DATA_FILE, 'r') as f:\n",
    "    nb_words = json.load(f)['nb_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.MAX_SEQ_LEN = MAX_SEQ_LEN\n",
    "        \n",
    "        self.q1_input = Input(shape=(MAX_SEQ_LEN, ))\n",
    "        self.q2_input = Input(shape=(MAX_SEQ_LEN, ))\n",
    "        \n",
    "        self.embedding = Embedding(nb_words + 1,\n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix],\n",
    "                 input_length=MAX_SEQ_LEN, \n",
    "                 trainable=False)        \n",
    "        \n",
    "        self.Bidirectional = Bidirectional(LSTM(SENT_EMB_DIM, return_sequences=True), merge_mode='sum')\n",
    "                \n",
    "    def predict(self):\n",
    "        \n",
    "        q1_input = self.q1_input      \n",
    "        q1_encoded = self.embedding(q1_input)\n",
    "        q1_bi = self.Bidirectional(q1_encoded)\n",
    "        \n",
    "        q2_input = self.q2_input\n",
    "        q2_encoded = self.embedding(q2_input)\n",
    "        q2_bi = self.Bidirectional(q2_encoded)\n",
    "                \n",
    "        attention = dot([q1_encoded, q2_encoded], [1,1])\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Dense((MAX_SEQ_LEN * SENT_EMB_DIM))(attention)\n",
    "        attention = Reshape((MAX_SEQUENCE_LENGTH, SENT_EMB_DIM))(attention)\n",
    "        \n",
    "        print(attention)\n",
    "        \n",
    "        merged = add([q1_encoded,attention])\n",
    "        merged = Flatten()(merged)\n",
    "        merged = Dense(200, activation='relu')(merged)\n",
    "        merged = Dropout(DROPOUT)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "        \n",
    "        is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "        \n",
    "        model = Model(inputs=[self.q1_input,self.q2_input], outputs=is_duplicate)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = Attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"reshape_4/Reshape:0\", shape=(?, 25, 300), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_30 (InputLayer)           (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_31 (InputLayer)           (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 25, 300)      28678800    input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 300, 300)     0           embedding_14[0][0]               \n",
      "                                                                 embedding_14[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 90000)        0           dot_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 7500)         675007500   flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 25, 300)      0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 25, 300)      0           embedding_14[0][0]               \n",
      "                                                                 reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 7500)         0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 200)          1500200     flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200)          800         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            201         batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 705,187,501\n",
      "Trainable params: 676,508,301\n",
      "Non-trainable params: 28,679,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = att.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363858, 25)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-def1f9d00720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mq1_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mq1_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ1_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "q1_input = Input(shape=(MAX_SEQ_LEN, ))\n",
    "q1_input(Q1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "printprint((\"Starting training at\"\"Starti , datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=NB_EPOCHS,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    verbose=2,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/bradleypallen/keras-quora-question-pairs/blob/master/quora-question-pairs-data-prep.ipynb\n",
    "https://github.com/Smerity/keras_snli\n",
    "https://github.com/bradleypallen/keras-quora-question-pairs/blob/master/quora-question-pairs-training-attention.ipynb\n",
    "https://github.com/bradleypallen/keras-quora-question-pairs/blob/master/quora-question-pairs-training.ipynb\n",
    "http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
