{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Chatbot_data' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git clone https://github.com/songys/Chatbot_data.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('Chatbot_data/ChatbotData .csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, answer = list(data_df['Q']), list(data_df['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphTokenizer(object):\n",
    "    def __init__(self, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "        self.word_index = {'<pad>': 0, '<unk>': 1, '<eos>': 2, '<sos>': 3}\n",
    "        self.index_word = {0: '<pad>', 1: '<unk>', 2: '<eos>', 3: '<sos>'}\n",
    "        self.indexer = len(self.word_index)\n",
    "        \n",
    "        self.filters = filters\n",
    "        \n",
    "    def fit_on_texts(self, text_dataset):\n",
    "        from konlpy.tag import Twitter\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        morph_analyzer = Twitter()\n",
    "\n",
    "        for text in tqdm(text_dataset):\n",
    "            for c in self.filters:\n",
    "                text = text.replace(c, '')\n",
    "            \n",
    "            morph_tokens = morph_analyzer.morphs(text.replace(' ', ''))\n",
    "\n",
    "            for t in morph_tokens:\n",
    "                if not t in self.word_index:\n",
    "                    self.word_index[t] = self.indexer\n",
    "                    self.index_word[self.indexer] = t\n",
    "                    self.indexer += 1\n",
    "\n",
    "    def texts_to_sequences(self, text_dataset):\n",
    "        from konlpy.tag import Twitter\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        morph_analyzer = Twitter()\n",
    "    \n",
    "        indicied_dataset = list()        \n",
    "        for text in tqdm(text_dataset):\n",
    "            for c in self.filters:\n",
    "                text = text.replace(c, '')\n",
    "        \n",
    "            morph_tokens = morph_analyzer.morphs(text.replace(' ', ''))\n",
    "            indicied_seq = list()\n",
    "            for t in morph_tokens:        \n",
    "                if t in self.word_index:\n",
    "                    indicied_seq.append(self.word_index[t])\n",
    "                else:\n",
    "                    indicied_seq.append(self.word_index['<unk>'])\n",
    "            \n",
    "            indicied_dataset.append(indicied_seq)    \n",
    "            \n",
    "        return indicied_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MorphTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23646/23646 [00:17<00:00, 1378.13it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(question + answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9805"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9805"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11823/11823 [00:07<00:00, 1532.07it/s]\n"
     ]
    }
   ],
   "source": [
    "question_sequences = tokenizer.texts_to_sequences(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11823/11823 [00:09<00:00, 1247.35it/s]\n"
     ]
    }
   ],
   "source": [
    "answer_seqeunces = tokenizer.texts_to_sequences(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:  31\n",
      "min:  1\n",
      "mean:  6.194028588344752\n",
      "std:  2.881177536849193\n"
     ]
    }
   ],
   "source": [
    "text_token_len_list = [len(s) for s in list(answer_seqeunces + question_sequences)]\n",
    "\n",
    "print(\"max: \", max(text_token_len_list))\n",
    "print(\"min: \", min(text_token_len_list))\n",
    "print(\"mean: \", np.mean(text_token_len_list))\n",
    "print(\"std: \", np.std(text_token_len_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 15\n",
    "\n",
    "prepro_questions = pad_sequences(question_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "prepro_answers = pad_sequences(answer_seqeunces, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   4,    5,    0, ...,    0,    0,    0],\n",
       "       [   6,    7,    8, ...,    0,    0,    0],\n",
       "       [  11,   12,   13, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [7303, 5666, 3808, ...,    0,    0,    0],\n",
       "       [ 219, 1523,   60, ...,    0,    0,    0],\n",
       "       [ 440,  115,  351, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 613,  134,  355, ...,    0,    0,    0],\n",
       "       [ 952, 7304,   19, ...,    0,    0,    0],\n",
       "       [  99,   57, 7243, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [6765,   90,  661, ...,    0,    0,    0],\n",
       "       [ 453, 4858,  205, ...,    0,    0,    0],\n",
       "       [9149, 5227,  351, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR_PATH = './data/'\n",
    "QUESTION_DATA_FILE_NAME = 'question.npy'\n",
    "ANSWER_DATA_FILE_NAME = 'answer.npy'\n",
    "DATA_CONFIGS_FILE_NAME = 'data_configs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(FILE_DIR_PATH):\n",
    "    os.makedirs(FILE_DIR_PATH)\n",
    "\n",
    "np.save(open(FILE_DIR_PATH + QUESTION_DATA_FILE_NAME, 'wb'), prepro_questions)\n",
    "np.save(open(FILE_DIR_PATH + ANSWER_DATA_FILE_NAME, 'wb'), prepro_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prepro_configs = {'vocab': tokenizer.word_index, \n",
    "                       'vocab_index_to_word': tokenizer.index_word, \n",
    "                       'vocab_size': len(tokenizer.word_index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(FILE_DIR_PATH + DATA_CONFIGS_FILE_NAME, 'w') as f:\n",
    "    json.dump(data_prepro_configs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
