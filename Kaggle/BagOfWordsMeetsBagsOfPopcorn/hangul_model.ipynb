{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-2-7b55cee8f2d9>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-7b55cee8f2d9>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    input_layer = tf.contrib.layers.embed_sequence(\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = 74065\n",
    "embedding_size = 128\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "\n",
    "\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "\n",
    "   #embedding layer를 선언합니다.\n",
    "   input_layer = tf.contrib.layers.embed_sequence(\n",
    "                   features['text'],\n",
    "                   vocab_size,\n",
    "                   embedding_size,\n",
    "                   initializer=params['embedding_initializer']\n",
    "                   )\n",
    "   # 현재 모델이 학습모드인지 여부를 확인하는 변수입니다.\n",
    "   training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "   # embedding layer에 대한 output에 대해 dropout을 취합니다.\n",
    "   dropout_emb = tf.layers.dropout(inputs=input_layer,\n",
    "                                  rate=0.2,\n",
    "                                  training=training)\n",
    "\n",
    "   #### CNN 구현체 부분 ####\n",
    "    conv = tf.layers.conv1d(\n",
    "           inputs=dropout_emb,\n",
    "           filters=32,\n",
    "           kernel_size=3,\n",
    "           padding='same',\n",
    "           activation=tf.nn.relu)\n",
    "  \n",
    "   pool = tf.reduce_max(input_tensor=conv, axis=1) #max-pooling layer\n",
    "\n",
    "   hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu) #Fully-connected layer\n",
    "\n",
    "   #####################\n",
    "\n",
    "\n",
    "   dropout_hidden = tf.layers.dropout(inputs=hidden, rate=0.2, training=training)\n",
    "   logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n",
    "  \n",
    "   #prediction 진행 시, None\n",
    "   if labels is not None:\n",
    "       labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "\n",
    "    #최종적으로 학습, 평가, 테스트의 단계로 나누어 활용\n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss = loss)\n",
    "    \n",
    "    elif EVAL:\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "        pred = tf.nn.sigmoid(logits)\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc': accuracy})\n",
    "        \n",
    "    elif PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'prob': tf.nn.sigmoid(logits),\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "params = {'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = 74065\n",
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(FILE_PATH+'nsmc_train_input.npy')\n",
    "label = np.load(FILE_PATH+'nsmc_train_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "   level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_float = train.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-19 10:38:27,336 : INFO : collecting all words and their counts\n",
      "2018-08-19 10:38:27,337 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-19 10:38:27,377 : INFO : PROGRESS: at sentence #10000, processed 100000 words, keeping 9595 word types\n",
      "2018-08-19 10:38:27,421 : INFO : PROGRESS: at sentence #20000, processed 200000 words, keeping 13708 word types\n",
      "2018-08-19 10:38:27,464 : INFO : PROGRESS: at sentence #30000, processed 300000 words, keeping 16720 word types\n",
      "2018-08-19 10:38:27,506 : INFO : PROGRESS: at sentence #40000, processed 400000 words, keeping 19238 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-19 10:38:27,550 : INFO : PROGRESS: at sentence #50000, processed 500000 words, keeping 21436 word types\n",
      "2018-08-19 10:38:27,591 : INFO : PROGRESS: at sentence #60000, processed 600000 words, keeping 23311 word types\n",
      "2018-08-19 10:38:27,638 : INFO : PROGRESS: at sentence #70000, processed 700000 words, keeping 25087 word types\n",
      "2018-08-19 10:38:27,679 : INFO : PROGRESS: at sentence #80000, processed 800000 words, keeping 26661 word types\n",
      "2018-08-19 10:38:27,721 : INFO : PROGRESS: at sentence #90000, processed 900000 words, keeping 28197 word types\n",
      "2018-08-19 10:38:27,760 : INFO : PROGRESS: at sentence #100000, processed 1000000 words, keeping 29673 word types\n",
      "2018-08-19 10:38:27,799 : INFO : PROGRESS: at sentence #110000, processed 1100000 words, keeping 31036 word types\n",
      "2018-08-19 10:38:27,840 : INFO : PROGRESS: at sentence #120000, processed 1200000 words, keeping 32370 word types\n",
      "2018-08-19 10:38:27,881 : INFO : PROGRESS: at sentence #130000, processed 1300000 words, keeping 33529 word types\n",
      "2018-08-19 10:38:27,951 : INFO : PROGRESS: at sentence #140000, processed 1400000 words, keeping 34736 word types\n",
      "2018-08-19 10:38:27,993 : INFO : collected 35838 word types from a corpus of 1499950 raw words and 149995 sentences\n",
      "2018-08-19 10:38:27,994 : INFO : Loading a fresh vocabulary\n",
      "2018-08-19 10:38:28,032 : INFO : min_count=5 retains 10765 unique words (30% of original 35838, drops 25073)\n",
      "2018-08-19 10:38:28,033 : INFO : min_count=5 leaves 1459866 word corpus (97% of original 1499950, drops 40084)\n",
      "2018-08-19 10:38:28,063 : INFO : deleting the raw counts dictionary of 35838 items\n",
      "2018-08-19 10:38:28,065 : INFO : sample=0.001 downsamples 31 most-common words\n",
      "2018-08-19 10:38:28,067 : INFO : downsampling leaves estimated 960043 word corpus (65.8% of prior 1459866)\n",
      "2018-08-19 10:38:28,095 : INFO : estimated required memory for 10765 words and 300 dimensions: 31218500 bytes\n",
      "2018-08-19 10:38:28,096 : INFO : resetting layer weights\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types dtype('<U11') dtype('<U11') dtype('<U11')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f1a57b78f067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m model_min5 = word2vec.Word2Vec(train, workers=num_workers, \n\u001b[1;32m     11\u001b[0m            \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_word_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             window = context, sample = downsampling)\n\u001b[0m",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_train_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             self.train(\n\u001b[1;32m    337\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m             trim_rule=trim_rule, **kwargs)\n\u001b[1;32m    485\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mprepare_weights\u001b[0;34m(self, hs, negative, wv, update, vocabulary)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0;31m# set initial input/projection and hidden weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[0;34m(self, hs, negative, wv)\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m             \u001b[0;31m# construct deterministic seed from word AND seed argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m             \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseeded_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types dtype('<U11') dtype('<U11') dtype('<U11')"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 3          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model_min5 = word2vec.Word2Vec(train, workers=num_workers, \n",
    "           size=num_features, min_count = min_word_count, \n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim-3.4.0-py3.6-win-amd64.egg\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2018-08-18 18:03:11,833 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2018-08-18 18:03:11,849 : INFO : collecting all words and their counts\n",
      "2018-08-18 18:03:11,850 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-18 18:03:11,897 : INFO : PROGRESS: at sentence #10000, processed 116864 words, keeping 12351 word types\n",
      "2018-08-18 18:03:11,942 : INFO : PROGRESS: at sentence #20000, processed 230625 words, keeping 17363 word types\n",
      "2018-08-18 18:03:11,982 : INFO : PROGRESS: at sentence #30000, processed 346781 words, keeping 21041 word types\n",
      "2018-08-18 18:03:12,037 : INFO : PROGRESS: at sentence #40000, processed 463975 words, keeping 24149 word types\n",
      "2018-08-18 18:03:12,088 : INFO : PROGRESS: at sentence #50000, processed 578191 words, keeping 26749 word types\n",
      "2018-08-18 18:03:12,135 : INFO : PROGRESS: at sentence #60000, processed 692060 words, keeping 28971 word types\n",
      "2018-08-18 18:03:12,181 : INFO : PROGRESS: at sentence #70000, processed 805075 words, keeping 31069 word types\n",
      "2018-08-18 18:03:12,228 : INFO : PROGRESS: at sentence #80000, processed 919741 words, keeping 32982 word types\n",
      "2018-08-18 18:03:12,281 : INFO : PROGRESS: at sentence #90000, processed 1035397 words, keeping 34880 word types\n",
      "2018-08-18 18:03:12,317 : INFO : PROGRESS: at sentence #100000, processed 1148906 words, keeping 36536 word types\n",
      "2018-08-18 18:03:12,360 : INFO : PROGRESS: at sentence #110000, processed 1262205 words, keeping 38131 word types\n",
      "2018-08-18 18:03:12,400 : INFO : PROGRESS: at sentence #120000, processed 1377485 words, keeping 39655 word types\n",
      "2018-08-18 18:03:12,442 : INFO : PROGRESS: at sentence #130000, processed 1491817 words, keeping 41052 word types\n",
      "2018-08-18 18:03:12,493 : INFO : PROGRESS: at sentence #140000, processed 1605854 words, keeping 42491 word types\n",
      "2018-08-18 18:03:12,542 : INFO : collected 43756 word types from a corpus of 1719985 raw words and 149995 sentences\n",
      "2018-08-18 18:03:12,543 : INFO : Loading a fresh vocabulary\n",
      "2018-08-18 18:03:12,593 : INFO : min_count=6 retains 12747 unique words (29% of original 43756, drops 31009)\n",
      "2018-08-18 18:03:12,594 : INFO : min_count=6 leaves 1664977 word corpus (96% of original 1719985, drops 55008)\n",
      "2018-08-18 18:03:12,661 : INFO : deleting the raw counts dictionary of 43756 items\n",
      "2018-08-18 18:03:12,664 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2018-08-18 18:03:12,665 : INFO : downsampling leaves estimated 1412624 word corpus (84.8% of prior 1664977)\n",
      "2018-08-18 18:03:12,729 : INFO : estimated required memory for 12747 words and 128 dimensions: 19426428 bytes\n",
      "2018-08-18 18:03:12,729 : INFO : resetting layer weights\n",
      "2018-08-18 18:03:13,015 : INFO : training model with 4 workers on 12747 vocabulary and 128 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2018-08-18 18:03:14,069 : INFO : EPOCH 1 - PROGRESS: at 45.72% examples, 635009 words/s, in_qsize 6, out_qsize 1\n",
      "2018-08-18 18:03:15,107 : INFO : EPOCH 1 - PROGRESS: at 92.32% examples, 633737 words/s, in_qsize 7, out_qsize 1\n",
      "2018-08-18 18:03:15,217 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-18 18:03:15,227 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-18 18:03:15,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-18 18:03:15,258 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-18 18:03:15,260 : INFO : EPOCH - 1 : training on 1719985 raw words (1412548 effective words) took 2.2s, 638853 effective words/s\n",
      "2018-08-18 18:03:16,286 : INFO : EPOCH 2 - PROGRESS: at 43.34% examples, 612120 words/s, in_qsize 7, out_qsize 0\n",
      "2018-08-18 18:03:17,303 : INFO : EPOCH 2 - PROGRESS: at 82.38% examples, 576093 words/s, in_qsize 8, out_qsize 0\n",
      "2018-08-18 18:03:17,632 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-18 18:03:17,647 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-18 18:03:17,652 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-18 18:03:17,661 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-18 18:03:17,663 : INFO : EPOCH - 2 : training on 1719985 raw words (1412763 effective words) took 2.4s, 592755 effective words/s\n",
      "2018-08-18 18:03:18,684 : INFO : EPOCH 3 - PROGRESS: at 42.75% examples, 603736 words/s, in_qsize 6, out_qsize 1\n",
      "2018-08-18 18:03:19,703 : INFO : EPOCH 3 - PROGRESS: at 80.67% examples, 562913 words/s, in_qsize 8, out_qsize 0\n",
      "2018-08-18 18:03:20,105 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-18 18:03:20,110 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-18 18:03:20,121 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-18 18:03:20,123 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-18 18:03:20,124 : INFO : EPOCH - 3 : training on 1719985 raw words (1412438 effective words) took 2.4s, 577310 effective words/s\n",
      "2018-08-18 18:03:21,150 : INFO : EPOCH 4 - PROGRESS: at 45.14% examples, 632792 words/s, in_qsize 7, out_qsize 0\n",
      "2018-08-18 18:03:22,156 : INFO : EPOCH 4 - PROGRESS: at 85.90% examples, 601623 words/s, in_qsize 7, out_qsize 0\n",
      "2018-08-18 18:03:22,454 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-18 18:03:22,488 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-18 18:03:22,491 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-18 18:03:22,499 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-18 18:03:22,501 : INFO : EPOCH - 4 : training on 1719985 raw words (1412665 effective words) took 2.4s, 597920 effective words/s\n",
      "2018-08-18 18:03:23,554 : INFO : EPOCH 5 - PROGRESS: at 43.95% examples, 612364 words/s, in_qsize 7, out_qsize 0\n",
      "2018-08-18 18:03:24,555 : INFO : EPOCH 5 - PROGRESS: at 73.69% examples, 515860 words/s, in_qsize 8, out_qsize 0\n",
      "2018-08-18 18:03:25,352 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-18 18:03:25,358 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-18 18:03:25,377 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-18 18:03:25,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-18 18:03:25,387 : INFO : EPOCH - 5 : training on 1719985 raw words (1413014 effective words) took 2.9s, 495518 effective words/s\n",
      "2018-08-18 18:03:25,387 : INFO : training on a 8599925 raw words (7063428 effective words) took 12.4s, 570973 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# 파라메터값 지정# 파라메터값  \n",
    "num_features = 128 # 문자 벡터 차원 수\n",
    "min_word_count = 6 # 최소 단어 수\n",
    "num_workers = 4 # 병렬 처리 스레드 수\n",
    "context = 3 # 문자열 창 크기\n",
    "downsampling = 1e-3 # 문자 빈도 수 Downsample\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "model = word2vec.Word2Vec(data, \n",
    "                          workers=num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./nsmc/preprocessed_train.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_string = np.array(train, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = {'text': train}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete embedding 0 review\n",
      "Complete embedding 1000 review\n",
      "Complete embedding 2000 review\n",
      "Complete embedding 3000 review\n",
      "Complete embedding 4000 review\n",
      "Complete embedding 5000 review\n",
      "Complete embedding 6000 review\n",
      "Complete embedding 7000 review\n",
      "Complete embedding 8000 review\n",
      "Complete embedding 9000 review\n",
      "Complete embedding 10000 review\n",
      "Complete embedding 11000 review\n",
      "Complete embedding 12000 review\n",
      "Complete embedding 13000 review\n",
      "Complete embedding 14000 review\n",
      "Complete embedding 15000 review\n",
      "Complete embedding 16000 review\n",
      "Complete embedding 17000 review\n",
      "Complete embedding 18000 review\n",
      "Complete embedding 19000 review\n",
      "Complete embedding 20000 review\n",
      "Complete embedding 21000 review\n",
      "Complete embedding 22000 review\n",
      "Complete embedding 23000 review\n",
      "Complete embedding 24000 review\n",
      "Complete embedding 25000 review\n",
      "Complete embedding 26000 review\n",
      "Complete embedding 27000 review\n",
      "Complete embedding 28000 review\n",
      "Complete embedding 29000 review\n",
      "Complete embedding 30000 review\n",
      "Complete embedding 31000 review\n",
      "Complete embedding 32000 review\n",
      "Complete embedding 33000 review\n",
      "Complete embedding 34000 review\n",
      "Complete embedding 35000 review\n",
      "Complete embedding 36000 review\n",
      "Complete embedding 37000 review\n",
      "Complete embedding 38000 review\n",
      "Complete embedding 39000 review\n",
      "Complete embedding 40000 review\n",
      "Complete embedding 41000 review\n",
      "Complete embedding 42000 review\n",
      "Complete embedding 43000 review\n",
      "Complete embedding 44000 review\n",
      "Complete embedding 45000 review\n",
      "Complete embedding 46000 review\n",
      "Complete embedding 47000 review\n",
      "Complete embedding 48000 review\n",
      "Complete embedding 49000 review\n",
      "Complete embedding 50000 review\n",
      "Complete embedding 51000 review\n",
      "Complete embedding 52000 review\n",
      "Complete embedding 53000 review\n",
      "Complete embedding 54000 review\n",
      "Complete embedding 55000 review\n",
      "Complete embedding 56000 review\n",
      "Complete embedding 57000 review\n",
      "Complete embedding 58000 review\n",
      "Complete embedding 59000 review\n",
      "Complete embedding 60000 review\n",
      "Complete embedding 61000 review\n",
      "Complete embedding 62000 review\n",
      "Complete embedding 63000 review\n",
      "Complete embedding 64000 review\n",
      "Complete embedding 65000 review\n",
      "Complete embedding 66000 review\n",
      "Complete embedding 67000 review\n",
      "Complete embedding 68000 review\n",
      "Complete embedding 69000 review\n",
      "Complete embedding 70000 review\n",
      "Complete embedding 71000 review\n",
      "Complete embedding 72000 review\n",
      "Complete embedding 73000 review\n",
      "Complete embedding 74000 review\n",
      "Complete embedding 75000 review\n",
      "Complete embedding 76000 review\n",
      "Complete embedding 77000 review\n",
      "Complete embedding 78000 review\n",
      "Complete embedding 79000 review\n",
      "Complete embedding 80000 review\n",
      "Complete embedding 81000 review\n",
      "Complete embedding 82000 review\n",
      "Complete embedding 83000 review\n",
      "Complete embedding 84000 review\n",
      "Complete embedding 85000 review\n",
      "Complete embedding 86000 review\n",
      "Complete embedding 87000 review\n",
      "Complete embedding 88000 review\n",
      "Complete embedding 89000 review\n",
      "Complete embedding 90000 review\n",
      "Complete embedding 91000 review\n",
      "Complete embedding 92000 review\n",
      "Complete embedding 93000 review\n",
      "Complete embedding 94000 review\n",
      "Complete embedding 95000 review\n",
      "Complete embedding 96000 review\n",
      "Complete embedding 97000 review\n",
      "Complete embedding 98000 review\n",
      "Complete embedding 99000 review\n",
      "Complete embedding 100000 review\n",
      "Complete embedding 101000 review\n",
      "Complete embedding 102000 review\n",
      "Complete embedding 103000 review\n",
      "Complete embedding 104000 review\n",
      "Complete embedding 105000 review\n",
      "Complete embedding 106000 review\n",
      "Complete embedding 107000 review\n",
      "Complete embedding 108000 review\n",
      "Complete embedding 109000 review\n",
      "Complete embedding 110000 review\n",
      "Complete embedding 111000 review\n",
      "Complete embedding 112000 review\n",
      "Complete embedding 113000 review\n",
      "Complete embedding 114000 review\n",
      "Complete embedding 115000 review\n",
      "Complete embedding 116000 review\n",
      "Complete embedding 117000 review\n",
      "Complete embedding 118000 review\n",
      "Complete embedding 119000 review\n",
      "Complete embedding 120000 review\n",
      "Complete embedding 121000 review\n",
      "Complete embedding 122000 review\n",
      "Complete embedding 123000 review\n",
      "Complete embedding 124000 review\n",
      "Complete embedding 125000 review\n",
      "Complete embedding 126000 review\n",
      "Complete embedding 127000 review\n",
      "Complete embedding 128000 review\n",
      "Complete embedding 129000 review\n",
      "Complete embedding 130000 review\n",
      "Complete embedding 131000 review\n",
      "Complete embedding 132000 review\n",
      "Complete embedding 133000 review\n",
      "Complete embedding 134000 review\n",
      "Complete embedding 135000 review\n",
      "Complete embedding 136000 review\n",
      "Complete embedding 137000 review\n",
      "Complete embedding 138000 review\n",
      "Complete embedding 139000 review\n",
      "Complete embedding 140000 review\n",
      "Complete embedding 141000 review\n",
      "Complete embedding 142000 review\n",
      "Complete embedding 143000 review\n",
      "Complete embedding 144000 review\n",
      "Complete embedding 145000 review\n",
      "Complete embedding 146000 review\n",
      "Complete embedding 147000 review\n",
      "Complete embedding 148000 review\n",
      "Complete embedding 149000 review\n"
     ]
    }
   ],
   "source": [
    "embeded_review = []\n",
    "NTK = np.zeros((128,), dtype=\"float32\")\n",
    "idx = 0\n",
    "\n",
    "for review in data:\n",
    "    \n",
    "    if idx % 1000 is 0: print(\"Complete embedding {0} review\".format(idx))\n",
    "    count = 0\n",
    "    review_vector = np.zeros((11, 128), dtype=\"float32\")\n",
    "    len = 0\n",
    "    for word in review:\n",
    "        if word in model.wv:\n",
    "            review_vector[count] = model.wv[word]\n",
    "        else:\n",
    "            review_vector[count] = NTK\n",
    "        count = count + 1\n",
    "        if(count >10) : break \n",
    "    embeded_review.append(review_vector)\n",
    "    idx = idx + 1\n",
    "    \n",
    "embeded_review = np.array(embeded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_review = np.array(embeded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 128)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_review[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_vector = np.zeros((12, 128), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "sent_length = 11\n",
    "vocabulary_size = 128\n",
    "num_classes = 1\n",
    "\n",
    "\n",
    "# inputs and labels\n",
    "sent_inputs = tf.placeholder(shape=[1,sent_length,vocabulary_size],dtype=tf.float32,name='sentence_inputs')\n",
    "sent_labels = tf.placeholder(shape=[1,num_classes],dtype=tf.float32,name='sentence_labels')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 16\n",
    "#NUM_EPOCHS = 10\n",
    "#vocab_size = 74065\n",
    "#embedding_size = 128\n",
    "\n",
    "#batch_size = 16\n",
    "#sent_length = 11\n",
    "#vocabulary_size = 128\n",
    "#num_classes = 1\n",
    "\n",
    "\n",
    "conv = tf.layers.conv1d(\n",
    "           inputs=sent_inputs,\n",
    "           filters=128,\n",
    "           kernel_size=3,\n",
    "           padding='same',\n",
    "           activation=tf.nn.relu)\n",
    "\n",
    "pool = tf.reduce_max(input_tensor=conv, axis=1) #max-pooling layer\n",
    "\n",
    "hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu) \n",
    "\n",
    "dropout_hidden = tf.layers.dropout(inputs=hidden, rate=0.2)\n",
    "\n",
    "logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n",
    "logits_sigmoid = tf.sigmoid(input = logits)\n",
    "pred = tf.cond(logits_sigmoid[0][0]>0.5, lambda: 1, lambda: 0)\n",
    "\n",
    "global_step = tf.train.get_global_step()\n",
    "loss = tf.losses.sigmoid_cross_entropy(sent_labels, logits)\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_sigmoid = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00912904]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(logits, feed_dict={sent_inputs: embeded_test[556].reshape(1,11,128), sent_labels: test_labels[234].reshape(1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### with open('./test_data.pickle', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete embedding 0 review\n",
      "Complete embedding 1000 review\n",
      "Complete embedding 2000 review\n",
      "Complete embedding 3000 review\n",
      "Complete embedding 4000 review\n",
      "Complete embedding 5000 review\n",
      "Complete embedding 6000 review\n",
      "Complete embedding 7000 review\n",
      "Complete embedding 8000 review\n",
      "Complete embedding 9000 review\n",
      "Complete embedding 10000 review\n",
      "Complete embedding 11000 review\n",
      "Complete embedding 12000 review\n",
      "Complete embedding 13000 review\n",
      "Complete embedding 14000 review\n",
      "Complete embedding 15000 review\n",
      "Complete embedding 16000 review\n",
      "Complete embedding 17000 review\n",
      "Complete embedding 18000 review\n",
      "Complete embedding 19000 review\n",
      "Complete embedding 20000 review\n",
      "Complete embedding 21000 review\n",
      "Complete embedding 22000 review\n",
      "Complete embedding 23000 review\n",
      "Complete embedding 24000 review\n",
      "Complete embedding 25000 review\n",
      "Complete embedding 26000 review\n",
      "Complete embedding 27000 review\n",
      "Complete embedding 28000 review\n",
      "Complete embedding 29000 review\n",
      "Complete embedding 30000 review\n",
      "Complete embedding 31000 review\n",
      "Complete embedding 32000 review\n",
      "Complete embedding 33000 review\n",
      "Complete embedding 34000 review\n",
      "Complete embedding 35000 review\n",
      "Complete embedding 36000 review\n",
      "Complete embedding 37000 review\n",
      "Complete embedding 38000 review\n",
      "Complete embedding 39000 review\n",
      "Complete embedding 40000 review\n",
      "Complete embedding 41000 review\n",
      "Complete embedding 42000 review\n",
      "Complete embedding 43000 review\n",
      "Complete embedding 44000 review\n",
      "Complete embedding 45000 review\n",
      "Complete embedding 46000 review\n",
      "Complete embedding 47000 review\n",
      "Complete embedding 48000 review\n",
      "Complete embedding 49000 review\n"
     ]
    }
   ],
   "source": [
    "embeded_test = []\n",
    "NTK = np.zeros((128,), dtype=\"float32\")\n",
    "idx = 0\n",
    "\n",
    "for review in test_data:\n",
    "    \n",
    "    if idx % 1000 is 0: print(\"Complete embedding {0} review\".format(idx))\n",
    "    count = 0\n",
    "    review_vector = np.zeros((11, 128), dtype=\"float32\")\n",
    "    len = 0\n",
    "    for word in review:\n",
    "        if word in model.wv:\n",
    "            review_vector[count] = model.wv[word]\n",
    "        else:\n",
    "            review_vector[count] = NTK\n",
    "        count = count + 1\n",
    "        if(count >10) : break \n",
    "    embeded_test.append(review_vector)\n",
    "    idx = idx + 1\n",
    "    \n",
    "embeded_test = np.array(embeded_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value conv1d/bias\n\t [[Node: conv1d/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv1d/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv1d/bias)]]\n\nCaused by op 'conv1d/bias/read', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-8563e059f293>\", line 17, in <module>\n    activation=tf.nn.relu)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 223, in conv1d\n    return layer.apply(inputs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 774, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 329, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 696, in __call__\n    self.build(input_shapes)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 168, in build\n    dtype=self.dtype)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 460, in add_variable\n    return self.add_weight(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 243, in add_weight\n    getter=vs.get_variable)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 534, in add_weight\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 497, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1328, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1090, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 435, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 404, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 796, in _get_single_variable\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2234, in variable\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2224, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2207, in default_variable_creator\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 259, in __init__\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 422, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 79, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3887, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value conv1d/bias\n\t [[Node: conv1d/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv1d/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv1d/bias)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value conv1d/bias\n\t [[Node: conv1d/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv1d/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv1d/bias)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-904680b1051f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0msent_inputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0membeded_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_labels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value conv1d/bias\n\t [[Node: conv1d/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv1d/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv1d/bias)]]\n\nCaused by op 'conv1d/bias/read', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-8563e059f293>\", line 17, in <module>\n    activation=tf.nn.relu)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 223, in conv1d\n    return layer.apply(inputs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 774, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 329, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 696, in __call__\n    self.build(input_shapes)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 168, in build\n    dtype=self.dtype)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 460, in add_variable\n    return self.add_weight(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 243, in add_weight\n    getter=vs.get_variable)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 534, in add_weight\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 497, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1328, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1090, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 435, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 404, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 796, in _get_single_variable\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2234, in variable\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2224, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2207, in default_variable_creator\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 259, in __init__\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 422, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 79, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3887, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value conv1d/bias\n\t [[Node: conv1d/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv1d/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](conv1d/bias)]]\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "session.run(pred,feed_dict={sent_inputs: embeded_test[1].reshape(1,11,128), sent_labels: test_labels[1].reshape(1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t = pd.read_csv('./data/nsmc/ratings_test.txt', quoting=3, delimiter='\\t', header=0)\n",
    "\n",
    "test_labels = np.array(t['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Train Loss at Epoch 0: 0.69\n",
      "Train Loss at Epoch 1: 0.69\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_steps = 2\n",
    "#tf.global_variables_initializer().run()\n",
    "# train_gen = BatchGenerator(batch_size,embeded_review,labels)\n",
    "# test_gen = BatchGenerator(batch_size,embeded_test,test_labels)\n",
    "\n",
    "test_interval = 50\n",
    "\n",
    "# Get accuracy \n",
    "def accuracy(labels,preds):\n",
    "    return np.sum(np.argmax(labels,axis=1)==preds)/labels.shape[0]\n",
    "\n",
    "print('Initialized\\n')\n",
    "\n",
    "\n",
    "for step in range(num_steps):\n",
    "    avg_loss = []\n",
    "    for idx in range(embeded_review.shape[0]):\n",
    "         \n",
    "        l,_ = session.run([loss,train_op],feed_dict={sent_inputs: embeded_review[idx].reshape(1,11,128), sent_labels: labels[idx].reshape(1,1)})\n",
    "        avg_loss.append(l)\n",
    "        \n",
    "    print('Train Loss at Epoch %d: %.2f'%(step,np.mean(avg_loss)))\n",
    "    test_accuracy = []\n",
    "    #if (step+1)%test_interval==0:        \n",
    "    #    for te_idx in range(embeded_test.shape[0]):\n",
    "    #        preds = session.run(pred,feed_dict={sent_inputs: embeded_test[te_idx].reshape(1,11,128), sent_labels: test_labels[te_idx].reshape(1,1)})\n",
    "    #        test_accuracy.append(accuracy(test_labels,preds))\n",
    "            \n",
    "    #    print('Test accuracy at Epoch %d: %.3f'%(step,np.mean(test_accuracy)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(pred,feed_dict={sent_inputs: embeded_test[327].reshape(1,11,128), sent_labels: test_labels[327].reshape(1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy at Epoch 2: 2482700.000\n"
     ]
    }
   ],
   "source": [
    "def accuracy(labels,preds):\n",
    "    return np.sum(labels==preds)/labels.shape[0]\n",
    "\n",
    "test_accuracy = []\n",
    "if 0==0:        \n",
    "    for te_idx in range(embeded_test.shape[0]):\n",
    "        preds = session.run(prediction,feed_dict={sent_inputs: embeded_test[te_idx].reshape(1,11,128), sent_labels: test_labels[te_idx].reshape(1,1)})\n",
    "        test_accuracy.append(accuracy(test_labels,preds))\n",
    "\n",
    "    print('Test accuracy at Epoch %d: %.3f'%(step,np.mean(test_accuracy)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value dense_5/bias\n\t [[Node: dense_5/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_5/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_5/bias)]]\n\nCaused by op 'dense_5/bias/read', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-8563e059f293>\", line 25, in <module>\n    logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 190, in dense\n    return layer.apply(inputs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 774, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 329, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 696, in __call__\n    self.build(input_shapes)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 899, in build\n    trainable=True)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 460, in add_variable\n    return self.add_weight(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 243, in add_weight\n    getter=vs.get_variable)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 534, in add_weight\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 497, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1328, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1090, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 435, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 404, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 796, in _get_single_variable\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2234, in variable\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2224, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2207, in default_variable_creator\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 259, in __init__\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 422, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 79, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3887, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_5/bias\n\t [[Node: dense_5/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_5/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_5/bias)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value dense_5/bias\n\t [[Node: dense_5/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_5/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_5/bias)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-2ff5c27d34e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0msent_inputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0membeded_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_labels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value dense_5/bias\n\t [[Node: dense_5/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_5/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_5/bias)]]\n\nCaused by op 'dense_5/bias/read', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-8563e059f293>\", line 25, in <module>\n    logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 190, in dense\n    return layer.apply(inputs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 774, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 329, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 696, in __call__\n    self.build(input_shapes)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 899, in build\n    trainable=True)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 460, in add_variable\n    return self.add_weight(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 243, in add_weight\n    getter=vs.get_variable)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 534, in add_weight\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 497, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1328, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1090, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 435, in get_variable\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 404, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 796, in _get_single_variable\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2234, in variable\n    use_resource=use_resource)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2224, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2207, in default_variable_creator\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 259, in __init__\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 422, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 79, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3887, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_5/bias\n\t [[Node: dense_5/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_5/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_5/bias)]]\n"
     ]
    }
   ],
   "source": [
    "session.run(pred,feed_dict={sent_inputs: embeded_test[1].reshape(1,11,128), sent_labels: test_labels[1].reshape(1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
