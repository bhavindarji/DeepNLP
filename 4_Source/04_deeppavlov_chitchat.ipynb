{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepPavlov sequence-to-sequence tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we are going to implement sequence-to-sequence [[original paper]](https://arxiv.org/abs/1409.3215) model in DeepPavlov.\n",
    "\n",
    "Sequence-to-sequence is the concept of mapping input sequence to target sequence. Sequence-to-sequence models consist of two main components: encoder and decoder. Encoder is used to encode the input sequence to dense representation and decoder uses this dense representation to generate target sequence.\n",
    "\n",
    "![sequence-to-sequence](img/seq2seq.png)\n",
    "\n",
    "Here, input sequence is ABC, special token <EOS\\> (end of sequence) is used as indicator to start decoding target sequence WXYZ.\n",
    "\n",
    "To implement this model in DeepPavlov we have to code some DeepPavlov abstractions:\n",
    "* **DatasetReader** to read the data\n",
    "* **DatasetIterator** to generate batches\n",
    "* **Vocabulary** to convert words to indexes\n",
    "* **Model** to train it and then use it\n",
    "* and some other components for pre- and postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import deeppavlov\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import chain\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & extract dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-16 18:40:50.275 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 205: Starting new HTTP connection (1): files.deeppavlov.ai:80\n",
      "2018-10-16 18:40:51.615 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 393: http://files.deeppavlov.ai:80 \"GET /datasets/personachat_v2.tar.gz HTTP/1.1\" 200 223217972\n",
      "2018-10-16 18:40:51.615 INFO in 'deeppavlov.core.data.utils'['utils'] at line 62: Downloading from http://files.deeppavlov.ai/datasets/personachat_v2.tar.gz to personachat\\personachat_v2.tar.gz\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 223M/223M [04:03<00:00, 916kB/s]\n",
      "2018-10-16 18:44:55.312 INFO in 'deeppavlov.core.data.utils'['utils'] at line 200: Extracting personachat\\personachat_v2.tar.gz archive into personachat\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://files.deeppavlov.ai/datasets/personachat_v2.tar.gz', './personachat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def download_decompress(url: str, download_path: [Path, str], extract_paths=None):\n",
    "    \"\"\"\n",
    "    다운로드 한 후 압축파일(.tar.gz/.gz/.zip) 풀어준다. 저장의 경우 폴더 구분해서 저장할 수 있다.\n",
    "    Arg:\n",
    "        url: download 할 url\n",
    "        download_path: 다운 한 파일의 저장 경로\n",
    "        extract_paths: 다운한 파일을 압춘 푼 파일을 저장할 경로(or 경로의 리스트)\n",
    "    \"\"\"\n",
    "    file_name = Path(urlparse(url).path).name #경로지정\n",
    "    download_path = Path(download_path) #경로지정\n",
    "\n",
    "    if extract_paths is None:\n",
    "        extract_paths = [download_path]\n",
    "    elif isinstance(extract_paths, list): # path가 list\n",
    "        extract_paths = [Path(path) for path in extract_paths]\n",
    "    else:\n",
    "        extract_paths = [Path(extract_paths)]\n",
    "\n",
    "    cache_dir = os.getenv('DP_CACHE_DIR') \n",
    "    extracted = False\n",
    "    if cache_dir:\n",
    "        cache_dir = Path(cache_dir)\n",
    "        url_hash = md5(url.encode('utf8')).hexdigest()[:15]\n",
    "        arch_file_path = cache_dir / url_hash\n",
    "        extracted_path = cache_dir / (url_hash + '_extracted')\n",
    "        extracted = extracted_path.exists()\n",
    "        if not extracted and not arch_file_path.exists():\n",
    "            simple_download(url, arch_file_path) # download 함수\n",
    "    else:\n",
    "        arch_file_path = download_path / file_name\n",
    "        simple_download(url, arch_file_path) # download 함수\n",
    "        extracted_path = extract_paths.pop()\n",
    "\n",
    "    if not extracted:\n",
    "        log.info('Extracting {} archive into {}'.format(arch_file_path, extracted_path)) \n",
    "        extracted_path.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "        \n",
    "        # 압축 방법따라 구분\n",
    "        if file_name.endswith('.tar.gz'):\n",
    "            untar(arch_file_path, extracted_path)\n",
    "        elif file_name.endswith('.gz'):\n",
    "            ungzip(arch_file_path, extracted_path / Path(file_name).with_suffix('').name)\n",
    "        elif file_name.endswith('.zip'):\n",
    "            with zipfile.ZipFile(arch_file_path, 'r') as zip_ref: \n",
    "                zip_ref.extractall(extracted_path)\n",
    "        else:\n",
    "            raise RuntimeError(f'Trying to extract an unknown type of archive {file_name}')\n",
    "\n",
    "        if not cache_dir:\n",
    "            arch_file_path.unlink()\n",
    "\n",
    "    for extract_path in extract_paths:\n",
    "        for src in extracted_path.iterdir():\n",
    "            dest = extract_path / src.name\n",
    "            if src.is_dir():\n",
    "                copytree(src, dest)\n",
    "            else:\n",
    "                extract_path.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy(str(src), str(dest))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DatasetReader is used to read and parse data from files. Here, we define new PersonaChatDatasetReader which reads [PersonaChat dataset](https://arxiv.org/abs/1801.07243). PersonaChat dataset consists of dialogs and user personalities.\n",
    "\n",
    "User personality is described by four sentences, e.g.:\n",
    "\n",
    "    i like to remodel homes.\n",
    "    i like to go hunting.\n",
    "    i like to shoot a bow.\n",
    "    my favorite holiday is halloween."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.train import build_model_from_config\n",
    "from deeppavlov.core.data.dataset_reader import DatasetReader\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "from deeppavlov.core.common.registry import register\n",
    "\n",
    "@register('personachat_dataset_reader') # 모델 이름 등록해놓는 decorator\n",
    "class PersonaChatDatasetReader(DatasetReader): # 인자로 받는 객체는 dataset읽은 class\n",
    "    \"\"\"\n",
    "    다운 로드한 personachat 데이터 읽기, parsing 하는 함수\n",
    "    \n",
    "    해당 데이터는 다음의 key값을 갖는 dictionary \n",
    "    [{\n",
    "        'persona': [list of persona sentences],\n",
    "        'x': input utterance,\n",
    "        'y': output utterance,\n",
    "        'dialog_history': list of previous utterances\n",
    "        'candidates': [list of candidate utterances]\n",
    "        'y_idx': index of y utt in candidates list\n",
    "      },\n",
    "       ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    def read(self, dir_path: str, mode='self_original'):\n",
    "        dir_path = Path(dir_path)\n",
    "        dataset = {}\n",
    "        for dt in ['train', 'valid', 'test']:\n",
    "            dataset[dt] = self._parse_data(dir_path / '{}_{}.txt'.format(dt, mode))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_data(filename):\n",
    "        examples = []\n",
    "        print(filename)\n",
    "        curr_persona = []\n",
    "        curr_dialog_history = []\n",
    "        persona_done = False\n",
    "        with filename.open('r') as fin:\n",
    "            for line in fin:\n",
    "                line = ' '.join(line.strip().split(' ')[1:])\n",
    "                your_persona_pref = 'your persona: '\n",
    "                if line[:len(your_persona_pref)] == your_persona_pref and persona_done:\n",
    "                    curr_persona = [line[len(your_persona_pref):]]\n",
    "                    curr_dialog_history = []\n",
    "                    persona_done = False\n",
    "                elif line[:len(your_persona_pref)] == your_persona_pref:\n",
    "                    curr_persona.append(line[len(your_persona_pref):])\n",
    "                else:\n",
    "                    persona_done = True\n",
    "                    x, y, _, candidates = line.split('\\t')\n",
    "                    candidates = candidates.split('|')\n",
    "                    example = {\n",
    "                        'persona': curr_persona,\n",
    "                        'x': x,\n",
    "                        'y': y,\n",
    "                        'dialog_history': curr_dialog_history[:],\n",
    "                        'candidates': candidates,\n",
    "                        'y_idx': candidates.index(y)\n",
    "                    }\n",
    "                    curr_dialog_history.extend([x, y])\n",
    "                    examples.append(example)\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def register(name: str = None) -> type:\n",
    "    \"\"\"\n",
    "    사용한 객체 이름으로 저장\n",
    "    \"\"\"\n",
    "    def decorate(model_cls: type, reg_name: str = None) -> type:\n",
    "        model_name = reg_name or short_name(model_cls)\n",
    "        global _REGISTRY\n",
    "        cls_name = model_cls.__module__ + ':' + model_cls.__name__\n",
    "        if model_name in _REGISTRY and _REGISTRY[model_name] != cls_name:\n",
    "            logger.warning('Registry name \"{}\" has been already registered and will be overwritten.'.format(model_name))\n",
    "        _REGISTRY[model_name] = cls_name\n",
    "        return model_cls\n",
    "\n",
    "    return lambda model_cls_name: decorate(model_cls_name, name)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "class DatasetReader:\n",
    "    \"\"\"데이터 set 읽기 위한 추상 class\"\"\"\n",
    "\n",
    "    def read(self, data_path: str, *args, **kwargs) -> Dict[str, List[Tuple[Any, Any]]]:\n",
    "        \"\"\"Reads a file from a path and returns data as a list of tuples of inputs and correct outputs\n",
    "         for every data type in ``train``, ``valid`` and ``test``.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personachat\\train_self_original.txt\n",
      "personachat\\valid_self_original.txt\n",
      "personachat\\test_self_original.txt\n"
     ]
    }
   ],
   "source": [
    "data = PersonaChatDatasetReader().read('./personachat') #데이터 불러온 후 parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 65719\n",
      "valid 7801\n",
      "test 7512\n"
     ]
    }
   ],
   "source": [
    "for k in data:\n",
    "    print(k, len(data[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': ['i like to remodel homes.',\n",
       "  'i like to go hunting.',\n",
       "  'i like to shoot a bow.',\n",
       "  'my favorite holiday is halloween.'],\n",
       " 'x': 'hi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape .',\n",
       " 'y': 'you must be very fast . hunting is one of my favorite hobbies .',\n",
       " 'dialog_history': [],\n",
       " 'candidates': ['my mom was single with 3 boys , so we never left the projects .',\n",
       "  'i try to wear all black every day . it makes me feel comfortable .',\n",
       "  'well nursing stresses you out so i wish luck with sister',\n",
       "  'yeah just want to pick up nba nfl getting old',\n",
       "  'i really like celine dion . what about you ?',\n",
       "  'no . i live near farms .',\n",
       "  'i wish i had a daughter , i am a boy mom . they are beautiful boys though still lucky',\n",
       "  'yeah when i get bored i play gone with the wind my favorite movie .',\n",
       "  'hi how are you ? i am eating dinner with my hubby and 2 kids .',\n",
       "  'were you married to your high school sweetheart ? i was .',\n",
       "  'that is great to hear ! are you a competitive rider ?',\n",
       "  'hi , i am doing ok . i am a banker . how about you ?',\n",
       "  'i am 5 years old',\n",
       "  'hi there . how are you today ?',\n",
       "  'i totally understand how stressful that can be .',\n",
       "  'yeah sometimes you do not know what you are actually watching',\n",
       "  'mother taught me to cook ! we are looking for an exterminator .',\n",
       "  'i enjoy romantic movie . what is your favorite season ? mine is summer .',\n",
       "  'editing photos takes a lot of work .',\n",
       "  'you must be very fast . hunting is one of my favorite hobbies .'],\n",
       " 'y_idx': 19}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['persona', 'x', 'y', 'dialog_history', 'candidates', 'y_idx'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset iterator is used to generate batches from parsed dataset (DatasetReader). Let's extract only *x* and *y* from parsed dataset and use them to predict sentence *y* by sentence *x*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n",
    "\n",
    "@register('personachat_iterator')\n",
    "class PersonaChatIterator(DataLearningIterator): # 인자로 받는 객체는 data iterate할 수 있도록 만든 Class(batch 가능)\n",
    "    def split(self, *args, **kwargs):\n",
    "        for dt in ['train', 'valid', 'test']:\n",
    "            setattr(self, dt, self._to_tuple(getattr(self, dt))) \n",
    "            # DataLearningIterator 객체의 'train', 'valid', 'test' 변수에 'train', 'valid', 'test' 의 x,y값 지정\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_tuple(data):\n",
    "        \"\"\"\n",
    "        데이터의 x,y 값 tuple로\n",
    "        \"\"\"\n",
    "        return list(map(lambda x: (x['x'], x['y']), data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@register('data_learning_iterator')\n",
    "class DataLearningIterator:\n",
    "    \"\"\"데이터셋 iterator, train,valid,test로 나눠 generate batch 함수 사용가능 \n",
    "\n",
    "    Args:\n",
    "        data: list of (x, y) pairs for every data type in ``'train'``, ``'valid'`` and ``'test'``\n",
    "        seed: random seed for data shuffling\n",
    "        shuffle: whether to shuffle data during batching\n",
    "\n",
    "    Attributes:\n",
    "        shuffle: whether to shuffle data during batching\n",
    "        random: instance of ``Random`` initialized with a seed\n",
    "    \"\"\"\n",
    "    def split(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, data: Dict[str, List[Tuple[Any, Any]]], seed: int = None, shuffle: bool = True,\n",
    "                 *args, **kwargs) -> None:\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.random = Random(seed)\n",
    "\n",
    "        self.train = data.get('train', [])\n",
    "        self.valid = data.get('valid', [])\n",
    "        self.test = data.get('test', [])\n",
    "        self.split(*args, **kwargs) # split함수 선언\n",
    "        self.data = {\n",
    "            'train': self.train,\n",
    "            'valid': self.valid,\n",
    "            'test': self.test,\n",
    "            'all': self.train + self.test + self.valid\n",
    "        }\n",
    "\n",
    "    def gen_batches(self, batch_size: int, data_type: str = 'train',\n",
    "                    shuffle: bool = None) -> Iterator[Tuple[tuple, tuple]]:\n",
    "        \"\"\"Generate batches of inputs and expected output to train neural networks\n",
    "\n",
    "        Args:\n",
    "            batch_size: number of samples in batch\n",
    "            data_type: can be either 'train', 'test', or 'valid'\n",
    "            shuffle: whether to shuffle dataset before batching\n",
    "\n",
    "        Yields:\n",
    "             a tuple of a batch of inputs and a batch of expected outputs\n",
    "        \"\"\"\n",
    "        if shuffle is None:\n",
    "            shuffle = self.shuffle\n",
    "\n",
    "        data = self.data[data_type] # 전체 데이터 중 train or valid or test 데이터 가져온다/\n",
    "        data_len = len(data) # 데이터 길이\n",
    "\n",
    "        if data_len == 0:\n",
    "            return\n",
    "\n",
    "        order = list(range(data_len)) # 길이에 대해서 range list 생성\n",
    "        if shuffle: \n",
    "            self.random.shuffle(order) # 생성한 순서 shuffle\n",
    "\n",
    "        if batch_size < 0:\n",
    "            batch_size = data_len # batch 안하는 경우\n",
    "\n",
    "        for i in range((data_len - 1) // batch_size + 1): # 배치 크기로 데이터 나눔\n",
    "            yield tuple(zip(*[data[o] for o in order[i * batch_size:(i + 1) * batch_size]])) # yield로 iterable한 객체 생성, \n",
    "\n",
    "    def get_instances(self, data_type: str = 'train') -> Tuple[tuple, tuple]:\n",
    "        \"\"\"Get all data for a selected data type\n",
    "\n",
    "        Args:\n",
    "            data_type (str): can be either ``'train'``, ``'test'``, ``'valid'`` or ``'all'``\n",
    "\n",
    "        Returns:\n",
    "             a tuple of all inputs for a data type and all expected outputs for a data type\n",
    "        \"\"\"\n",
    "        data = self.data[data_type]\n",
    "        return tuple(zip(*data))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look on data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi , how are you doing ? i am getting ready to do some cheetah chasing to stay in shape .'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: can you stenotype as fast as you can talk ?\n",
      "y: faster . the keyboard layout is easier in my opinion .\n",
      "----------\n",
      "x: he must be really good at gaming .\n",
      "y: it does not take much to be better than me , but yea he loves it .\n",
      "----------\n",
      "x: hello there ! how are you ?\n",
      "y: i am looking for love , i will never stop . was the youngest of eight kids did not get enough love .\n",
      "----------\n",
      "x: oops , going . are you talking about the hurricane ?\n",
      "y: yeh . i am a police officer n on duty at midnight . where do you live ?\n",
      "----------\n",
      "x: i do not have any yet\n",
      "y: well i am told 45 to 50 . how old are you\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "iterator = PersonaChatIterator(data)\n",
    "batch = [el for el in iterator.gen_batches(5, 'train')][0] # 5 크기로 배치로 묶은 후 첫 데이터 (shuffle 사용)\n",
    "for x, y in zip(*batch):\n",
    "    print('x:', x)\n",
    "    print('y:', y)\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer is used to extract tokens from utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JungHyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JungHyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\JungHyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     C:\\Users\\JungHyun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\nonbreaking_prefixes.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Hello', 'my', 'friend']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deeppavlov.models.tokenizers.lazy_tokenizer import LazyTokenizer\n",
    "tokenizer = LazyTokenizer()\n",
    "tokenizer(['Hello my friend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@register('lazy_tokenizer')\n",
    "class LazyTokenizer(Component):\n",
    "    \"\"\"nltk tokenizer 사용해서 tokenizing\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @overrides\n",
    "    def __call__(self, batch, *args, **kwargs):\n",
    "        if len(batch) > 0 and isinstance(batch[0], str):\n",
    "            batch = [word_tokenize(utt) for utt in batch] # nltk의 word_tokenize\n",
    "        return batch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary prepares mapping from tokens to token indexes. It uses train data to build this mapping.\n",
    "\n",
    "We will implement DialogVocab (inherited from SimpleVocabulary) wich adds all tokens from *x* and *y* utterances to vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-16 22:51:57.560 WARNING in 'deeppavlov.core.common.registry'['registry'] at line 54: Registry name \"dialog_vocab\" has been already registered and will be overwritten.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "\n",
    "@register('dialog_vocab')\n",
    "class DialogVocab(SimpleVocabulary):\n",
    "    def fit(self, *args):\n",
    "        tokens = chain(*args)\n",
    "        super().fit(tokens)\n",
    "\n",
    "    def __call__(self, batch, **kwargs):\n",
    "        indices_batch = []\n",
    "        for utt in batch:\n",
    "            tokens = [self[token] for token in utt]\n",
    "            indices_batch.append(tokens)\n",
    "        return indices_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@register('simple_vocab')\n",
    "class SimpleVocabulary(Estimator): # Estimator는 Component와 Serializable 객체를 인자로 받는 추상 클래스 (fit함수)\n",
    "    \"\"\"Vocabulary 생성.\"\"\"\n",
    "    def __init__(self, \n",
    "                 special_tokens=tuple(), \n",
    "                 default_token=None,\n",
    "                 max_tokens=2**30,\n",
    "                 min_freq=0,\n",
    "                 pad_with_zeros=False,\n",
    "                 unk_token=None,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.special_tokens = special_tokens # PAD, BOS, EOS, UNK\n",
    "        self.default_token = default_token\n",
    "        self._max_tokens = max_tokens\n",
    "        self._min_freq = min_freq # 최소 반복\n",
    "        self._pad_with_zeros = pad_with_zeros #ture or false\n",
    "        self.unk_token = unk_token # UNK\n",
    "        self.reset()\n",
    "        if self.load_path:\n",
    "            self.load()\n",
    "    \n",
    "    def fit(self, *args):\n",
    "        # token으로 vocab 생성\n",
    "        self.reset()\n",
    "        tokens = chain(*args)\n",
    "        # filter(None, <>) -- to filter empty tokens\n",
    "        self.freqs = Counter(filter(None, chain(*tokens)))\n",
    "        # special token 부터 vocab(t2i/i2t)에 등록\n",
    "        for special_token in self.special_tokens:\n",
    "            self._t2i[special_token] = self.count # token to index\n",
    "            self._i2t.append(special_token) # index to token\n",
    "            self.count += 1\n",
    "        # 기본 token vocab에 등록\n",
    "        for token, freq in self.freqs.most_common()[:self._max_tokens]:\n",
    "            if freq >= self._min_freq:\n",
    "                self._t2i[token] = self.count\n",
    "                self._i2t.append(token)\n",
    "                self.count += 1\n",
    "\n",
    "    def _add_tokens_with_freqs(self, tokens, freqs):\n",
    "        self.freqs = Counter()\n",
    "        self.freqs.update(dict(zip(tokens, freqs)))\n",
    "        # min_freq보다 작은 것들을 제거하고 t2i, i2t를 만듬\n",
    "        for token, freq in zip(tokens, freqs):\n",
    "            if freq >= self._min_freq or token in self.special_tokens:\n",
    "                self._t2i[token] = self.count\n",
    "                self._i2t.append(token)\n",
    "                self.count += 1\n",
    "\n",
    "    def __call__(self, batch, **kwargs):\n",
    "        indices_batch = []\n",
    "        for sample in batch:\n",
    "            indices_batch.append([self[token] for token in sample])\n",
    "        if self._pad_with_zeros and self.is_str_batch(batch):\n",
    "            indices_batch = zero_pad(indices_batch)\n",
    "        return indices_batch\n",
    "\n",
    "    def save(self):\n",
    "        # 객체에서 token과 각 token의 count 값 저장\n",
    "        log.info(\"[saving vocabulary to {}]\".format(self.save_path))\n",
    "        with self.save_path.open('wt', encoding='utf8') as f:\n",
    "            for n in range(len(self)):\n",
    "                token = self._i2t[n]\n",
    "                cnt = self.freqs[token]\n",
    "                f.write('{}\\t{:d}\\n'.format(token, cnt))\n",
    "\n",
    "    def load(self):\n",
    "        self.reset()\n",
    "        # 저장한 vocab 불러옴\n",
    "        if self.load_path:\n",
    "            if self.load_path.is_file():\n",
    "                log.info(\"[loading vocabulary from {}]\".format(self.load_path))\n",
    "                tokens, counts = [], []\n",
    "                for ln in self.load_path.open('r', encoding='utf8'):\n",
    "                    token, cnt = ln.split('\\t', 1)\n",
    "                    tokens.append(token)\n",
    "                    counts.append(int(cnt))\n",
    "                self._add_tokens_with_freqs(tokens, counts)\n",
    "            elif isinstance(self.load_path, Path):\n",
    "                if not self.load_path.parent.is_dir():\n",
    "                    raise ConfigError(\"Provided `load_path` for {} doesn't exist!\".format(\n",
    "                        self.__class__.__name__))\n",
    "        else:\n",
    "            raise ConfigError(\"`load_path` for {} is not provided!\".format(self))\n",
    "\n",
    "    @property\n",
    "    def len(self):\n",
    "        return len(self)\n",
    "\n",
    "    def keys(self):\n",
    "        return (self[n] for n in range(self.len))\n",
    "\n",
    "    def values(self):\n",
    "        return list(range(self.len))\n",
    "\n",
    "    def items(self):\n",
    "        return zip(self.keys(), self.values())\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, (int, np.integer)):\n",
    "            return self._i2t[key]\n",
    "        elif isinstance(key, str):\n",
    "            return self._t2i[key]\n",
    "        else:\n",
    "            raise NotImplementedError(\"not implemented for type `{}`\".format(type(key)))\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self._t2i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._i2t)\n",
    "\n",
    "    def is_str_batch(self, batch):\n",
    "        # batch의 각 data가 string인지 확인\n",
    "        if not self.is_empty(batch):\n",
    "            non_empty = [item for item in batch if len(item) > 0]\n",
    "            if isinstance(non_empty[0], str) or isinstance(non_empty[0][0], str):\n",
    "                return True\n",
    "            elif isinstance(non_empty[0][0], (int, np.integer)):\n",
    "                return False\n",
    "            else:\n",
    "                raise RuntimeError(f'The elements passed to the vocab are not strings '\n",
    "                                   f'or integers! But they are {type(element)}')\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def reset(self):\n",
    "        # default index is the position of default_token\n",
    "        if self.default_token is not None:\n",
    "            default_ind = self.special_tokens.index(self.default_token)\n",
    "        else:\n",
    "            default_ind = 0\n",
    "        self.freqs = None\n",
    "        unk_index = 0\n",
    "        if self.unk_token in self.special_tokens:\n",
    "            unk_index = self.special_tokens.index(self.unk_token)\n",
    "        self._t2i = defaultdict(lambda: unk_index)\n",
    "        self._i2t = []\n",
    "        self.count = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def is_empty(batch):\n",
    "        non_empty = [item for item in batch if len(item) > 0]\n",
    "        self._i2t = []\n",
    "        self.count = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def is_empty(batch):\n",
    "        non_empty = [item for item in batch if len(item) > 0]\n",
    "        return len(non_empty) == 0\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "class Estimator(Component, Serializable):\n",
    "    \"\"\"fit 하기 위한 추상 Class\"\"\"\n",
    "    @abstractmethod\n",
    "    def fit(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create instance of DialogVocab. We define save and load paths, minimal frequence of tokens which are added to vocabulary and set of special tokens.\n",
    "\n",
    "Special tokens are:\n",
    "* <PAD\\> - padding\n",
    "* <BOS\\> - begin of sequence\n",
    "* <EOS\\> - end of sequence\n",
    "* <UNK\\> - unknown token - token which is not presented in vocabulary\n",
    "\n",
    "And fit it on tokens from *x* and *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-16 22:52:40.802 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 86: [saving vocabulary to C:\\Users\\JungHyun\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\download\\vocab.dict]\n"
     ]
    }
   ],
   "source": [
    "vocab = DialogVocab(\n",
    "    save_path='./vocab.dict',\n",
    "    load_path='./vocab.dict',\n",
    "    min_freq=2,\n",
    "    special_tokens=('<PAD>','<BOS>', '<EOS>', '<UNK>',),\n",
    "    unk_token='<UNK>'\n",
    ")\n",
    "\n",
    "vocab.fit(tokenizer(iterator.get_instances(data_type='train')[0]), tokenizer(iterator.get_instances(data_type='train')[1]))\n",
    "vocab.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most frequent tokens in train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 103487),\n",
       " ('.', 101599),\n",
       " ('you', 48296),\n",
       " ('?', 43771),\n",
       " (',', 39500),\n",
       " ('a', 34214),\n",
       " ('to', 32105),\n",
       " ('do', 30574),\n",
       " ('is', 28579),\n",
       " ('my', 26953)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.freqs.most_common(10) # freqs : Counter 객체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of tokens in vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11595"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use built vocabulary to encode some tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 70, 13, 240, 3, 3, 2, 0]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab([['<BOS>', 'hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this', '<EOS>', '<PAD>']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed sequences of token indexes to neural model we should make their lengths equal. If sequence is too short we add <PAD\\> symbols to the end of sequence. If sequence is too long we just cut it.\n",
    "\n",
    "SentencePadder implements such behavior, it also adds <BOS\\> and <EOS\\> tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.models.component import Component\n",
    "\n",
    "@register('sentence_padder')\n",
    "class SentencePadder(Component):\n",
    "    def __init__(self, length_limit, pad_token_id=0, start_token_id=1, end_token_id=2, *args, **kwargs):\n",
    "        self.length_limit = length_limit\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.start_token_id = start_token_id\n",
    "        self.end_token_id = end_token_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        for i in range(len(batch)): # batch 단위로 데이터 받아옴\n",
    "            batch[i] = batch[i][:self.length_limit] # 각 데이터를 limit 길이 까지만 자름\n",
    "            batch[i] = [self.start_token_id] + batch[i] + [self.end_token_id] # 앞 뒤로 start, end token 추가\n",
    "            batch[i] += [self.pad_token_id] * (self.length_limit + 2 - len(batch[i]))  # 앞뒤로 limit 보다 작은 길이는 padding 추가\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Component(metaclass=ABCMeta):\n",
    "    \"\"\"pipeline 위한 추상 class\"\"\"\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def destroy(self):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<BOS>', 'hello', 'my', 'friend', '<UNK>', '<UNK>', '<EOS>', '<PAD>']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padder = SentencePadder(length_limit=6) #최대 길이 6으로 padder 객체 생성\n",
    "vocab(padder(vocab([['hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this']]))) # 길이5인 예제 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Model\n",
    "Model consists of two main components: encoder and decoder. We can implement them independently and then put them together in one Seq2Seq model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "Encoder builds hidden representation of input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(inputs, inputs_len, embedding_matrix, cell_size, keep_prob=1.0):\n",
    "    # inputs: tf.int32 tensor, [batch size x seq_len], 각 값은 token ID\n",
    "    # inputs_len: tf.int32 tensor, [batch size]\n",
    "    # embedding_matrix: tf.float32 tensor, [vocab_size x vocab_dim]\n",
    "    # cell_size: cell의 hidden size(hidden state의 dimension)\n",
    "    # keep_prob: dropout keep 확률\n",
    "    with tf.variable_scope('encoder'):\n",
    "        # first of all we should embed every token in input sequence (use tf.nn.embedding_lookup, don't forget about dropout)\n",
    "        x_emb = tf.nn.dropout(tf.nn.embedding_lookup(embedding_matrix, inputs), keep_prob=keep_prob)\n",
    "        \n",
    "        # 하나의 GRU cell (LSTM 사용 가능)\n",
    "        encoder_cell = tf.nn.rnn_cell.GRUCell(\n",
    "                            num_units=cell_size,\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            name='encoder_cell')\n",
    "        \n",
    "        # use tf.nn.dynamic_rnn to encode input sequence, use actual length of input sequence\n",
    "        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(cell=encoder_cell, inputs=x_emb, sequence_length=inputs_len, dtype=tf.float32)\n",
    "    return encoder_outputs, encoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your encoder implementation:\n",
    "\n",
    "next cell output shapes are\n",
    "\n",
    "32 x 10 x 100 and 32 x 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'encoder/rnn/transpose_1:0' shape=(32, 10, 100) dtype=float32>,\n",
       " <tf.Tensor 'encoder/rnn/while/Exit_3:0' shape=(32, 100) dtype=float32>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "실제 encoder 아닌 data 32개, 최대 길이 10인 sequence 생성(vocab은 100개) / cell의 hidden state의 dim은 100\n",
    "\"\"\"\"\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "vocab_size = 100\n",
    "hidden_dim = 100\n",
    "inputs = tf.cast(tf.random_uniform(shape=[32, 10]) * vocab_size, tf.int32) # [batch size x seq_len] batch : 32 / seq_len : 10 \n",
    "# vocab_size를 곱해서 0~1 을 0~100의 수 가지도록 한 뒤 integer로 만듬\n",
    "mask = tf.cast(tf.random_uniform(shape=[32, 10]) * 2, tf.int32) # [batch size x seq_len]\n",
    "# 0 or 1 값 가지도록\n",
    "inputs_len = tf.reduce_sum(mask, axis=1) # 각 row에 대해서 reduce_sum을 해서 [32 x 1] 값 만듬 => random하게 길이 설정\n",
    "embedding_matrix = tf.random_uniform(shape=[vocab_size, hidden_dim]) # embedding matrix\n",
    "\n",
    "encoder(inputs, inputs_len, embedding_matrix, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "Decoder uses encoder outputs and encoder state to produce output sequence.\n",
    "\n",
    "Here, you should:\n",
    "* define your decoder_cell (GRU or LSTM)\n",
    "\n",
    "it will be your baseline seq2seq model.\n",
    "\n",
    "\n",
    "And, to improve the model:\n",
    "* add Teacher Forcing\n",
    "* add Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(encoder_outputs, encoder_state, embedding_matrix, mask,\n",
    "            cell_size, max_length, y_ph,\n",
    "            start_token_id=1, keep_prob=1.0,\n",
    "            teacher_forcing_rate_ph=None,\n",
    "            use_attention=False, is_train=True):\n",
    "    # decoder\n",
    "    # encoder_outputs: tf.float32 tensor, [batch size x seq_len x encoder_cell_size]\n",
    "    # encoder_state: tf.float32 tensor, [batch size x encoder_cell_size]\n",
    "    # embedding_matrix: tf.float32 tensor, [vocab_size x vocab_dim]\n",
    "    # mask: tf.int32 tensor, [batch size x seq_len] sequence 값들 중 maked 된 값은 0\n",
    "    # cell_size: hidden state의 dimension\n",
    "    # max_length: output의 max_length\n",
    "    # start_token_id: vocab에서 start token <BOS> 의 id\n",
    "    # keep_prob: dropout 확률\n",
    "    # teacher_forcing_rate_ph: teacher forcing 사용시 확률\n",
    "    # use_attention: attention 사용 유무\n",
    "    # is_train: 학습 유무, inference 시에는 teacher forcing 사용 안함\n",
    "    with tf.variable_scope('decoder'):\n",
    "        # define decoder recurrent cell\n",
    "        decoder_cell = tf.nn.rnn_cell.GRUCell(\n",
    "                            num_units=cell_size,\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            name='decoder_cell')\n",
    "        \n",
    "        # initial value of output_token on previsous step is start_token\n",
    "        output_token = tf.ones(shape=(tf.shape(encoder_outputs)[0],), dtype=tf.int32) * start_token_id \n",
    "        # [batch,1] = 1 가지도록, 모든 data에 대해 start token 가지도록 함\n",
    "        \n",
    "        # decoder_state의 첫 값(encoder의 output state)\n",
    "        decoder_state = encoder_state\n",
    "\n",
    "        pred_tokens = []\n",
    "        logits = []\n",
    "\n",
    "        # use for loop to sequentially call recurrent cell\n",
    "        for i in range(max_length):\n",
    "            \"\"\"\n",
    "            TEACHER FORCING\n",
    "            # here you can try to implement teacher forcing for your model\n",
    "            # details about teacher forcing are explained further in tutorial\n",
    "            \n",
    "            # pseudo code:\n",
    "            NOTE THAT FOLLOWING CONDITIONS SHOULD BE EVALUATED AT GRAPH RUNTIME\n",
    "            use tf.cond and tf.logical operations instead of python if\n",
    "            \n",
    "            if i > 0 and is_train and random_value < teacher_forcing_rate_ph:\n",
    "                input_token = y_ph[:, i-1] # 예측 token 이 아니라 실제 이전 token을 다음 input으로 넣는다.\n",
    "            else:\n",
    "                input_token = output_token\n",
    "\n",
    "            input_token_emb = tf.nn.embedding_lookup(embedding_matrix, input_token)\n",
    "            \n",
    "            \"\"\"\n",
    "            if i > 0:\n",
    "                input_token_emb = tf.cond(\n",
    "                                      tf.logical_and(\n",
    "                                          is_train,\n",
    "                                          tf.random_uniform(shape=(), maxval=1) <= teacher_forcing_rate_ph \n",
    "                                          # 일정 확률 이상일때 실제 token 사용\n",
    "                                      ),\n",
    "                                      lambda: tf.nn.embedding_lookup(embedding_matrix, y_ph[:, i-1]), # teacher forcing\n",
    "                                      lambda: tf.nn.embedding_lookup(embedding_matrix, output_token)\n",
    "                                      )\n",
    "            else:\n",
    "                input_token_emb = tf.nn.embedding_lookup(embedding_matrix, output_token) # 처음에는 start token의 embedding 값\n",
    "\n",
    "            \"\"\"\n",
    "            ATTENTION MECHANISM\n",
    "            # here you can add attention to your model\n",
    "            # you can find details about attention further in tutorial\n",
    "            \"\"\"            \n",
    "            if use_attention: # attention 사용\n",
    "                # compute attention and concat attention vector to input_token_emb\n",
    "                att = dot_attention(encoder_outputs, decoder_state, mask, scope='att') \n",
    "                # decoder state 값과 encoder의 output들에 대해서 dot attention 계산\n",
    "                \n",
    "                input_token_emb = tf.concat([input_token_emb, att], axis=-1) # attention 값들에 대해서 가중 평균 계산\n",
    "\n",
    "\n",
    "            input_token_emb = tf.nn.dropout(input_token_emb, keep_prob=keep_prob) # dropout 적용\n",
    "            # call recurrent cell\n",
    "            decoder_outputs, decoder_state = decoder_cell(input_token_emb, decoder_state)\n",
    "            decoder_outputs = tf.nn.dropout(decoder_outputs, keep_prob=keep_prob)\n",
    "            # project decoder output to embeddings dimension\n",
    "            embeddings_dim = embedding_matrix.get_shape()[1]\n",
    "            output_proj = tf.layers.dense(decoder_outputs, embeddings_dim, activation=tf.nn.tanh,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          name='proj', reuse=tf.AUTO_REUSE) # output token 위한 dense layer\n",
    "            # compute logits\n",
    "            output_logits = tf.matmul(output_proj, embedding_matrix, transpose_b=True) # output_proj x (embedding_matrix)^T\n",
    "\n",
    "            logits.append(output_logits) \n",
    "            output_probs = tf.nn.softmax(output_logits)\n",
    "            output_token = tf.argmax(output_probs, axis=-1)\n",
    "            pred_tokens.append(output_token)\n",
    "\n",
    "        y_pred_tokens = tf.transpose(tf.stack(pred_tokens, axis=0), [1, 0])\n",
    "        y_logits = tf.transpose(tf.stack(logits, axis=0), [1, 0, 2])\n",
    "    return y_pred_tokens, y_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of next cell should be with shapes:\n",
    "\n",
    "    32 x 10\n",
    "    32 x 10 x 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'decoder/transpose:0' shape=(32, 10) dtype=int64>,\n",
       " <tf.Tensor 'decoder/transpose_1:0' shape=(32, 10, 100) dtype=float32>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "vocab_size = 100\n",
    "hidden_dim = 100\n",
    "inputs = tf.cast(tf.random_uniform(shape=[32, 10]) * vocab_size, tf.int32) # bs x seq_len\n",
    "mask = tf.cast(tf.random_uniform(shape=[32, 10]) * 2, tf.int32) # bs x seq_len\n",
    "inputs_len = tf.reduce_sum(mask, axis=1)\n",
    "embedding_matrix = tf.random_uniform(shape=[vocab_size, hidden_dim])\n",
    "\n",
    "teacher_forcing_rate = tf.random_uniform(shape=())\n",
    "y = tf.cast(tf.random_uniform(shape=[32, 10]) * vocab_size, tf.int32)\n",
    "\n",
    "encoder_outputs, encoder_state = encoder(inputs, inputs_len, embedding_matrix, hidden_dim)\n",
    "decoder(encoder_outputs, encoder_state, embedding_matrix, mask, hidden_dim, max_length=10,\n",
    "        y_ph=y, teacher_forcing_rate_ph=teacher_forcing_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq model should be inherited from TFModel class and implement following methods:\n",
    "* train_on_batch - this method is called in training phase\n",
    "* \\_\\_call\\_\\_ - this method is called to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.models.tf_model import TFModel\n",
    "\n",
    "@register('seq2seq')\n",
    "class Seq2Seq(TFModel):\n",
    "    def __init__(self, **kwargs):\n",
    "        # hyperparameters\n",
    "        \n",
    "        # dimension of word embeddings\n",
    "        self.embeddings_dim = kwargs.get('embeddings_dim', 100)\n",
    "        # size of recurrent cell in encoder and decoder\n",
    "        self.cell_size = kwargs.get('cell_size', 200)\n",
    "        # dropout keep_probability\n",
    "        self.keep_prob = kwargs.get('keep_prob', 0.8)\n",
    "        # learning rate\n",
    "        self.learning_rate = kwargs.get('learning_rate', 3e-04)\n",
    "        # max length of output sequence\n",
    "        self.max_length = kwargs.get('max_length', 20)\n",
    "        self.grad_clip = kwargs.get('grad_clip', 5.0) # gradient regularization\n",
    "        self.start_token_id = kwargs.get('start_token_id', 1)\n",
    "        self.vocab_size = kwargs.get('vocab_size', 11595)\n",
    "        self.teacher_forcing_rate = kwargs.get('teacher_forcing_rate', 0.0)\n",
    "        self.use_attention = kwargs.get('use_attention', False)\n",
    "        \n",
    "        # create tensorflow session to run computational graph in it\n",
    "        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        self.sess_config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=self.sess_config)\n",
    "        \n",
    "        self.init_graph()\n",
    "        \n",
    "        # define train op\n",
    "        self.train_op = self.get_train_op(self.loss, self.lr_ph,\n",
    "                                          optimizer=tf.train.AdamOptimizer,\n",
    "                                          clip_norm=self.grad_clip) \n",
    "        # initialize graph variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        # load saved model if there is one\n",
    "        if self.load_path is not None:\n",
    "            self.load()\n",
    "        \n",
    "    def init_graph(self):\n",
    "        # create placeholders\n",
    "        self.init_placeholders()\n",
    "\n",
    "        self.x_mask = tf.cast(self.x_ph, tf.int32) #?\n",
    "        self.y_mask = tf.cast(self.y_ph, tf.int32) #?\n",
    "        \n",
    "        self.x_len = tf.reduce_sum(self.x_mask, axis=1) # 각 데이터의 길이\n",
    "        \n",
    "        # create embeddings matrix for tokens\n",
    "        self.embeddings = tf.Variable(tf.random_uniform((self.vocab_size, self.embeddings_dim), -0.1, 0.1, name='embeddings'), dtype=tf.float32)\n",
    "\n",
    "        # encoder\n",
    "        encoder_outputs, encoder_state = encoder(self.x_ph, self.x_len, self.embeddings, self.cell_size, self.keep_prob_ph)\n",
    "\n",
    "        # decoder\n",
    "        self.y_pred_tokens, y_logits = decoder(encoder_outputs, encoder_state, self.embeddings, self.x_mask,\n",
    "                                                      self.cell_size, self.max_length,\n",
    "                                                      self.y_ph, self.start_token_id, self.keep_prob_ph,\n",
    "                                                      self.teacher_forcing_rate_ph, self.use_attention, self.is_train_ph)\n",
    "        \n",
    "        # loss\n",
    "        self.y_ohe = tf.one_hot(self.y_ph, depth=self.vocab_size)\n",
    "        self.y_mask = tf.cast(self.y_mask, tf.float32) # 연산을 위해 casting\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.y_ohe, logits=y_logits) * self.y_mask\n",
    "        self.loss = tf.reduce_sum(self.loss) / tf.reduce_sum(self.y_mask) # loss 평균\n",
    "    \n",
    "    def init_placeholders(self):\n",
    "        # placeholders for inputs\n",
    "        self.x_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name='x_ph')\n",
    "        # at inference time y_ph is used (y_ph exists in computational graph)  when teacher forcing is activated, so we add dummy default value\n",
    "        # this dummy value is not actually used at inference\n",
    "        self.y_ph = tf.placeholder_with_default(tf.zeros_like(self.x_ph), shape=(None,None), name='y_ph')\n",
    "\n",
    "        # placeholders for model parameters\n",
    "        self.lr_ph = tf.placeholder(dtype=tf.float32, shape=[], name='lr_ph')\n",
    "        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name='keep_prob_ph')\n",
    "        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name='is_train_ph')\n",
    "        self.teacher_forcing_rate_ph = tf.placeholder_with_default(0.0, shape=[], name='teacher_forcing_rate_ph')\n",
    "            \n",
    "    def _build_feed_dict(self, x, y=None):\n",
    "        feed_dict = {\n",
    "            self.x_ph: x,\n",
    "        }\n",
    "        if y is not None:\n",
    "            feed_dict.update({\n",
    "                self.y_ph: y,\n",
    "                self.lr_ph: self.learning_rate,\n",
    "                self.keep_prob_ph: self.keep_prob,\n",
    "                self.is_train_ph: True,\n",
    "                self.teacher_forcing_rate_ph: self.teacher_forcing_rate,\n",
    "            })\n",
    "        return feed_dict\n",
    "    \n",
    "    def train_on_batch(self, x, y):\n",
    "        feed_dict = self._build_feed_dict(x, y)\n",
    "        loss, _ = self.sess.run([self.loss, self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        feed_dict = self._build_feed_dict(x)\n",
    "        y_pred = self.sess.run(self.y_pred_tokens, feed_dict=feed_dict)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TFModel(NNModel, metaclass=TfModelMeta):\n",
    "    \"\"\"Parent class for all components using TensorFlow.\"\"\"\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        if not hasattr(self, 'sess'):\n",
    "            raise RuntimeError('Your TensorFlow model {} must'\n",
    "                               ' have sess attribute!'.format(self.__class__.__name__))\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def load(self, exclude_scopes: Optional[Iterable] = ('Optimizer',)) -> None:\n",
    "        \"\"\"Load model parameters from self.load_path\"\"\"\n",
    "        path = str(self.load_path.resolve())\n",
    "        # Check presence of the model files\n",
    "        if tf.train.checkpoint_exists(path):\n",
    "            log.info('[loading model from {}]'.format(path))\n",
    "            # Exclude optimizer variables from saved variables\n",
    "            var_list = self._get_saveable_variables(exclude_scopes)\n",
    "            saver = tf.train.Saver(var_list)\n",
    "            saver.restore(self.sess, path)\n",
    "\n",
    "    def save(self, exclude_scopes: Optional[Iterable] = ('Optimizer',)) -> None:\n",
    "        \"\"\"Save model parameters to self.save_path\"\"\"\n",
    "        path = str(self.save_path.resolve())\n",
    "        log.info('[saving model to {}]'.format(path))\n",
    "        var_list = self._get_saveable_variables(exclude_scopes)\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        saver.save(self.sess, path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_saveable_variables(exclude_scopes=tuple()):\n",
    "        all_vars = variables._all_saveable_objects()\n",
    "        vars_to_train = [var for var in all_vars if all(sc not in var.name for sc in exclude_scopes)]\n",
    "        return vars_to_train\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_trainable_variables(exclude_scopes=tuple()):\n",
    "        all_vars = tf.global_variables()\n",
    "        vars_to_train = [var for var in all_vars if all(sc not in var.name for sc in exclude_scopes)]\n",
    "        return vars_to_train\n",
    "\n",
    "    def get_train_op(self,\n",
    "                     loss,\n",
    "                     learning_rate,\n",
    "                     optimizer=None,\n",
    "                     clip_norm=None,\n",
    "                     learnable_scopes=None,\n",
    "                     optimizer_scope_name=None):\n",
    "        \"\"\" Get train operation for given loss\n",
    "\n",
    "        Args:\n",
    "            loss: loss, tf tensor or scalar\n",
    "            learning_rate: scalar or placeholder\n",
    "            clip_norm: clip gradients norm by clip_norm\n",
    "            learnable_scopes: which scopes are trainable (None for all)\n",
    "            optimizer: instance of tf.train.Optimizer, default Adam\n",
    "\n",
    "        Returns:\n",
    "            train_op\n",
    "        \"\"\"\n",
    "        if optimizer_scope_name is None:\n",
    "            opt_scope = tf.variable_scope('Optimizer')\n",
    "        else:\n",
    "            opt_scope = tf.variable_scope(optimizer_scope_name)\n",
    "        with opt_scope:\n",
    "            if learnable_scopes is None:\n",
    "                variables_to_train = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            else:\n",
    "                variables_to_train = []\n",
    "                for scope_name in learnable_scopes:\n",
    "                    variables_to_train.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_name))\n",
    "\n",
    "            if optimizer is None:\n",
    "                optimizer = tf.train.AdamOptimizer\n",
    "\n",
    "            # For batch norm it is necessary to update running averages\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(extra_update_ops):\n",
    "\n",
    "                def clip_if_not_none(grad):\n",
    "                    if grad is not None:\n",
    "                        return tf.clip_by_norm(grad, clip_norm)\n",
    "\n",
    "                opt = optimizer(learning_rate)\n",
    "                grads_and_vars = opt.compute_gradients(loss, var_list=variables_to_train)\n",
    "                if clip_norm is not None:\n",
    "                    grads_and_vars = [(clip_if_not_none(grad), var)\n",
    "                                      for grad, var in grads_and_vars]\n",
    "                train_op = opt.apply_gradients(grads_and_vars)\n",
    "        return train_op\n",
    "\n",
    "    @staticmethod\n",
    "    def print_number_of_parameters():\n",
    "        \"\"\"\n",
    "        Print number of *trainable* parameters in the network\n",
    "        \"\"\"\n",
    "        log.info('Number of parameters: ')\n",
    "        variables = tf.trainable_variables()\n",
    "        blocks = defaultdict(int)\n",
    "        for var in variables:\n",
    "            # Get the top level scope name of variable\n",
    "            block_name = var.name.split('/')[0]\n",
    "            number_of_parameters = np.prod(var.get_shape().as_list())\n",
    "            blocks[block_name] += number_of_parameters\n",
    "        for block_name, cnt in blocks.items():\n",
    "            log.info(\"{} - {}.\".format(block_name, cnt))\n",
    "        total_num_parameters = np.sum(list(blocks.values()))\n",
    "        log.info('Total number of parameters equal {}'.format(total_num_parameters))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create model with random weights and default parameters, change path to model, otherwise it will be stored in deeppavlov/download folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-71-42648474b479>:67: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-17 01:14:32.802 WARNING in 'tensorflow'['tf_logging'] at line 125: From <ipython-input-71-42648474b479>:67: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s2s = Seq2Seq(\n",
    "    save_path='PATH_TO_YOUR_WORKING_DIR/model',\n",
    "    load_path='PATH_TO_YOUR_WORKING_DIR/model'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we firstly run all preprocessing steps and call seq2seq model, and then convert token indexes to tokens. As result we should get some random sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nintendo',\n",
       "  'uni',\n",
       "  'roomage',\n",
       "  'cricket',\n",
       "  'gamer',\n",
       "  'gnc',\n",
       "  'suffer',\n",
       "  'loosing',\n",
       "  'fished',\n",
       "  'galleries',\n",
       "  'galleries',\n",
       "  'close',\n",
       "  'owe',\n",
       "  'guesses',\n",
       "  'bowl',\n",
       "  'cali',\n",
       "  'energetic',\n",
       "  'frightening',\n",
       "  'bad',\n",
       "  'cali']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(s2s(padder(vocab([['today','is','so','hot']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['music',\n",
       "  'music',\n",
       "  'trout',\n",
       "  'cooper',\n",
       "  'settle',\n",
       "  'successful',\n",
       "  'caesar',\n",
       "  'agriculture',\n",
       "  'agriculture',\n",
       "  'seas',\n",
       "  'give',\n",
       "  'ahahah',\n",
       "  'starved',\n",
       "  'uses',\n",
       "  'spaniel',\n",
       "  'cum',\n",
       "  'f',\n",
       "  'cum',\n",
       "  'especially',\n",
       "  'f']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(s2s(padder(vocab([['hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this']]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention mechanism\n",
    "Attention mechanism [[paper](https://arxiv.org/abs/1409.0473)] allows to aggregate information from \"memory\" according to current state. By aggregating we suppose weighted sum of \"memory\" items. Weight of each memory item depends on current state.\n",
    "\n",
    "Without attention decoder could use only last hidden state of encoder. Attention mechanism gives access to all encoder states during decoding.\n",
    "\n",
    "![attention](img/attention.png)\n",
    "\n",
    "One of the simpliest ways to compute attention weights (*a_ij*) is to compute them by dot product between memory items and state and then apply softmax function. Other ways of computing *multiplicative* attention could be found in this [paper](https://arxiv.org/abs/1508.04025).\n",
    "\n",
    "We also need a mask to skip some sequence elements like <PAD\\>. To make weight of undesired memory items close to zero we can add big negative value to logits (result of dot product) before applying softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_mask(values, mask):\n",
    "    # adds big negative to masked values\n",
    "    INF = 1e30\n",
    "    return -INF * (1 - tf.cast(mask, tf.float32)) + values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_attention(memory, state, mask, scope=\"dot_attention\"):\n",
    "    # inputs: bs x seq_len x hidden_dim\n",
    "    # state: bs x hidden_dim\n",
    "    # mask: bs x seq_len\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        # dot product between each item in memory and state\n",
    "        logits = tf.matmul(memory, tf.expand_dims(state, axis=1), transpose_b=True)\n",
    "        logits = tf.squeeze(logits, [2])\n",
    "        \n",
    "        # apply mask to logits\n",
    "        logits = softmax_mask(logits, mask)\n",
    "        \n",
    "        # apply softmax to logits\n",
    "        att_weights = tf.expand_dims(tf.nn.softmax(logits), axis=2)\n",
    "        \n",
    "        # compute weighted sum of items in memory\n",
    "        att = tf.reduce_sum(att_weights * memory, axis=1)\n",
    "        return att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your implementation:\n",
    "\n",
    "outputs should be with shapes 32 x 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "memory = tf.random_normal(shape=[32, 10, 100]) # bs x seq_len x hidden_dim\n",
    "state = tf.random_normal(shape=[32, 100]) # bs x hidden_dim\n",
    "mask = tf.cast(tf.random_normal(shape=[32, 10]), tf.int32) # bs x seq_len\n",
    "dot_attention(memory, state, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teacher forcing\n",
    "\n",
    "We have implemented decoder, which takes as input it's own output during training and inference time. But, at early stages of training it could be hard for model to produce long sequences depending on it's own close to random output. Teacher forcing can help with this: instead of feeding model's output we can feed ground truth tokens. It helps model on training time, but on inference we still can rely only on it's own output.\n",
    "\n",
    "\n",
    "Using model's output:\n",
    "\n",
    "<img src=\"img/sampling.png\" alt=\"sampling\" width=50%/>\n",
    "\n",
    "Teacher forcing:\n",
    "\n",
    "<img src=\"img/teacher_forcing.png\" alt=\"teacher_forcing\" width=50%/>\n",
    "\n",
    "It is not necessary to feed ground truth tokens on each time step - we can randomly choose with some rate if we want ground truth input or predicted by model.\n",
    "*teacher_forcing_rate* parameter of seq2seq model can control such behavior.\n",
    "\n",
    "More details about teacher forcing could be found in DeepLearningBook [Chapter 10.2.1](http://www.deeplearningbook.org/contents/rnn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create model with random weights and default parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we firstly run all preprocessing steps and call seq2seq model, and then convert token indexes to tokens. As result we should get some random sequence of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In postprocessing step we are going to remove all <PAD\\>, <BOS\\>, <EOS\\> tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register('postprocessing')\n",
    "class SentencePostprocessor(Component):\n",
    "    def __init__(self, pad_token='<PAD>', start_token='<BOS>', end_token='<EOS>', *args, **kwargs):\n",
    "        self.pad_token = pad_token\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        for i in range(len(batch)):\n",
    "            batch[i] = ' '.join(self._postproc(batch[i]))\n",
    "        return batch\n",
    "    \n",
    "    def _postproc(self, utt):\n",
    "        if self.end_token in utt:\n",
    "            utt = utt[:utt.index(self.end_token)]\n",
    "        return utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess = SentencePostprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clever clever clever clever ponder ponder whipped royalty lax desire duramax iran dope tuck teaches envious envious coordination parrots parrots']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocess(vocab(s2s(padder(vocab([['who', 'are', 'you']])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['music music trout cooper settle successful caesar agriculture agriculture seas give ahahah starved uses spaniel cum f cum especially f']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocess(vocab(s2s(padder(vocab([['hello', 'my', 'friend', 'there_is_no_such_word_in_dataset', 'and_this']])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create config file\n",
    "Let's put is all together in one config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"dataset_reader\": {\n",
    "    \"name\": \"personachat_dataset_reader\",\n",
    "    \"data_path\": \"YOUR_PATH_TO_FOLDER_WITH_PERSONACHAT_DATASET\"\n",
    "  },\n",
    "  \"dataset_iterator\": {\n",
    "    \"name\": \"personachat_iterator\",\n",
    "    \"seed\": 1337,\n",
    "    \"shuffle\": True\n",
    "  },\n",
    "  \"chainer\": {\n",
    "    \"in\": [\"x\"],\n",
    "    \"in_y\": [\"y\"],\n",
    "    \"pipe\": [\n",
    "      {\n",
    "        \"name\": \"lazy_tokenizer\",\n",
    "        \"id\": \"tokenizer\",\n",
    "        \"in\": [\"x\"],\n",
    "        \"out\": [\"x_tokens\"]\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"lazy_tokenizer\",\n",
    "        \"id\": \"tokenizer\",\n",
    "        \"in\": [\"y\"],\n",
    "        \"out\": [\"y_tokens\"]\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"dialog_vocab\",\n",
    "        \"id\": \"vocab\",\n",
    "        \"save_path\": \"YOUR_PATH_TO_WORKING_DIR/vocab.dict\",\n",
    "        \"load_path\": \"YOUR_PATH_TO_WORKING_DIR/vocab.dict\",\n",
    "        \"min_freq\": 2,\n",
    "        \"special_tokens\": [\"<PAD>\",\"<BOS>\", \"<EOS>\", \"<UNK>\"],\n",
    "        \"unk_token\": \"<UNK>\",\n",
    "        \"fit_on\": [\"x_tokens\", \"y_tokens\"],\n",
    "        \"in\": [\"x_tokens\"],\n",
    "        \"out\": [\"x_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"ref\": \"vocab\",\n",
    "        \"in\": [\"y_tokens\"],\n",
    "        \"out\": [\"y_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"sentence_padder\",\n",
    "        \"id\": \"padder\",\n",
    "        \"length_limit\": 20,\n",
    "        \"in\": [\"x_tokens_ids\"],\n",
    "        \"out\": [\"x_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"ref\": \"padder\",\n",
    "        \"in\": [\"y_tokens_ids\"],\n",
    "        \"out\": [\"y_tokens_ids\"]\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"seq2seq\",\n",
    "        \"id\": \"s2s\",\n",
    "        \"max_length\": \"#padder.length_limit+2\",\n",
    "        \"cell_size\": 250,\n",
    "        \"embeddings_dim\": 50,\n",
    "        \"vocab_size\": 11595,\n",
    "        \"keep_prob\": 0.8,\n",
    "        \"learning_rate\": 3e-04,\n",
    "        \"teacher_forcing_rate\": 0.0,\n",
    "        \"use_attention\": False,\n",
    "        \"save_path\": \"YOUR_PATH_TO_WORKING_DIR/model\",\n",
    "        \"load_path\": \"YOUR_PATH_TO_WORKING_DIR/model\",\n",
    "        \"in\": [\"x_tokens_ids\"],\n",
    "        \"in_y\": [\"y_tokens_ids\"],\n",
    "        \"out\": [\"y_predicted_tokens_ids\"],\n",
    "      },\n",
    "      {\n",
    "        \"ref\": \"vocab\",\n",
    "        \"in\": [\"y_predicted_tokens_ids\"],\n",
    "        \"out\": [\"y_predicted_tokens\"]\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"postprocessing\",\n",
    "        \"in\": [\"y_predicted_tokens\"],\n",
    "        \"out\": [\"y_predicted_tokens\"]\n",
    "      }\n",
    "    ],\n",
    "    \"out\": [\"y_predicted_tokens\"]\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"log_every_n_batches\": 100,\n",
    "    \"val_every_n_epochs\":0,\n",
    "    \"batch_size\": 64,\n",
    "    \"validation_patience\": 0,\n",
    "    \"epochs\": 20,\n",
    "    \"metrics\": [\"bleu\"],\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with model using config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.infer import build_model_from_config\n",
    "model = build_model_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(['Hi, how are you?', 'Any ideas my dear friend?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run experiments with and without attention, with teacher forcing and without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.train import train_evaluate_model_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(config, open('seq2seq.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_model_from_config('seq2seq.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_from_config(config)\n",
    "model(['hi, how are you?', 'any ideas my dear friend?', 'okay, i agree with you', 'good bye!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the model you can try to use multilayer (use MultiRNNCell) encoder and decoder, try to use attention with trainable parameters (not dot product scoring function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
