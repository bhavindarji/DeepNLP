{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 전처리 하기 위하여, 아래에서 사용할 라이브러리는 다음과 같다.\n",
    "\n",
    "import re # 정규표현식\n",
    "from bs4 import BeautifulSoup # \n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 데이터를 Pandas 라이브러리로 불러온다.\n",
    "DEFAULT_PATH ='~/.kaggle/competitions/word2vec-nlp-tutorial/'\n",
    "train = pd.read_csv(DEFAULT_PATH+\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(DEFAULT_PATH+\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터의 구조는 \"id\", \"sentiment\", \"review\"로 구분되어 있으며, \"review\"를 보고 \"sentiment\"를 판단하는 문장\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\\\\"The Classic War of the Worlds\\\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells\\' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\\\"critics\\\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\\\"critics\\\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells\\' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\\\"critics\\\\\" perceive to be its shortcomings.\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raw text는 다음과 같은 구조로 되어 있다. 내부에 html과 관련된 특수기호 및 문자 (html 태그) ]들이 들어가 있다.\n",
    "train.review[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing( review, remove_stopwords = False ): \n",
    "    # 불용어 제거는 옵션으로 선택 가능하다.\n",
    "    \n",
    "    # 1. HTML 태그 제거\n",
    "    review_text = BeautifulSoup(review, \"html5lib\").get_text()\t\n",
    "\t\n",
    "    # 2. 영어가 아닌 특수문자들을 공백(\" \")으로 바꾸기\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "\n",
    "    # 3. 대문자들을 소문자로 바꾸고 공백단위로 텍스트들 나눠서 리스트로 만든다.\n",
    "    words = review_text.lower().split()\n",
    "\t\n",
    "    if remove_stopwords: \n",
    "        # 4. 불용어들을 제거\n",
    "    \n",
    "        #영어에 관련된 불용어 불러오기\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        # 불용어가 아닌 단어들로 이루어진 새로운 리스트 생성\n",
    "        words = [w for w in words if not w in stops]\n",
    "        # 5. 단어 리스트를 공백을 넣어서 하나의 글로 합친다.\t\n",
    "        clean_review = ' '.join(words)\n",
    "    \t\n",
    "    else: # 불용어 제거하지 않을 때\n",
    "        clean_review = ' '.join(words)\n",
    "\n",
    "    return clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train데이터를 전처리\n",
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append(preprocessing(review, remove_stopwords = True))\n",
    " \n",
    "#전처리한 데이터의 첫 번째 데이터 출력\n",
    "clean_train_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watching time chasers obvious made bunch friends maybe sitting around one day film school said hey let pool money together make really bad movie something like ever said still ended making really bad movie dull story bad script lame acting poor cinematography bottom barrel stock music etc corners cut except one would prevented film release life like'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test데이터를 전처리\n",
    "clean_test_reviews = []\n",
    "for review in test['review']:\n",
    "    clean_test_reviews.append(preprocessing(review, remove_stopwords = True))\n",
    " \n",
    "#전처리한 데이터의 첫 번째 데이터 출력\n",
    "clean_test_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 리뷰 갯수:  25000\n"
     ]
    }
   ],
   "source": [
    "reviews = list(train['review'])\n",
    "sentiments = list(train['sentiment'])\n",
    "\n",
    "num_reviews = train[\"review\"].size\n",
    "print('전체 리뷰 갯수: ', num_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 단어 갯수:  123351\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_reviews+clean_test_reviews)\n",
    "\n",
    "#train\n",
    "train_text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n",
    "#test\n",
    "test_text_sequences = tokenizer.texts_to_sequences(clean_test_reviews)\n",
    "\n",
    "word_vocab = tokenizer.word_index\n",
    "print(\"전체 단어 갯수: \", len(word_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input train data tensor: (25000, 50)\n",
      "Shape of label train tensor: (25000,)\n",
      "Shape of input test data tensor: (50000, 50)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "train_inputs = pad_sequences(train_text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "train_labels = np.array(train['sentiment'])\n",
    "\n",
    "test_inputs = pad_sequences(test_text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "print('Shape of input train data tensor:', train_inputs.shape)\n",
    "print('Shape of label train tensor:', train_labels.shape)\n",
    "print('Shape of input test data tensor:', test_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR_PATH = './data/'\n",
    "INPUT_TRAIN_DATA_FILE_NAME = 'train_input.npy'\n",
    "LABEL_TRAIN_DATA_FILE_NAME = 'train_label.npy'\n",
    "INPUT_TEST_DATA_FILE_NAME = 'test_input.npy'\n",
    "\n",
    "DATA_CONFIGS_FILE_NAME = 'data_configs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(FILE_DIR_PATH):\n",
    "    os.makedirs(FILE_DIR_PATH)\n",
    "\n",
    "np.save(open(FILE_DIR_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'wb'), train_inputs)\n",
    "np.save(open(FILE_DIR_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'wb'), train_labels)\n",
    "np.save(open(FILE_DIR_PATH + INPUT_TEST_DATA_FILE_NAME, 'wb'), test_inputs)\n",
    "\n",
    "data_configs = word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab) # vocab size 추가\n",
    "\n",
    "import json\n",
    "\n",
    "with open(FILE_DIR_PATH + DATA_CONFIGS_FILE_NAME, 'w') as f:\n",
    "    json.dump(data_prepro_configs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n",
      "CPU times: user 46.6 s, sys: 187 ms, total: 46.8 s\n",
      "Wall time: 47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def clean_reviews(reviews, remove_stopwords=False, stem=False):\n",
    "    \"\"\"\n",
    "    to clean review strings\n",
    "    review: a list of review strings\n",
    "    remove_stop_words: whether to remove stop words\n",
    "    output: a list of clean reviews\n",
    "    \"\"\"\n",
    "    # 1. Remove HTML\n",
    "    reviews_text = list(map(lambda x: BeautifulSoup(x, 'html.parser').get_text(), reviews))\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    reviews_text = list(map(lambda x: re.sub(\"[^a-zA-Z]\",\" \", x), reviews_text))\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = list(map(lambda x: x.lower().split(), reviews_text))\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        set_of_stopwords = set(stopwords.words(\"english\"))\n",
    "        meaningful_words = list(map(lambda x: [w for w in x if not w in set_of_stopwords], words))\n",
    "    \n",
    "    # 5. Optionally stem the words\n",
    "    if stem:\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        stemmed_words = list(map(lambda x: [porter_stemmer.stem(w) for w in x], meaningful_words))\n",
    "        stemmed_words = list(map(lambda x:[wordnet_lemmatizer.lemmatize(w) for w in x], stemmed_words))\n",
    "    \n",
    "        # 6. Join the words to a single string\n",
    "        clean_review = map(lambda x: ' '.join(x), stemmed_words)\n",
    "    else:\n",
    "        clean_review = list(map(lambda x: ' '.join(x), meaningful_words))\n",
    "    \n",
    "    return clean_review\n",
    "\n",
    "clean_test_reviews = clean_reviews(list(train['review'].values), remove_stopwords=True)\n",
    "\n",
    "%%time\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # 1. HTML 태그 지우기\n",
    "    review_text = BeautifulSoup(raw_review, \"html5lib\").get_text()\n",
    "    \n",
    "    # 2. 영어가 아닌 특수문자들을 공백(\" \")으로 바꾸기\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 3. 대문자들을 소문자로 바꾸고 공백단위로 텍스트들을 나누기\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 4. 불용어들을 제거\n",
    "    filtered_sentence = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    # 5. 불용어가 아닌 텍스트를 공백 단위로 나누어서 리턴\n",
    "    return( \" \".join( filtered_sentence ))\n",
    "\n",
    "clean_train_reviews = list()\n",
    "for i, r in enumerate(reviews):\n",
    "    if((i+1)%1000 == 0):\n",
    "        print (\"Review %d of %d\\n\" % (i+1, num_reviews))\n",
    "    clean_train_reviews.append(review_to_words(r))\n",
    "    \n",
    "%%time\n",
    "\n",
    "def review_to_words( raw_review, stop_words = False ):\n",
    "    # 1. HTML 태그 지우기\n",
    "    reviews_text = list(map(lambda x: BeautifulSoup(x, 'html.parser').get_text(), reviews))\n",
    "    \n",
    "    # 2. 영어가 아닌 특수문자들을 공백(\" \")으로 바꾸기\n",
    "    reviews_text = list(map(lambda x: re.sub(\"[^a-zA-Z]\",\" \", x), reviews_text))\n",
    "    \n",
    "    # 3. 대문자들을 소문자로 바꾸고 공백단위로 텍스트들을 나누기\n",
    "    words = list(map(lambda x: x.lower().split(), reviews_text))\n",
    "    \n",
    "    if stop_words:\n",
    "        # 4. 불용어들을 제거 (Optional)\n",
    "        set_of_stopwords = set(stopwords.words(\"english\")) #영어에 관련된 불용어 불러오기\n",
    "        filtered_sentence = list(map(lambda x: [w for w in x if not w in set_of_stopwords], words))\n",
    "    \n",
    "        # 5. 불용어가 아닌 텍스트를 공백 단위로 나누어서 리턴    \n",
    "        clean_review = list(map(lambda x: ' '.join(x), filtered_sentence))\n",
    "        \n",
    "    else:\n",
    "        clean_review = list(map(lambda x: ' '.join(x), words))\n",
    "    \n",
    "    return clean_review\n",
    "\n",
    "clean_test_reviews = review_to_words(list(train['review'].values), stop_words = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
