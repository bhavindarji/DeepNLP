{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 카글 텍스트 분류 - 합성곱 신경망 활용 접근방법\n",
    "\n",
    "\n",
    "- 이번 장은 앞서 00장에서 간략하게 설명하였던 합성곱 신경망을 활용하여 텍스트 분류 문제를 풀어보고자 한다. 합성곱 신경망은 주로 이미지에서 특징을 추출하여 이미지 판단을 하는 역할로 큰 성능을 이루었는데, 텍스트에서도 좋은 효과를 낼 수 있다는 점을 Yoon Kim (2014) 박사가 \"Convolution Neural Network for Sentence Classificaion\" (http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf) 활용하여 입증하였다.\n",
    "\n",
    "<img src=\"./Fig/fig1-cnn_text_classification.png\"> \n",
    "\n",
    "**Fig1. Yoon Kim's Text Classification**\n",
    "\n",
    "- RNN이 단어의 입력의 순서를 중요하게 반영한다면, CNN은 문장의 지역 정보를 보존하면서 각 문장 성분의 등장 정보들을 학습에 반영하는 구조로 풀어가고 있습니다. 학습을 하면서 각 필터 사이즈를 조절하면서 언어의 특징 값을 추출하게 되는데, 기존의 N-gram (2 그램, 3 그램) 방식과 유사하다고 볼 수 있습니다.\n",
    "\n",
    "- 예를 들어 \"나는 배가 고프다\" 라는 문장을 2그램을 사용한다면, \"나 는 / 는 배 / 배 가 / 가 고프 / 고프 다/\" 로 각각 문장의 단어 성분을 쪼개어 활용 하는 접근방법을, 단어를 각 백터값을 투영하여 컨볼루션 필터값에 적용하는 원리입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코드 설명\n",
    "\n",
    " - 이전 내용에서 기본적인 Kaggle 연동 및 데이터 분석 및 전처리를 진행하여, 여기에서는 관련된 주제 위주로 설명을 하겠습니다. Full Code는 http://Kaggle/BagOfWordsMeetsBagsOfPopcorn/cnn_text_classification-bagofwords-book_ver.ipynb 를 참조하시기 바랍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# from tensorflow.python.keras.datasets import imdb\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "#전처리 lib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "FILE_DIR_PATH = './data/'\n",
    "INPUT_TRAIN_DATA_FILE_NAME = 'train_input.npy'\n",
    "LABEL_TRAIN_DATA_FILE_NAME = 'train_label.npy'\n",
    "INPUT_TEST_DATA_FILE_NAME = 'test_input.npy'\n",
    "\n",
    "DATA_CONFIGS_FILE_NAME = 'data_configs.json'\n",
    "\n",
    "train_input_data = np.load(open(FILE_DIR_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'rb'))\n",
    "train_label_data = np.load(open(FILE_DIR_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'rb'))\n",
    "test_input_data = np.load(open(FILE_DIR_PATH + INPUT_TEST_DATA_FILE_NAME, 'rb'))\n",
    "\n",
    "prepro_configs = None\n",
    "\n",
    "with open(FILE_DIR_PATH + DATA_CONFIGS_FILE_NAME, 'r') as f:\n",
    "    prepro_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SPLIT = 0.2\n",
    "RNG_SEED = 13371447\n",
    "\n",
    "input_train, input_eval, label_train, label_eval = train_test_split(train_input_data, train_label_data, test_size=TEST_SPLIT, random_state=RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size = len(prepro_configs)\n",
    "embedding_size = 128\n",
    "\n",
    "def mapping_fn(X, Y=None):\n",
    "    input, label = {'x': X}, Y\n",
    "    return input, label\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(input_train))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_eval, label_eval))\n",
    "    dataset = dataset.shuffle(buffer_size=len(input_eval))\n",
    "    dataset = dataset.batch(16)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_len_train = np.array([min(len(x), sentence_size) for x in x_train_variable])\n",
    "# x_len_test = np.array([min(len(x), sentence_size) for x in x_test_variable])\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    #embedding layer를 선언합니다.\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "                    features['x'],\n",
    "                    vocab_size,\n",
    "                    embedding_size,\n",
    "                    initializer=params['embedding_initializer']\n",
    "                    )\n",
    "    # 현재 모델이 학습모드인지 여부를 확인하는 변수입니다.\n",
    "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    # embedding layer에 대한 output에 대해 dropout을 취합니다.\n",
    "    dropout_emb = tf.layers.dropout(inputs=input_layer,\n",
    "                                   rate=0.5,\n",
    "                                   training=training)\n",
    "\n",
    "    ## filters = 32이고 kernel_size = 3이면, 길이가 3인 32개의 다른 필터를 생성합니다. 32개의 컨볼루션들을 생성합니다.\n",
    "    ## conv1d는 (배치사이즈, 길이, 채널)로 입력값을 받는데, 배치사이즈: 문장 숫자 | 길이: 각 문장의 단어의 개수 | 채널: 임베딩 출력 차원수임\n",
    "    conv = tf.layers.conv1d(\n",
    "            inputs=dropout_emb,\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)\n",
    "    dropout_hidden = tf.layers.dropout(inputs=hidden, rate=0.2, training=training)\n",
    "#     logits = tf.layers.dense(inputs=dropout_hidden, units=1, name='result')\n",
    "    logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n",
    "    \n",
    "    #prediction 진행 시, None\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss = loss)\n",
    "    \n",
    "    elif EVAL:\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "        pred = tf.nn.sigmoid(logits)\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc': accuracy})\n",
    "        \n",
    "    elif PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'prob': tf.nn.sigmoid(logits),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 2, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1179204a8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "params = {'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0)}\n",
    "\n",
    "model_dir = os.path.join(os.getcwd(), \"checkpoint/cnn_model\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "config_tf = tf.estimator.RunConfig()\n",
    "config_tf._save_checkpoints_steps = 100\n",
    "config_tf._save_checkpoints_secs = None\n",
    "config_tf._keep_checkpoint_max =  2\n",
    "config_tf._log_step_count_steps = 100\n",
    "\n",
    "est = tf.estimator.Estimator(model_fn, model_dir=model_dir, config=config_tf, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n",
      "Experiment started at 04:41:32\n",
      ".......................................\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.7878452, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.466\n",
      "INFO:tensorflow:loss = 0.7050733, step = 101 (9.556 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.4889\n",
      "INFO:tensorflow:loss = 0.6866376, step = 201 (8.704 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.0016\n",
      "INFO:tensorflow:loss = 0.64270514, step = 301 (9.089 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.3906\n",
      "INFO:tensorflow:loss = 0.646101, step = 401 (8.779 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.3459\n",
      "INFO:tensorflow:loss = 0.61298263, step = 501 (8.814 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 600 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.1644\n",
      "INFO:tensorflow:loss = 0.6897214, step = 601 (8.957 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 700 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.2219\n",
      "INFO:tensorflow:loss = 0.5841862, step = 701 (8.911 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 800 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.4241\n",
      "INFO:tensorflow:loss = 0.5066953, step = 801 (8.753 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 900 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.9814\n",
      "INFO:tensorflow:loss = 0.3330156, step = 901 (9.106 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.106\n",
      "INFO:tensorflow:loss = 0.37358075, step = 1001 (9.004 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1100 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.1317\n",
      "INFO:tensorflow:loss = 0.3837756, step = 1101 (8.984 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1200 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.3688\n",
      "INFO:tensorflow:loss = 0.39939052, step = 1201 (8.796 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1300 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.923\n",
      "INFO:tensorflow:loss = 0.3898734, step = 1301 (8.387 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1400 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.9891\n",
      "INFO:tensorflow:loss = 0.3014354, step = 1401 (9.100 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1500 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.3903\n",
      "INFO:tensorflow:loss = 0.37168205, step = 1501 (8.779 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1600 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.9639\n",
      "INFO:tensorflow:loss = 0.22471777, step = 1601 (9.122 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1700 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.8616\n",
      "INFO:tensorflow:loss = 0.39880854, step = 1701 (9.206 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1800 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 12.0021\n",
      "INFO:tensorflow:loss = 0.25351852, step = 1801 (8.332 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1900 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.3653\n",
      "INFO:tensorflow:loss = 0.26538596, step = 1901 (8.799 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.0664\n",
      "INFO:tensorflow:loss = 0.22154477, step = 2001 (9.036 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2100 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.2225\n",
      "INFO:tensorflow:loss = 0.38530907, step = 2101 (8.910 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2200 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.9942\n",
      "INFO:tensorflow:loss = 0.45051515, step = 2201 (9.096 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2300 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.5501\n",
      "INFO:tensorflow:loss = 0.24444747, step = 2301 (8.658 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2400 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.3962\n",
      "INFO:tensorflow:loss = 0.31822956, step = 2401 (8.775 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2500 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.2496\n",
      "INFO:tensorflow:loss = 0.2455531, step = 2501 (8.889 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2600 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.2175\n",
      "INFO:tensorflow:loss = 0.22207914, step = 2601 (8.914 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2700 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.4256\n",
      "INFO:tensorflow:loss = 0.27604103, step = 2701 (8.752 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2800 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.6517\n",
      "INFO:tensorflow:loss = 0.36141852, step = 2801 (8.582 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2900 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.0706\n",
      "INFO:tensorflow:loss = 0.19083075, step = 2901 (9.033 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.1514\n",
      "INFO:tensorflow:loss = 0.24506514, step = 3001 (8.968 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3100 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 11.7441\n",
      "INFO:tensorflow:loss = 0.32116035, step = 3101 (8.515 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3130 into /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.34330332.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-02-04:46:15\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt-3130\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-02-04:46:15\n",
      "INFO:tensorflow:Saving dict for global step 3130: acc = 0.8322, global_step = 3130, loss = 0.39899728\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3130: /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/model.ckpt-3130\n",
      ".......................................\n",
      "Experiment finished at 04:46:15\n",
      "\n",
      "Experiment elapsed time: 283.420402 seconds\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)\n",
    "time_start = datetime.utcnow()\n",
    "print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\") \n",
    "\n",
    "est.train(train_input_fn, steps=20000)\n",
    "est.evaluate(eval_input_fn)\n",
    "\n",
    "time_end = datetime.utcnow()\n",
    "print(\".......................................\")\n",
    "print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find trained model in model_dir: /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/cnn, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# 예측된 모델을 불러 체크포인트로 결과치를 불러온다.\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\":test_input_data}, shuffle=False)\n",
    "cnn_classifier.predict(input_fn=predict_input_fn)\n",
    "\n",
    "predictions = np.array([p['logits'][0] for p in cnn_classifier.predict(input_fn=predict_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find trained model in model_dir: /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/cnn, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "test dataset shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# 예측된 모델을 불러 체크포인트로 결과치를 불러온다.\n",
    "test_input_data = np.load(open(FILE_DIR_PATH + INPUT_TEST_DATA_FILE_NAME, 'rb')) #테스트데이터 로드\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\":test_input_data}, shuffle=False) #numpy 형태로 저장\n",
    "cnn_classifier.predict(input_fn=predict_input_fn)\n",
    "\n",
    "predictions = np.array([p['logits'][0] for p in cnn_classifier.predict(input_fn=predict_input_fn)])\n",
    "\n",
    "DEFAULT_PATH ='~/.kaggle/competitions/word2vec-nlp-tutorial/'\n",
    "    \n",
    "#테스트 데이터 로드\n",
    "test = pd.read_csv(DEFAULT_PATH+\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "print (\"test dataset shape: {}\".format(test.shape))\n",
    "    \n",
    "    \n",
    "#알아보기 쉽게 데이터랑 붙여두는 편이 좋을 거 같습니다.\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":list(predictions)} )\n",
    "\n",
    "#지금까지 처리한 결과를 파일로 저장합니다.\n",
    "output.to_csv( \"Bag_of_Words_model_test.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(sentences):\n",
    "    indexes = [text_to_index(sentence) for sentence in sentences]\n",
    "    x = sequence.pad_sequences(indexes, \n",
    "                               maxlen=sentence_size, \n",
    "                               truncating='post',\n",
    "                               padding='post',\n",
    "                               value=pad_id)\n",
    "    length = np.array([min(len(x), sentence_size) for x in indexes])\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": x, \"len\": length}, shuffle=False)\n",
    "    predictions = {}\n",
    "    for path, classifier in all_classifiers.items():\n",
    "        predictions[path] = [p['logistic'][0] for p in classifier.predict(input_fn=predict_input_fn)]\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        print(sentence)\n",
    "        for path in all_classifiers:\n",
    "            print(\"\\t{} {}\".format(path, predictions[path][idx]))\n",
    "#             predictions[path][idx]\n",
    "    \n",
    "    return predictions[path][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-dd869d8851b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Current Progress %d \\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msentimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclean_test_reviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#알아보기 쉽게 데이터랑 붙여두는 편이 좋을 거 같습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-aa33724cb1ac>\u001b[0m in \u001b[0;36mprint_predictions\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     x = sequence.pad_sequences(indexes, \n\u001b[1;32m     12\u001b[0m                                \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-aa33724cb1ac>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     x = sequence.pad_sequences(indexes, \n\u001b[1;32m     12\u001b[0m                                \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-aa33724cb1ac>\u001b[0m in \u001b[0;36mtext_to_index\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moov_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-aa33724cb1ac>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moov_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "# 예측된 모델을 불러 체크포인트로 결과치를 불러온다.\n",
    "\n",
    "sentimental = []\n",
    "\n",
    "for i in range(len(clean_test_reviews)):\n",
    "    if ( (i+1) % 1000 == 0):\n",
    "        print (\"Current Progress %d \\n\" % (i+1))\n",
    "    sentimental.append(print_predictions([clean_test_reviews[i]]))\n",
    "    \n",
    "#알아보기 쉽게 데이터랑 붙여두는 편이 좋을 거 같습니다.\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":sentimental} )\n",
    "\n",
    "#지금까지 처리한 결과를 파일로 저장합니다.\n",
    "output.to_csv( \"Bag_of_Words_model_test.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이전 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input function\n",
    "\n",
    "#각 문장의 길이를 계산한다, max 길이는 200\n",
    "\n",
    "x_len_train = np.array([min(len(x), sentence_size) for x in x_train_variable])\n",
    "x_len_test = np.array([min(len(x), sentence_size) for x in x_test_variable])\n",
    "\n",
    "def parser(x, length, y):\n",
    "    features = {\"x\": x, \"len\": length}\n",
    "    return features, y\n",
    "\n",
    "#len을 활용하여 기존 전처리 이후의 길이를 보존\n",
    "#from_tensor_slices를 활용하면 numpy 데이터 구조에서 쉽게 변환\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(x_train_variable))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Classification\n",
    "\n",
    "CNN을 활용하여 text를 분류해보자, n-gram의 효과로 활용\n",
    "\n",
    "https://www.semanticscholar.org/paper/Learning-to-Rank-Short-Text-Pairs-with-Deep-Neural-Severyn-Moschitti/452f7411af7d471dd3ba84c2b06b2aaffc38cdb9\n",
    "\n",
    "Embedding Layer -> Dropout -> Conv1D -> GlobalMax1D -> Hidden Dense Layer -> Dropout -> Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = {}\n",
    "\n",
    "def train_and_evaluate(classifier):\n",
    "    # 예측 테스트를 위해 모델을 학습시키고 저장한다.\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps=1)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])\n",
    "    \n",
    "    # name scopes의 재사용을 위해 graph를 reset한다.\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool),\n",
    "                             num_thresholds=21)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step=0)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/cnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11776c978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#head: pre-made estimator로 평가를 할 때, 일정한 함수를 사용하게 세팅\n",
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    #embedding layer를 선언한다.\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "                    features['x'],\n",
    "                    vocab_size,\n",
    "                    embedding_size,\n",
    "                    initializer=params['embedding_initializer']\n",
    "                    )\n",
    "\n",
    "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    dropout_emb = tf.layers.dropout(inputs=input_layer,\n",
    "                                   rate=0.2,\n",
    "                                   training=training)\n",
    "\n",
    "    conv = tf.layers.conv1d(\n",
    "            inputs=dropout_emb,\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)  \n",
    "    dropout_hidden = tf.layers.dropout(inputs=hidden, rate=0.2, training=training)\n",
    "    logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n",
    "    \n",
    "    #prediction 진행 시, None\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer() #여러가지 Optimizer 활용가능\n",
    "    \n",
    "    def _train_op_fn(loss):\n",
    "#         tf.summary('loss', loss)\n",
    "        return optimizer.minimize(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "\n",
    "    \n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        mode=mode,\n",
    "        logits=logits,\n",
    "        train_op_fn=_train_op_fn)\n",
    "\n",
    "\n",
    "cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                        model_dir=os.path.join(model_dir, 'cnn'),\n",
    "                                        params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_and_evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b31a89aa73fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#학습 후, 결과치를 tensorboard로 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# tensorboard --logdir=./checkpoint/cnn_classifier/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_and_evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "#학습 후, 결과치를 tensorboard로 확인\n",
    "# tensorboard --logdir=./checkpoint/cnn_classifier/\n",
    "train_and_evaluate(cnn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-72e06e092849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logistic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcnn_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = np.array([p['logistic'][0] for p in cnn_classifier.predict(input_fn=eval_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 prediction으로 테스트 해 본다\n",
    "\n",
    "def text_to_index(sentence):\n",
    "    # Remove punctuation characters except for the apostrophe\n",
    "    translator = str.maketrans('', '', string.punctuation.replace(\"'\", ''))\n",
    "    tokens = sentence.translate(translator).lower().split()\n",
    "    return np.array([1] + [word_index[t] if t in word_index else oov_id for t in tokens])\n",
    "\n",
    "def print_predictions(sentences):\n",
    "    indexes = [text_to_index(sentence) for sentence in sentences]\n",
    "    x = sequence.pad_sequences(indexes, \n",
    "                               maxlen=sentence_size, \n",
    "                               truncating='post',\n",
    "                               padding='post',\n",
    "                               value=pad_id)\n",
    "    length = np.array([min(len(x), sentence_size) for x in indexes])\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": x, \"len\": length}, shuffle=False)\n",
    "    predictions = {}\n",
    "    for path, classifier in all_classifiers.items():\n",
    "        predictions[path] = [p['logistic'][0] for p in classifier.predict(input_fn=predict_input_fn)]\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        print(sentence)\n",
    "        for path in all_classifiers:\n",
    "            print(\"\\t{} {}\".format(path, predictions[path][idx]))\n",
    "#             predictions[path][idx]\n",
    "    \n",
    "    return predictions[path][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions([\n",
    "    'I do not like this movie'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions(['fuck you', 'this movie sucks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 했던 것을 모두 활용하여 제출용 데이터를 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "default_path = '/Users/user/.kaggle/competitions/word2vec-nlp-tutorial/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 데이터 로드\n",
    "test = pd.read_csv(default_path+\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "print (\"test dataset shape: {}\".format(test.shape))\n",
    "\n",
    "# 불용어 제거 및 태그를 삭제 후, 데이터를 저장할 장소를 만들자\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = []\n",
    "\n",
    "print (\"테스트 영화 리뷰 전처리 진행...\\n\")\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1) % 1000 == 0 ):\n",
    "        print (\"Review %d of %d\\n\" % (i+1, num_reviews))\n",
    "    clean_review = review_to_words( test[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 파일은 이렇게 생겼다고 합니다.\n",
    "print (test.head())\n",
    "\n",
    "#이 파일은 \"sentiment\" 행이 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측된 모델을 불러 체크포인트로 결과치를 불러온다.\n",
    "\n",
    "sentimental = []\n",
    "\n",
    "for i in range(len(clean_test_reviews)):\n",
    "    if ( (i+1) % 1000 == 0):\n",
    "        print (\"Current Progress %d \\n\" % (i+1))\n",
    "    sentimental.append(print_predictions([clean_test_reviews[i]]))\n",
    "    \n",
    "#알아보기 쉽게 데이터랑 붙여두는 편이 좋을 거 같습니다.\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":sentimental} )\n",
    "\n",
    "#지금까지 처리한 결과를 파일로 저장합니다.\n",
    "output.to_csv( \"Bag_of_Words_model_test.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#알아보기 쉽게 데이터랑 붙여두는 편이 좋을 거 같습니다.\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":final_result} )\n",
    "\n",
    "#결과값 저장\n",
    "output.to_csv( \"final_bof.csv\", index=False, quoting=3 )'\n",
    "\n",
    "#0.5 기준으로 값들을 변환\n",
    "\n",
    "def correct_val(x):\n",
    "    if x >= 0.5:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 0\n",
    "    \n",
    "    return x\n",
    "\n",
    "final_result = output['sentiment'].apply(correct_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
