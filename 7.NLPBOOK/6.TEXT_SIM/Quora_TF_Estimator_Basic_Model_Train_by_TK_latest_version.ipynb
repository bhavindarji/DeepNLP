{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
    "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
    "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
    "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_data = np.load(open(Q1_TRAINING_DATA_FILE, 'rb'))\n",
    "q2_data = np.load(open(Q2_TRAINING_DATA_FILE, 'rb'))\n",
    "labels = np.load(open(LABEL_TRAINING_DATA_FILE, 'rb'))\n",
    "prepro_configs = None\n",
    "\n",
    "with open(NB_WORDS_DATA_FILE, 'r') as f:\n",
    "    prepro_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange(base, hypothesis, label):\n",
    "    features = {\"base\": base, \"hypothesis\": hypothesis}\n",
    "    return features, label\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((Q1_train, Q2_train, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.batch(16)\n",
    "    dataset = dataset.map(rearrange)\n",
    "#     dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((Q1_test, Q2_test, y_test))\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.batch(16)\n",
    "    dataset = dataset.map(rearrange)\n",
    "#     dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = prepro_configs['vocab_size']\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "CONV_FEATURE_DIM = 300\n",
    "CONV_OUTPUT_DIM = 128\n",
    "CONV_WINDOW_SIZE = 3\n",
    "\n",
    "SIMILARITY_DENSE_FEATURE_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = VOCAB_SIZE\n",
    "embedding_size = WORD_EMBEDDING_DIM\n",
    "conv_channel_size = CONV_FEATURE_DIM\n",
    "conv_window_size = CONV_WINDOW_SIZE\n",
    "conv_output_feature_size = CONV_OUTPUT_DIM\n",
    "max_pool_window_size = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "similairiry_dense_dim = SIMILARITY_DENSE_FEATURE_DIM\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    word_embeddings = tf.get_variable('word_embeddings', [vocabulary_size, embedding_size])\n",
    "    \n",
    "    def basic_conv_sementic_network(inputs, name, reuse=False):\n",
    "        conv_layer = tf.layers.conv1d(inputs, conv_channel_size, conv_window_size, \n",
    "                                      activation=tf.nn.relu, \n",
    "                                      name=name + 'conv_1d',\n",
    "                                      padding='same', \n",
    "                                      reuse=reuse)\n",
    "        max_pool_layer = tf.layers.max_pooling1d(conv_layer, max_pool_window_size, 1)\n",
    "        output_layer = tf.layers.dense(max_pool_layer, conv_output_feature_size, \n",
    "                                        activation=tf.nn.relu,\n",
    "                                        reuse=reuse,\n",
    "                                        name=name + 'dense')\n",
    "        output_layer = tf.squeeze(output_layer, 1)\n",
    "        \n",
    "        return output_layer\n",
    "    \n",
    "    base_embedded_matrix = tf.nn.embedding_lookup(word_embeddings, features['base'])\n",
    "    hypothesis_embedded_matrix = tf.nn.embedding_lookup(word_embeddings, features['hypothesis'])\n",
    "    \n",
    "    base_sementic_matrix = basic_conv_sementic_network(base_embedded_matrix, 'base')\n",
    "    hypothesis_sementic_matrix = basic_conv_sementic_network(hypothesis_embedded_matrix, 'hypothesis')\n",
    "    \n",
    "    merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1)\n",
    "    #norm_merged_matrix = tf.layers.batch_normalization(merged_matrix)\n",
    "    similarity_dense_layer = tf.layers.dense(merged_matrix, similairiry_dense_dim,\n",
    "                                             activation=tf.nn.relu)\n",
    "    \n",
    "    logit_layer = tf.layers.dense(similarity_dense_layer, 1)\n",
    "    logit_layer = tf.squeeze(logit_layer, 1)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  predictions={\n",
    "                      'prob':tf.nn.sigmoid(logit_layer)\n",
    "                  })\n",
    "            \n",
    "    loss =tf.losses.sigmoid_cross_entropy(labels, logit_layer)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.nn.sigmoid(logit_layer))\n",
    "        eval_metric_ops = {'acc': accuracy}\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  eval_metric_ops= eval_metric_ops,\n",
    "                  loss=loss)\n",
    "\n",
    "    global_step = tf.train.get_global_step()\n",
    "    #loss = tf.losses.mean_squared_error(labels, logit_layer)\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "\n",
    "    accuracy = tf.metrics.accuracy(labels, logit_layer)\n",
    "    eval_metric_ops = {'acc': accuracy}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "              mode=mode,\n",
    "              train_op=train_op,\n",
    "              loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'train_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc4ef9a6828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "est = tf.estimator.Estimator(model_fn, model_dir='train_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.69315964, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 100 into train_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.71294993.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7fc4ef9a6630>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.train(train_input_fn, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-17-03:39:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train_model/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-17-03:39:42\n",
      "INFO:tensorflow:Saving dict for global step 100: acc = 0.63194734, global_step = 100, loss = 0.6421221\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: train_model/model.ckpt-100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.63194734, 'loss': 0.6421221, 'global_step': 100}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR_PATH = './'\n",
    "Q1_TEST_DATA_FILE = 'q1_test.npy'\n",
    "Q2_TEST_DATA_FILE = 'q2_test.npy'\n",
    "\n",
    "test_q1_data = np.load(open(FILE_DIR_PATH + Q1_TEST_DATA_FILE, 'rb'))\n",
    "test_q2_data = np.load(open(FILE_DIR_PATH + Q2_TEST_DATA_FILE, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"base\":test_q1_data, \n",
    "                                                         \"hypothesis\":test_q2_data}, \n",
    "                                                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train_model/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array([p['prob'] for p in est.predict(input_fn=\n",
    "predict_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3563466"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mind/anaconda3/envs/torch_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DEFAULT_PATH='~/.kaggle/competitions/quora-question-pairs/'\n",
    "\n",
    "test = pd.read_csv(DEFAULT_PATH + \"test.csv\", encoding='utf-8')\n",
    "test = test.dropna() #drop empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_id                                          question1  \\\n",
       "0       0  How does the Surface Pro himself 4 compare wit...   \n",
       "1       1  Should I have a hair transplant at age 24? How...   \n",
       "2       2  What but is the best way to send money from Ch...   \n",
       "3       3                        Which food not emulsifiers?   \n",
       "4       4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  \n",
       "1        How much cost does hair transplant require?  \n",
       "2                      What you send money to China?  \n",
       "3                                  What foods fibre?  \n",
       "4                     How their can I start reading?  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3563466"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(test[\"test_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame( data={\"test_id\":test[\"test_id\"], \"is_duplicate\": list(predictions)} )\n",
    "output.to_csv( \"cnn_predict.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
