{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF.Data \n",
    "\n",
    " - 머신러닝에서 가장 많이 시간을 소요하는 것은 데이터를 어떻게 다루느냐이다. 일반적으로 데이터를 분석, 전처리, 파이프라인 만드는 과정에 머신러닝 프로젝트의 70~80% 시간을 소비한다 해도 과언이 아니다. 텐서플로우에서는 Dataset API를 활용하여 단순히 연구자 뿐문아니라 서비스화를 위해 끊임없이 개선하고 있다. 비동기성과 최적화를 통한 빠른 데이터 처리로 GPU가 병목현상 없이 효율적으로 활용 될 수 있게 해주며, 이미지, 텍스트 뿐만 아니라 넘파이 (Numpy), 판다스 (Pandas) 데이터 구조가 활용이 가능하도록 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF.data for pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from sklearn.preprocessing import LabelEncoder # 레이블을 원핫 인코딩을 통해 진행하도록 한다.\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#아이리스 데이터셋을 다운로드 한다.\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "filename = \"raw.csv\"\n",
    "open(filename, 'wb').write(r.content)\n",
    "\n",
    "#다운로드 받은 데이터셋을 판다스로 저장한다.\n",
    "dataset = pd.read_csv('raw.csv', header=None, names=['sepal_length','sepal_width','petal_length','petal_width','species'])\n",
    "dataset.head() # .head() 기능을 활용하여 Top 5를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow의 feature graph형태로 변환한다\n",
    "numeric_columns = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "numeric_features = [tf.feature_column.numeric_column(key = column) for column in numeric_columns]\n",
    "train_df = dataset[numeric_columns]\n",
    "\n",
    "# Categorical String 데이터를 정수형으로 변환한다.\n",
    "label_df = dataset['species']\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(label_df)\n",
    "labels = pd.DataFrame(data=integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_input_fn(x_df, y_df):\n",
    "    #판다스 데이터를 입력으로 사용하는 함수이다. 배치 크기, 반복 횟수, 데이터를 섞는 등의 기능을 할 수 있다.\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x = x_df,\n",
    "        y = y_df,\n",
    "        batch_size = 16,\n",
    "        num_epochs = None,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "training_input_fn = pd_input_fn(train_df, labels)\n",
    "# 선형 분류기를 활용하여 학습의 진행을 테스트 해 본다.\n",
    "linear_classifier = tf.estimator.LinearClassifier(feature_columns=numeric_features, n_classes=3)\n",
    "linear_classifier.train(training_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/cp/9zlspqj13_j36gnd9q2yp0180000gn/T/tmprfgwjip0\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/cp/9zlspqj13_j36gnd9q2yp0180000gn/T/tmprfgwjip0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12312b2b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/cp/9zlspqj13_j36gnd9q2yp0180000gn/T/tmprfgwjip0/model.ckpt.\n",
      "INFO:tensorflow:loss = 17.577797, step = 1\n",
      "INFO:tensorflow:global_step/sec: 403.794\n",
      "INFO:tensorflow:loss = 6.355757, step = 101 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 573.891\n",
      "INFO:tensorflow:loss = 4.838097, step = 201 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 552.541\n",
      "INFO:tensorflow:loss = 4.179614, step = 301 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 607.936\n",
      "INFO:tensorflow:loss = 4.8892717, step = 401 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 522.122\n",
      "INFO:tensorflow:loss = 3.55782, step = 501 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 530.943\n",
      "INFO:tensorflow:loss = 5.812096, step = 601 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 557.497\n",
      "INFO:tensorflow:loss = 4.3543167, step = 701 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 534.997\n",
      "INFO:tensorflow:loss = 2.9101386, step = 801 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 581.263\n",
      "INFO:tensorflow:loss = 3.55812, step = 901 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 554.496\n",
      "INFO:tensorflow:loss = 4.237404, step = 1001 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 600.219\n",
      "INFO:tensorflow:loss = 3.474036, step = 1101 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 566.352\n",
      "INFO:tensorflow:loss = 3.1276593, step = 1201 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 552.431\n",
      "INFO:tensorflow:loss = 1.762866, step = 1301 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 560.12\n",
      "INFO:tensorflow:loss = 1.4099519, step = 1401 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 601.244\n",
      "INFO:tensorflow:loss = 0.8681412, step = 1501 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 574.36\n",
      "INFO:tensorflow:loss = 2.0043855, step = 1601 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 585.857\n",
      "INFO:tensorflow:loss = 3.3182561, step = 1701 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 524.504\n",
      "INFO:tensorflow:loss = 1.4395477, step = 1801 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 511.481\n",
      "INFO:tensorflow:loss = 1.0576636, step = 1901 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 564.86\n",
      "INFO:tensorflow:loss = 2.4755673, step = 2001 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 563.183\n",
      "INFO:tensorflow:loss = 2.7079692, step = 2101 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 526.977\n",
      "INFO:tensorflow:loss = 1.9295304, step = 2201 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 579.612\n",
      "INFO:tensorflow:loss = 1.8550255, step = 2301 (0.174 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6e8b137560a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtraining_input_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlinear_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    841\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    857\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m    858\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1057\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    565\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1044\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18_py_3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Txt file\n",
    "dataset = tf.data.TextLineDataset(\"./data/abstracts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TextLineDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset은 단순히 Tensorflow의 Graph중 하나이기 때문에, Graph를 initialize 시켜야한다. 어떻게 보면 복잡한것 같지만, 데이터를 어떻게 넣을지,\n",
    "# 고민 안해도 된다.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#텍스트 데이터 불러오기\n",
    "dataset = tf.data.TextLineDataset(\"./data/dataset_test.txt\")\n",
    "\n",
    "#일반적으로 데이터셋을 불러오고 기존처럼 입력을 하면 오류가 나나다.\n",
    "# for line in dataset:\n",
    "#     print(line)\n",
    "\n",
    "# dataset을 사용하면 텐서플로우 기존에 Graph형식으로 적용하는 방식이다. 따라서, Graph를 초기화하고, node를 추가해야함.\n",
    "\n",
    "#one_shot_iterator: iterator를 생성하고, dataset을 반복한다. 또한, 데이터셋 yield과정이 끝나면 업데이트를 한다.\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(2):\n",
    "        print(sess.run(next_element))\n",
    "        \n",
    "# 데이터셋의 기능을 활용하여 텍스트 데이터를 다뤄보자\n",
    "dataset = dataset.map(lambda string: tf.string_split([string]).values) #Space 별 쪼개기\n",
    "dataset = dataset.shuffle(buffer_size=3) #3 by 3 elements를 불러서 각 상황마다 반복을 한다.\n",
    "dataset = dataset.batch(1) #배치를 생성한다.\n",
    "dataset = dataset.prefetch(1) #prefech 기능인데, 항상 불러오기 전에 배치 1개를 대기 시키는 기능이다.\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_element))\n",
    "    \n",
    "# iterators를 초기화하는 이유는 무엇인가?\n",
    "dataset = tf.data.TextLineDataset(\"./data/dataset_test.txt\")\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "init_op = iterator.initializer # 여러번의 epoch를 진행 가능함\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(next_element))\n",
    "    print(sess.run(next_element))\n",
    "    \n",
    "    # iterator를 처음으로 돌려서 첫 문장이 다시한번 생성되게 하는 과정이다.\n",
    "    # tf.Estimator는 초기화를 안해도 되지만, 학습을 할 때나, 평가를 할 때 weights를 다시 불러오게 되는 cost가 든다.\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(next_element))\n",
    "    \n",
    "# Text Pipeline을 제작하여 보자\n",
    "\n",
    "sentences = tf.data.TextLineDataset(\"./data/sentences.txt\")\n",
    "labels = tf.data.TextLineDataset(\"./data/labels.txt\")\n",
    "\n",
    "# Zip을 사용하여 iterate를 진행하자\n",
    "dataset = tf.data.Dataset.zip((sentences, labels))\n",
    "\n",
    "# oneshot iterator를 활용하여 zipped 데이터셋을 진행하자\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(1):\n",
    "        print(sess.run(dataset))\n",
    "        \n",
    "# Look up table을 제작해보자\n",
    "\n",
    "words = tf.contrib.lookup.index_table_from_file(\"data/words.txt\", num_oov_buckets=1)\n",
    "tags = tf.contrib.lookup.index_table_from_file(\"data/tags.txt\")\n",
    "\n",
    "# Padding batch를 만들어서 각 ids에 맞는 길이로 학습하자\n",
    "padded_shapes = (tf.TensorShape([None]), #문장 사이즈\n",
    "                tf.TensorShape([None])) # 레이블 사이즈\n",
    "\n",
    "padding_values = (params.id_pad_word,\n",
    "                 params.id_pad_tag)\n",
    "\n",
    "# padding_values는 단어이어만 하는데, \"<pad>\"와 같은 id를 제거하고 진행해야한다.\n",
    "# tf.data에는 패딩 기능도 있어서 패딩을 참조하여 진행하면 된다.\n",
    "dataset = (dataset\n",
    "          .shuffle(buffer_size=3)\n",
    "          .padded_batch(32, padded_shapes=padded_shapes, padding_values=padding_values))\n",
    "\n",
    "# Prediction 때는 패딩을 제외해야 하는데, dynamric_rnn이나 tf.sequence_mask를 활용하면 된다.\n",
    "sentences = sentences.map(lambda tokens: (vocab.lookup(tokens), tf.sizes(tokens)))\n",
    "\n",
    "\n",
    "\n",
    "# GPU는 데이터를 학습 시킬때만 활용하는 방향으로 진행한다. CPU는 전처리로 활용하여 극대화\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    dataset = ..?\n",
    "    \n",
    "# shuffle과 repeaet을 동시에 사용함으로써, 최적화\n",
    "tf.contrib.data.shuffle_and_repeat()\n",
    "\n",
    "# 병렬처리 하는법?\n",
    "# 병렬처리를 진행하여 데이터를 전처리 할 때, 한번에 진행하는 기능으로 속도를 빠르게 적용\n",
    "num_threads = 4\n",
    "\n",
    "# Prefectch 데이터 (미리 데이터를 올리는 법)\n",
    "# GPU가 백프로퍼게이션을 진행 하는 동시에 CPU가 처리를 하는 과정으로 바꾸자\n",
    "# 이 기능을 사용하면 서로의 사용이 극대화 됨\n",
    "# dataset.prefetch(1)을 마지막 파이프라인에 넣어주기만 하면 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF.Data\n",
    "\n",
    "## 머신러닝의 시작은 결국 데이터를 어떻게 다루느냐가 Key 요소 중 하나.\n",
    "\n",
    "## 단순히 연구자 뿐만 아니라, 상용화 (Product)를 위해서 끊임없이 개선하고 있음.\n",
    "\n",
    "### **The tf.data mission**\n",
    "\n",
    "Input piplines for Tensorflow should be:\n",
    "\n",
    "**Fast** : to keep up with GPUs and TPUs\n",
    "\n",
    "**Flexible** : to handle diverse data sources and use cases\n",
    "\n",
    "**Easy to use** : to democratize machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Transform Load for Tensorflow\n",
    "\n",
    "#### data pipeline을 구축함으로써, 학습 데이터 준비와 학습 시간 최적화 - 1\n",
    "\n",
    "<img src=\"tf.data1.png\">\n",
    "\n",
    "```python\n",
    "#Extract\n",
    "files = tf.data.Dataset.list_files(file_pattern)\n",
    "dataset = tf.data.TFRecordDataset(files)\n",
    "\n",
    "#Transform\n",
    "dataset = dataset.shuffle(10000)\n",
    "dataset = dataset.repeat(NUM_EPOCHS)\n",
    "dataset = dataset.map(lambda x: tf.parse_single_example(x, features))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "#Load\n",
    "iterator = dataset.make_one_shot_iterator() #Sequencial Access\n",
    "features = iterator.get_next()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "- CNN benchmarks reach > **13,000 images/second** with tf.data -> 8달 전과 비교하여 성능 2배 성장 (DGX - Imagenet)\n",
    " <br />\n",
    "- 텐서플로우 벤치마크 프로젝트를 사용하여 활용: www.tensorflow.org/performance/datasets_performance\n",
    "\n",
    "- New tf.contrib.data.prefetch_to_device() for GPUs tf 1.8 (tf-nightly에 보유)\n",
    "\n",
    "- https://www.tensorflow.org/versions/master/performance/datasets_performance\n",
    "\n",
    "#### parallel 기능을 활용하여, 학습 데이터 준비와 학습 시간 최적화 - 2\n",
    "\n",
    "<img src=\"tf.data2.png\">\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "## 속도를 올리기 위해서는?? Parallel!!\n",
    "## 기존에 shuffle / batch 등등 각각 진행하던 방식을 1개로 합침\n",
    "\n",
    "#Extract\n",
    "files = tf.data.Dataset.list_files(file_pattern)\n",
    "# dataset = tf.data.TFRecordDataset(files) ->\n",
    "dataset = tf.data.TFRecordDataset(files, num_parallel_reads=32)\n",
    "\n",
    "#Transform\n",
    "\n",
    "#shuffle_and_repeat: epochs와 buffers사이에서 정지하는 현상 방지\n",
    "dataset = dataset.apply(\n",
    "    tf.contrib.data.shuffle_and_repeat(10000, NUM_EPOCHS))\n",
    "\n",
    "#map_and_batch: map과 data transfer를 동시에 함\n",
    "dataset = dataset.apply(\n",
    "    tf.contrib.data.map_and_batch(lambda x: ..., BATCH_SIZE))\n",
    "\n",
    "#Load\n",
    "\n",
    "#prefetch_to_device = 그 다음 batch가 미리 GPU 메모리 대기\n",
    "dataset = dataset.apply(tf.contrib.data.prefetch_to_device(\"/gpu:0\"))\n",
    "iterator = dataset.make_one_shot_iterator() #Sequencial Access\n",
    "features = iterator.get_next()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flexibility\n",
    "\n",
    "- tf.SparseTensor를 지원 (1.5ver) -> 복잡한 Categorical data나 embedding 모델을 다룰때\n",
    "\n",
    "- Custom Python code via Dataset.from_generator() ->?\n",
    "\n",
    "- Custom C++ code via DatasetOpKernel plugis\n",
    "\n",
    "-> 실무 새로운 데이터셋을 만들거나 개선할때 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy of Use\n",
    "\n",
    "**데이터를 읽고 쓰는 것을 조금 더 쉽게 지원하기 위한 기능 추가**\n",
    "\n",
    "#### Use Python for loops in eager exeuction mode\n",
    "\n",
    "```python\n",
    "#Extract\n",
    "files = tf.data.Dataset.list_files(file_pattern)\n",
    "dataset = tf.data.TFRecordDataset(files)\n",
    "\n",
    "#Transform\n",
    "dataset = dataset.shuffle(10000)\n",
    "dataset = dataset.repeat(NUM_EPOCHS)\n",
    "dataset = dataset.map(lambda x: tf.parse_single_example(x, features))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "#Eager execution make datset a normal Python iterable.\n",
    "\n",
    "for batch in dataset:\n",
    "    train_model(batch)\n",
    "```\n",
    "\n",
    "</br>\n",
    "\n",
    "#### Standard Method CSV file with protocal buffer (tf 1.8)\n",
    "\n",
    "```python\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "#make_batched_features_dataset\n",
    "dataset = tf.contrib.data.make_batched_features_dataset(\n",
    "    file_pattern, BATCH_SIZE, features, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "#일반적으로는 속도를 위해서는 tf.example, tfrecord와 같은 binary를 추천\n",
    "#하지만 큰 데이터를 항상 가지고 있는 것은 아님.\n",
    "\n",
    "#kaggle의 예\n",
    "#$ pip install kaggle\n",
    "#$ kaggle datasets downaload -d theronk/million-headlins -p .\n",
    "\n",
    "for batch in dataset:\n",
    "    train_model(batch[\"publish_data\"], batch[\"headline_text\"])\n",
    "\n",
    "```\n",
    "\n",
    "### Integration with Esitmators(and Keras comming soon!!)\n",
    "\n",
    "```python\n",
    "def input_fn():\n",
    "    dataset = tf.contrib.data.make_csv_dataset(\n",
    "        \"*.csv\", BATCH_SIZE, num_epochs=NUM_EPOCHS)\n",
    "    return dataset\n",
    "\n",
    "# train an estimator on the dataset\n",
    "tf.esitmator.Estimator(model_fn=train_model).train(input_fn=input_fn)\n",
    "\n",
    "```\n",
    "\n",
    "*공식 홈페이지에서의 추가 정보들과 Performance 참고*\n",
    "\n",
    "- www.tensorflow.org/programmers_guide/datasets\n",
    "- www.tensorflow.org/performance/datasets_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 및 활용\n",
    "\n",
    "ref: https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428\n",
    "\n",
    "1. Importing Data: 데이터셋 생성\n",
    " - From numpy\n",
    " - From tensor\n",
    " - From a placeholder\n",
    " - From generator\n",
    " \n",
    " </br>\n",
    "\n",
    "2. Create an Iterator: 생성된 데이터셋을 바탕으로 Iterator 인스턴스를 만들기\n",
    " - One shot Iterator\n",
    " - Initializable Iterator\n",
    " - Reinitializable Iterator\n",
    " - Feedable Iterator\n",
    " \n",
    " </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinseongjin/tf17_py3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of x: (300, 2)\n",
      "[0.18595173 0.27372692]\n"
     ]
    }
   ],
   "source": [
    "#numpy에서 데이터 불러오기\n",
    "\n",
    "x = np.random.sample((300,2))\n",
    "print(\"size of x:\", x.shape)\n",
    "\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Session을 통하여 데이터를 print해 볼 수 있음\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features와 label을 불러오는 경우\n",
    "features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "\n",
    "# tfrecords를 활용한다면, 따로 데이터와 label을 나눌 필요가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Data 처리\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder 처리방법\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator 활용하여 처리\n",
    "# generator를 활용하여 init하기\n",
    "# 주로 sequence와 같은 길이가 다른 element들이 있을때 유용\n",
    "\n",
    "sequence = np.array([[1],[2,3],[3,4]])\n",
    "\n",
    "def generator():\n",
    "    for el in sequence:\n",
    "        yield el\n",
    "        \n",
    "dataset = tf.data.Dataset().from_generator(generator,\n",
    "                                           output_types=tf.float32, \n",
    "                                           output_shapes=[tf.float32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an Iterator - get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One shot Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46524592 0.91114116]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.sample((100,2))\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "# create the iterator\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el)) # output: [ 0.42116176  0.40666069]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initalizable Iterator\n",
    "\n",
    " - Dynamic dataset을 다루기 위해 placeholder와 함께 활용\n",
    " - placeholder를 feed-dict 메커니즘을 활용하여 초기화 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9624355 0.6523885]\n"
     ]
    }
   ],
   "source": [
    "# using a placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "data = np.random.sample((100,2))\n",
    "\n",
    "iter = dataset.make_initializable_iterator() # create the iterator\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # feed the placeholder with data\n",
    "    sess.run(iter.initializer, feed_dict={ x: data }) #initializer를 선언\n",
    "    print(sess.run(el)) # output [ 0.52374458  0.71968478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 2.], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "## train 과 test 데이터셋 동시에 다루기\n",
    "\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()_\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #     initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "\n",
    "    #     switch to test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinitializable Iterator\n",
    "\n",
    " - 위의 Initalizable Iterator와 유사하지만, 새로운 데이터를 feed 하는 대신, 새로운 데이터로 변경 할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.13652812, 0.69115812]), array([0.24063632])]\n"
     ]
    }
   ],
   "source": [
    "# making fake data using numpy\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "\n",
    "# create two datasets, one for training and one for test\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
    "\n",
    "# 한가지 트릭으로, Generic Iterator를 생성한다.\n",
    "\n",
    "# shape와 type으로 iterator를 생성하고,\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "# 두개를 동시에 초기화한다.\n",
    "\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "# We get the next element as before\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "# session을 활용하여 2개의 초기화 연산을 실행한다.\n",
    "\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_init_op) # switch to train dataset\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "    sess.run(test_init_op) # switch to val dataset\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Consuming data\n",
    "  \n",
    "  - 데이터를 모델에 pass하기 위하여 get_next() 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.1603\n",
      "Iter: 1, Loss: 0.1561\n",
      "Iter: 2, Loss: 0.1521\n",
      "Iter: 3, Loss: 0.1482\n",
      "Iter: 4, Loss: 0.1446\n",
      "Iter: 5, Loss: 0.1412\n",
      "Iter: 6, Loss: 0.1380\n",
      "Iter: 7, Loss: 0.1351\n",
      "Iter: 8, Loss: 0.1323\n",
      "Iter: 9, Loss: 0.1296\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "# using two numpy arrays\n",
    "features, labels = (np.array([np.random.sample((100,2))]), \n",
    "                    np.array([np.random.sample((100,1))]))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator() #iterator생성하기\n",
    "#첫번째 layer와 label에 iter.get_next를 통해 직접 Tensor를 넣는다.\n",
    "x, y = iter.get_next()\n",
    "\n",
    "# 간단한 뉴럴넷 만들기\n",
    "\n",
    "# pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(x, 8, activation=tf.tanh) \n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "# pass the second value from iter.get_net() as label\n",
    "loss = tf.losses.mean_squared_error(prediction, y)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(EPOCHS):\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIPS: BATCHING / REPEAT / MAP / SHUFFLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch\n",
    " - 일반적으로 복잡한 Batch의 처리를 간단하게 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59718665 0.89988339]\n",
      " [0.88266584 0.91656019]\n",
      " [0.84153486 0.9416401 ]\n",
      " [0.41580571 0.67149192]]\n"
     ]
    }
   ],
   "source": [
    "# BATCHING\n",
    "BATCH_SIZE = 4\n",
    "x = np.random.sample((100,2))\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat\n",
    "\n",
    "- .repeat()을 통해, 데이터를 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# REPEAT\n",
    "BATCH_SIZE = 4\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "# #     실행하면 무한 loop 진행\n",
    "#     while True:\n",
    "#         print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle\n",
    " - shuffle을 통해 데이터를 섞는다. overfitting을 막기위해 중요한 기능이다.\n",
    " - buffer_size를 지정하여 uniform하게 선택되도록 한다. (seed와 유사)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [4]\n",
      " [3]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "# SHUFFLE\n",
    "BATCH_SIZE = 4\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.shuffle(buffer_size=100)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP\n",
    " - map을 활용하여, 2개의 element를 2로 곱하는 예를 들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[4]\n",
      "[6]\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "# MAP\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.map(lambda x: x*2)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     this will run forever\n",
    "        for _ in range(len(x)):\n",
    "            print(sess.run(el))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chanwoo tut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF.DATA를 시작하기 전에 알아야 할 개념!\n",
    "\n",
    "## Background - Dynamic vs. Static\n",
    "\n",
    "### Dynamic 방식으로 데이터를 처리하면, 모든 데이터를 메모리에 올리지 않고, 필요할 때만 추출한다. tf.data를 시작하기 전에 iterator / Generator / Yield에 대해 이해하고 있어야 활용이 용이하다.\n",
    "\n",
    "- http://stackabuse.com/python-generators/ \n",
    "\n",
    "- http://pymbook.readthedocs.io/en/latest/igd.htm\n",
    "\n",
    "- https://dojang.io/mod/page/view.php?id=1117\n",
    "\n",
    " 1. Iterator - Repeatable Object (반복가능한 객체)\n",
    "\n",
    " 2. Generator - Iterator를 만들어주는 것, lazy generation of values (on-demand)\n",
    " \n",
    " 3. Yield - 함수에서 return과 동일한 역할 수행\n",
    "\n",
    "### Adventages\n",
    "\n",
    "- 온디멘드 방식으로 메모리를 적게 사용 (Streaming, Big Data)\n",
    "\n",
    "- 사용하지 않는 값들은 저장하고 있지 않음\n",
    "\n",
    "### Disadventages\n",
    "\n",
    "- Static 방식에 비해 로드 때마다 IO 연산이 발생되는데, 이후 TF.DATA는 관련 방식을 어떻게 해결하는지 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic (동적) 접근\n",
      "0\n",
      "1\n",
      "Static(정적)변환 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Generator 활용의 예\n",
    "\n",
    "#yield: 함수 실행 중간에 빠져나올 수 있는 generator를 만들 때 사용\n",
    "\n",
    "def num_gen(n):\n",
    "    num = 0\n",
    "    while num < n:\n",
    "        yield num # loop 중간에 값을 가져옴\n",
    "        num += 1\n",
    "\n",
    "gen_execute = num_gen(10)\n",
    "\n",
    "print(\"Dynamic (동적) 접근\")\n",
    "print(next(gen_execute))\n",
    "print(next(gen_execute))\n",
    "# 위와 같은 방식으로 데이터를 필요 할 때만 가져오는 방식으로 접근한다.\n",
    "\n",
    "# Static으로 변환하고 싶으면 list를 활용하자\n",
    "print(\"Static(정적)변환 {}\".format(list(num_gen(10))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
