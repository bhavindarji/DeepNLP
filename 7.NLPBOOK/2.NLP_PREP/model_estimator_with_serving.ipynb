{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  1.8.0\n",
      "Pandas Version:  0.22.0\n",
      "Numpy Version:  1.14.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "print('Tensorflow Version: ', tf.__version__)\n",
    "print('Pandas Version: ', pd.__version__)\n",
    "print('Numpy Version: ', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/'\n",
    "MODEL_PATH = './model/'\n",
    "\n",
    "MORPH_QUERY_DATA_FILE = 'morph_query.npy'\n",
    "COMMON_QUERY_DATA_FILE = 'common_query.npy'\n",
    "INTENT_DATA_FILE = 'intent.npy'\n",
    "DATA_SETTING_CONFIG = 'large_intent_data_map.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_data = np.load(open(DATA_PATH + MORPH_QUERY_DATA_FILE, 'rb'))\n",
    "common_data = np.load(open(DATA_PATH + COMMON_QUERY_DATA_FILE, 'rb'))\n",
    "labels = np.load(open(DATA_PATH + INTENT_DATA_FILE, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.stack((morph_data, common_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "\n",
    "morph_train = X_train[:,0]\n",
    "common_train = X_train[:,1]\n",
    "morph_test = X_test[:,0]\n",
    "common_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Dataset Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys:  ['intent', 'pad_sym', 'unk_sym', 'common_vocab', 'morph_vocab']\n",
      "intents:  {'뽀로로': 0, '감정/감각': 1, '요청/제안': 2, '인사': 3, '일상': 4, '칭찬': 5, '기타': 6, '불만': 7}\n"
     ]
    }
   ],
   "source": [
    "json_data=open(DATA_PATH + DATA_SETTING_CONFIG).read()\n",
    "data_configs = json.loads(json_data)\n",
    "print('keys: ', list(data_configs.keys()))\n",
    "print('intents: ', data_configs['intent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Define input funtion for tf.Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(x, y):\n",
    "    return { 'query': x }, y\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((morph_train, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.map(parse)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.repeat(1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def test_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((morph_test, y_test))\n",
    "    dataset = dataset.map(parse)\n",
    "    dataset = dataset.batch(32)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Default Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_SEQ_LEN = 15\n",
    "\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "CONV_CHANNEL_DIM = 150\n",
    "CONV_WINDOW_SIZE = 3\n",
    "DNN_FEATURE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(data_configs['morph_vocab'])\n",
    "embedding_size = WORD_EMBEDDING_DIM\n",
    "\n",
    "conv_channel_dim = CONV_CHANNEL_DIM\n",
    "conv_window_size = CONV_WINDOW_SIZE\n",
    "\n",
    "dense_dim = DNN_FEATURE_SIZE\n",
    "num_classes = len(data_configs['intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cnn_model_fn(features, labels, mode):\n",
    "    word_embeddings = tf.get_variable('word_embeddings', [vocabulary_size, embedding_size])\n",
    "    \n",
    "    word_embedded_layer = tf.nn.embedding_lookup(word_embeddings, features['query'])\n",
    "    conv_layer = tf.layers.conv1d(word_embedded_layer, conv_channel_dim, \n",
    "                                  conv_window_size, activation=tf.nn.relu,\n",
    "                                  padding='same')\n",
    "    max_pool_layer = tf.layers.max_pooling1d(conv_layer, LIMIT_SEQ_LEN, 1)\n",
    "    dense_layer_1 = tf.layers.dense(max_pool_layer, dense_dim,\n",
    "                              activation=tf.nn.relu)\n",
    "    logit_layer = tf.layers.dense(dense_layer_1, num_classes)\n",
    "    logit_layer = tf.squeeze(logit_layer, axis=1)\n",
    "    \n",
    "    prob_layer = tf.nn.softmax(logit_layer)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = { 'class': tf.argmax(prob_layer, 1) }\n",
    "        export_outputs = {\n",
    "          'prediction': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  predictions=predictions,\n",
    "                  export_outputs=export_outputs)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        one_hot_labels = tf.one_hot(labels, num_classes)\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.argmax(prob_layer, 1))\n",
    "        metrics = {'accuracy': accuracy}\n",
    "        loss = tf.losses.softmax_cross_entropy(one_hot_labels, logit_layer)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "    else:\n",
    "        one_hot_labels = tf.one_hot(labels, num_classes)\n",
    "        global_step = tf.train.get_global_step()\n",
    "        loss = tf.losses.softmax_cross_entropy(one_hot_labels, logit_layer)\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  train_op=train_op,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model/basic_cnn_0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2294b8d860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "est = tf.estimator.Estimator(basic_cnn_model_fn, model_dir=MODEL_PATH + 'basic_cnn_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and eval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model/basic_cnn_0/model.ckpt-486\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 487 into ./model/basic_cnn_0/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.6312933, step = 486\n",
      "INFO:tensorflow:global_step/sec: 439.458\n",
      "INFO:tensorflow:loss = 0.16019377, step = 586 (0.228 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 648 into ./model/basic_cnn_0/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.1686485.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f2294b8d710>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.train(train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-07:24:10\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model/basic_cnn_0/model.ckpt-648\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-07:24:10\n",
      "INFO:tensorflow:Saving dict for global step 648: accuracy = 0.83652174, global_step = 648, loss = 0.53325325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.83652174, 'loss': 0.53325325, 'global_step': 648}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.evaluate(test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = est.predict(test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model for Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def serving_input_receiver_fn():\n",
    "    \n",
    "    feature_spec = {\n",
    "        'query': tf.FixedLenFeature([], dtype=tf.string)\n",
    "    }\n",
    "    \n",
    "    default_batch_size = 1\n",
    "    \n",
    "    serialized_tf_example = tf.placeholder(\n",
    "        dtype=tf.string, shape=[None],\n",
    "        name='input_query_tensor')\n",
    "    \n",
    "    received_tensors = { 'query': serialized_tf_example }\n",
    "    features = tf.parse_example(serialized_tf_example, feature_spec)\n",
    "    \n",
    "    fn = lambda query: tf.decode_raw(query, tf.int32)\n",
    "    \n",
    "    features['query'] = tf.map_fn(fn, features['query'], dtype=tf.int32)\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features, received_tensors)\n",
    "'''\n",
    "\n",
    "def serving_input_receiver_fn():\n",
    "    receiver_tensor = {\n",
    "        'query': tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        key: tensor\n",
    "        for key, tensor in receiver_tensor.items()\n",
    "    }\n",
    "    fn = lambda query: tf.decode_raw(query, tf.int64)\n",
    "    features['query'] = tf.map_fn(fn, features['query'], dtype=tf.int64)\n",
    "    features['query'] = tf.reshape(features['query'], [-1, LIMIT_SEQ_LEN])\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['prediction', 'serving_default']\n",
      "INFO:tensorflow:Restoring parameters from ./model/basic_cnn_0/model.ckpt-648\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"./served_model/test_model/temp-b'1529565991'/saved_model.pb\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'./served_model/test_model/1529565991'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.export_savedmodel(export_dir_base, serving_input_receiver_fn,\n",
    "                      strip_default_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from b'./served_model/test_model/1529565991/variables/variables'\n"
     ]
    }
   ],
   "source": [
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "    export_dir = \"./served_model/test_model/1529565991\",\n",
    "    signature_def_key=\"serving_default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([ 691, 1723,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "    1], dtype=np.int64)\n",
    "arr = arr.tostring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predictor_fn({'query': [arr]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['class']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
