{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import imdb #imdb 데이터셋을 불러온다\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 tf.data\n",
    "\n",
    "  머신러닝에서 많은 시간을 소요하는 것은 데이터를 다루는 것이다. 일반적으로 데이터 분석, 전처리, 파이프라인 만드는 과정에서 약 70~80%의 시간을 소비한다 해도 과언이 아니다. 텐서플로우에서는 Dataset API를 활용하여, 단순한 연구자 뿐만 아니라 서비스화를 위해서 데이터에 대한 고민들을 해결 해 주고 있다. 학습 속도를 올리기 위해  GPU가 병목현상 없이 효율적으로 최적화를 지원하는 것 뿐만 아니라 이미지, 텍스트를 넘어 넘파이 (Numpy) 및 판다스(Pandas)데이터 구조가 활용하도록 지원이 가능하다.\n",
    "  \n",
    "  tf.data를 사용하기 위한 데이터를 활용하기 위해, IMDB 영화 리뷰 데이터셋을 불러오자. 데이터셋은 2만5천개의 리뷰의 데이터가 긍/부정으로 나누어져 있고, 전처리도 되어 있다. IMDB 영화 리뷰 데이터셋을 선정한 이유는 MNIST와 처럼 데이터셋을 tensorflow-keras를 활용하여 가장 손쉽게 불러오는 자연어 데이터이기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "xTrain shape: (25000, 200)\n",
      "xTest shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 10000 #문장의 단어 사이즈\n",
    "SENT_SIZE = 200 #문장 길이\n",
    "EMB_SIZE = 128 #임베딩 사이즈\n",
    "MODEL_DIR = tempfile.mkdtemp() #임시 디렉토리에 모델 저장\n",
    "\n",
    "PAD_ID = 0 # 200단어 이하 문장에 대해서 0 값을 채워 넣는다.\n",
    "START_ID = 1 # 시작 id 값\n",
    "OOV_ID = 2 # out of vocab\n",
    "INDEX_OFFSET = 2\n",
    "\n",
    "#IMDB 데이터셋을 로드 합니다. 학습과 테스트 셋으로 나눕니다.\n",
    "(xTrainVar, yTrain), (xTestVar, yTest) = imdb.load_data(num_words=VOCAB_SIZE,\n",
    "                                                      start_char=START_ID,\n",
    "                                                      oov_char=OOV_ID,\n",
    "                                                      index_from=INDEX_OFFSET)\n",
    "\n",
    "# 패딩 채워주기\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "xTrain = sequence.pad_sequences(xTrainVar, \n",
    "                                 maxlen=SENT_SIZE,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=PAD_ID)\n",
    "xTest = sequence.pad_sequences(xTestVar, \n",
    "                                maxlen=SENT_SIZE,\n",
    "                                truncating='post',\n",
    "                                padding='post', \n",
    "                                value=PAD_ID)\n",
    "\n",
    "print(\"xTrain shape:\", xTrain.shape)\n",
    "print(\"xTest shape:\", xTest.shape)\n",
    "\n",
    "xLenTrain = np.array([min(len(x), SENT_SIZE) for x in xTrainVar])\n",
    "xLenTest = np.array([min(len(x), SENT_SIZE) for x in xTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
