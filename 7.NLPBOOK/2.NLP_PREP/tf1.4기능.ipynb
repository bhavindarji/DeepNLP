{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset/chat.in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "\n",
    "delimiter = \"\\t\"\n",
    "\n",
    "def read_in_data(data_path):\n",
    "    user = []\n",
    "    bot = []\n",
    "    with open(data_path, mode=\"rt\", encoding=\"utf-8\") as fh:\n",
    "        utt = fh.readlines()\n",
    "        \n",
    "        for i, line in enumerate(utt):\n",
    "            split_line = line.split(delimiter)\n",
    "            query = split_line[0].replace(\"\\\"\", \"\").replace(\"\\n\", \"\")\n",
    "            answer = split_line[1].replace(\"\\\"\", \"\").replace(\"\\n\", \"\")\n",
    "            \n",
    "            user.append(query)\n",
    "            bot.append(answer)\n",
    "            \n",
    "        return user, bot\n",
    "    \n",
    "user_query, answer_query = read_in_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 dataset으로 변환 및 문장 쪼개기\n",
    "dict_data = {'user': user_query, 'bot': answer_query}\n",
    "df_data = pd.DataFrame(dict_data, columns=['user', 'bot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_tokenizer = lambda x: twitter.morphs(x)\n",
    "df_data['user_token'] = df_data['user'].apply(twit_tokenizer) #Tokenizer로 저장\n",
    "df_data['bot_token'] = df_data['bot'].apply(twit_tokenizer) #Tokenizer로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>bot</th>\n",
       "      <th>user_token</th>\n",
       "      <th>bot_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>메롱</td>\n",
       "      <td>저랑 놀고 싶으신거군요</td>\n",
       "      <td>[메롱]</td>\n",
       "      <td>[저, 랑, 놀고, 싶으, 신거, 군요]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아 그러면 안되</td>\n",
       "      <td>네? 어떤 말씀이신지 궁금하네요</td>\n",
       "      <td>[아, 그러면, 안되]</td>\n",
       "      <td>[네, ?, 어떤, 말씀, 이, 신지, 궁금하네, 요]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>오늘 하루 어때?</td>\n",
       "      <td>당신과 함께해서 좋은거 같아요</td>\n",
       "      <td>[오늘, 하루, 어때, ?]</td>\n",
       "      <td>[당신, 과, 함께해서, 좋, 은, 거, 같아, 요]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>너 지금 얼마 있어?</td>\n",
       "      <td>저는 마음만큼은 부자에요.</td>\n",
       "      <td>[너, 지금, 얼마, 있어, ?]</td>\n",
       "      <td>[저, 는, 마음, 만큼은, 부자, 에요, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user                bot          user_token  \\\n",
       "0           메롱       저랑 놀고 싶으신거군요                [메롱]   \n",
       "1     아 그러면 안되  네? 어떤 말씀이신지 궁금하네요        [아, 그러면, 안되]   \n",
       "2    오늘 하루 어때?   당신과 함께해서 좋은거 같아요     [오늘, 하루, 어때, ?]   \n",
       "3  너 지금 얼마 있어?     저는 마음만큼은 부자에요.  [너, 지금, 얼마, 있어, ?]   \n",
       "\n",
       "                        bot_token  \n",
       "0          [저, 랑, 놀고, 싶으, 신거, 군요]  \n",
       "1  [네, ?, 어떤, 말씀, 이, 신지, 궁금하네, 요]  \n",
       "2   [당신, 과, 함께해서, 좋, 은, 거, 같아, 요]  \n",
       "3      [저, 는, 마음, 만큼은, 부자, 에요, .]  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize 된 단어를 합쳐서 사전을 만들어보자\n",
    "token_dict = itertools.chain.from_iterable(list(df_data['user_token'] + list(df_data['bot_token'])))\n",
    "token_dict = list(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real vocabulary size    38\n",
      "Limited vocabulary size 38\n"
     ]
    }
   ],
   "source": [
    "#http://tensorlayer.readthedocs.io/en/latest/\n",
    "#딕셔너리 만들기\n",
    "\n",
    "vocab_size = 38\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = \\\n",
    "        tl.nlp.build_words_dataset(token_dict, vocab_size, True)\n",
    "    \n",
    "ids = lambda x: tl.nlp.words_to_word_ids(x, dictionary) #to ids\n",
    "context = lambda x: tl.nlp.word_ids_to_words(x, reverse_dictionary) #to ids\n",
    "np_array = lambda x: np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [저, 랑, 놀고, 싶으, 신거, 군요]\n",
       "1    [네, ?, 어떤, 말씀, 이, 신지, 궁금하네, 요]\n",
       "2     [당신, 과, 함께해서, 좋, 은, 거, 같아, 요]\n",
       "3      [저, 는, 마음, 만큼은, 부자, 에요, UNK]\n",
       "Name: bot_ids, dtype: object"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#token - ids 생성\n",
    "df_data['user_ids'] = df_data['user_token'].apply(ids).apply(np_array)\n",
    "df_data['user_ids'].apply(context)\n",
    "\n",
    "df_data['bot_ids'] = df_data['bot_token'].apply(ids).apply(np_array)\n",
    "df_data['bot_ids'].apply(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary ./dataset/test5.in from data ./dataset/chat.in\n"
     ]
    }
   ],
   "source": [
    "#Vocab Dictionary 생성하기, most frequent word\n",
    "tl.nlp.create_vocabulary('./dataset/test5.in', './dataset/chat.in', max_vocabulary_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 'EOS'],\n",
       " [10, 11, 12, 'EOS'],\n",
       " [19, 20, 21, 1, 'EOS'],\n",
       " [29, 30, 31, 32, 1, 'EOS']]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.prepro.sequences_add_end_id(list(df_data['user_ids']), end_id='EOS') # 단순히 add end_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1], dtype=object)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.prepro.sequences_add_end_id(list(df_data['user_ids']), end_id=999) # 단순히 add end_ID\n",
    "tl.prepro.sequences_add_start_id(list(df_data['user_ids']), start_id=123) # 단순히 add end_ID\n",
    "tl.prepro.sequences_add_end_id_after_pad(list(df_data['user_ids']), end_id=99, pad_id=0) #add end_ID + padding, 근데 잘 안됨\n",
    "tl.prepro.sequences_get_mask(list(df_data['user_ids']), pad_val=0) #이것도 왜..?ㄴ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"query\": np.array(df_data['user_ids'])},\n",
    "    y=np.array(df_data['bot_ids']),\n",
    "    num_epochs=None,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([4]), array([10, 11, 12]), array([19, 20, 21,  1]),\n",
       "       array([29, 30, 31, 32,  1])], dtype=object)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(df_data['user_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    \n",
    "    user = tf.constant(np.array(df_data['user_ids']), tf.float32)\n",
    "    bot = tf.constant(np.array(df_data['bot_ids']), tf.float32)\n",
    "    \n",
    "    return user, bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-8cd5fc7c2581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-239-7588964fd173>\u001b[0m in \u001b[0;36mtrain_input_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bot_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf14/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/tf14/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "train_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features, labels, mode):\n",
    "    layer1 = tf.layers.dense(features, 10)\n",
    "    layer2 = tf.layers.dense(layer1, 10)\n",
    "    layer3 = tf.layers.dense(layer2, 10)\n",
    "    layer4 = tf.layers.dense(layer3, 10)\n",
    "    out = tf.layers.dense(layer4, 1)\n",
    "    \n",
    "    global_step = tf.train.get_global_step()\n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels, out)\n",
    "    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss, global_step)\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/ty/43yv2fzx4l9df6zcch8zml6m0000gn/T/tmpzjx7elb8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/ty/43yv2fzx4l9df6zcch8zml6m0000gn/T/tmpzjx7elb8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/ty/43yv2fzx4l9df6zcch8zml6m0000gn/T/tmpzjx7elb8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13b3ef438>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/ty/43yv2fzx4l9df6zcch8zml6m0000gn/T/tmpzjx7elb8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13b3ef438>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-99007344c34b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tf14/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf14/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    709\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglobal_step_read_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 711\u001b[0;31m             features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m    712\u001b[0m       \u001b[0;31m# Check if the user created a loss summary, and add one if they didn't.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m       \u001b[0;31m# We assume here that the summary is called 'loss'. If it is not, we will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf14/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'config'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_fn_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-234-8d6bd6e2bd1e>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(features, labels, mode)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlayer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlayer3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlayer4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf14/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 _reuse=reuse)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "est = tf.estimator.Estimator(model)\n",
    "est.train(train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'query': <tf.Tensor 'random_shuffle_queue_DequeueMany:1' shape=(128,) dtype=string>},\n",
       " <tf.Tensor 'random_shuffle_queue_DequeueMany:2' shape=(128,) dtype=string>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.estimator.DNNClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.record로 저장하는 것은 binary 파일이며, 이미지나 각 문서를 열때 걸리는 시간을 최소화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    [4]\n",
       "1           [10, 11, 12]\n",
       "2        [19, 20, 21, 1]\n",
       "3    [29, 30, 31, 32, 1]\n",
       "Name: user_ids, dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['user_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecord_dir = './dataset/corpus_set.tfrecord'\n",
    "\n",
    "dataset_writer = tf.python_io.TFRecordWriter(train_tfrecord_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got [[4], [10, 11, 12], [19, 20, 21, 1], [29, 30, 31, 32, 1]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-0fa09414a34e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m features = {'train/query': _bytes_feature(tf.compat.as_bytes(list(df_data['user_ids']))),\n\u001b[0m\u001b[1;32m      2\u001b[0m            'train/bot': _bytes_feature(tf.compat.as_bytes(list(df_data['bot_ids'])))}\n",
      "\u001b[0;32m~/tf14/lib/python3.6/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 65\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got [[4], [10, 11, 12], [19, 20, 21, 1], [29, 30, 31, 32, 1]]"
     ]
    }
   ],
   "source": [
    "features = {'train/query': _bytes_feature(tf.compat.as_bytes(list(df_data['user_ids']))),\n",
    "           'train/bot': _bytes_feature(tf.compat.as_bytes(list(df_data['bot_ids'])))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 10\n",
    "window_size = 3\n",
    "dict_size = 400\n",
    "batch_size = 32\n",
    "max_length = 30\n",
    "filter_size = window_size*embed_size\n",
    "\n",
    "summ = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(mode, features, labels):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "\n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([dict_size, embed_size], seed=7777),\n",
    "            trainable=False)\n",
    "    flags = tf.stack([\n",
    "        tf.zeros(embed_size),\n",
    "        0.1*tf.ones(embed_size),\n",
    "        0.2*tf.ones(embed_size)])\n",
    "\n",
    "    embeddings = tf.assign(embeddings[:3], flags)\n",
    "\n",
    "    # embedding\n",
    "    input_embeds = tf.nn.embedding_lookup(embeddings, features['ids'])\n",
    "\n",
    "    input_flat = tf.layers.flatten(input_embeds)\n",
    "    input_flat = tf.expand_dims(input_flat, -1)\n",
    "\n",
    "    init_word = tf.nn.embedding_lookup(embeddings, features['init_word'])\n",
    "    init_word = tf.layers.flatten(init_word)\n",
    "    init_word = tf.expand_dims(init_word, -1)\n",
    "\n",
    "    if not PREDICT:\n",
    "        label_onehot = tf.one_hot(labels, depth=dict_size, dtype=tf.float32)\n",
    "\n",
    "    # encoder\n",
    "    encoder_conv1 = tf.layers.conv1d(\n",
    "            inputs=input_flat,\n",
    "            filters=2*embed_size,\n",
    "            kernel_size=filter_size,\n",
    "            strides=embed_size,\n",
    "            padding='same')\n",
    "\n",
    "    encoder_conv2 = tf.layers.conv1d(\n",
    "            inputs=encoder_conv1,\n",
    "            filters=2*embed_size,\n",
    "            kernel_size=filter_size,\n",
    "            strides=embed_size,\n",
    "            padding='same')\n",
    "\n",
    "    encoder_conv3 = tf.layers.conv1d(\n",
    "            inputs=encoder_conv2,\n",
    "            filters=2*embed_size,\n",
    "            kernel_size=filter_size,\n",
    "            strides=embed_size,\n",
    "            padding='same')\n",
    "\n",
    "    encoder_conv4 = tf.layers.conv1d(\n",
    "            inputs=encoder_conv3,\n",
    "            filters=2*embed_size,\n",
    "            kernel_size=filter_size,\n",
    "            strides=embed_size,\n",
    "            padding='same')\n",
    "\n",
    "    encoder_glu = encoder_conv4[:, :, embed_size:]*tf.nn.sigmoid(encoder_conv4[:, :, :embed_size])\n",
    "\n",
    "    #decoder\n",
    "    next_ids = []\n",
    "    outs = []\n",
    "    for l in range(max_length):\n",
    "        if l == 0:\n",
    "            decoder_input = init_word\n",
    "            reuse = False\n",
    "        else:\n",
    "            reuse = True\n",
    "\n",
    "        decoder_conv1 = tf.layers.conv1d(\n",
    "                inputs=decoder_input,\n",
    "                filters=2*embed_size,\n",
    "                kernel_size=filter_size,\n",
    "                strides=embed_size,\n",
    "                reuse=reuse,\n",
    "                name='decoder_1')\n",
    "\n",
    "        print('decoder conv1 shape ', decoder_conv1.shape)\n",
    "\n",
    "        decoder_conv1 = decoder_conv1 + decoder_input\n",
    "\n",
    "        print('decoder conv1 res shape ', decoder_conv1.shape)\n",
    "\n",
    "        decoder_conv2 = tf.layers.conv1d(\n",
    "                inputs=decoder_conv1,\n",
    "                filters=2*embed_size,\n",
    "                kernel_size=filter_size,\n",
    "                strides=embed_size,\n",
    "                reuse=reuse,\n",
    "                name='decoder_2')\n",
    "\n",
    "        print('decoder conv2 shape ', decoder_conv2.shape)\n",
    "\n",
    "        decoder_conv2 = decoder_conv2 + decoder_conv1\n",
    "\n",
    "        print('decoder conv2 res shape ', decoder_conv2.shape)\n",
    "\n",
    "        decoder_conv3 = tf.layers.conv1d(\n",
    "                inputs=decoder_conv2,\n",
    "                filters=2*embed_size,\n",
    "                kernel_size=filter_size,\n",
    "                strides=embed_size,\n",
    "                reuse=reuse,\n",
    "                name='decoder_3')\n",
    "\n",
    "        print('decoder conv3 shape ', decoder_conv3.shape)\n",
    "\n",
    "        decoder_conv3 = decoder_conv3 + decoder_conv2\n",
    "\n",
    "        print('decoder conv3 shape ', decoder_conv3.shape)\n",
    "\n",
    "        decoder_conv4 = tf.layers.conv1d(\n",
    "                inputs=decoder_conv3,\n",
    "                filters=2*embed_size,\n",
    "                kernel_size=filter_size,\n",
    "                strides=embed_size,\n",
    "                reuse=reuse,\n",
    "                name='decoder_4')\n",
    "\n",
    "        print('decoder conv4 shape ', decoder_conv4.shape)\n",
    "\n",
    "\n",
    "        decoder_glu = decoder_conv4[:, :, embed_size:]*tf.nn.sigmoid(decoder_conv4[:, :, :embed_size])\n",
    "\n",
    "        tiled_decoder_glu = tf.tile(decoder_glu, [1, int(encoder_glu.shape[1]), 1])\n",
    "\n",
    "        dot_prod = tf.matmul(encoder_glu, decoder_glu, transpose_b=True)\n",
    "\n",
    "        attention = tf.nn.softmax(dot_prod, dim=1)\n",
    "\n",
    "        z_plus_e = encoder_glu + input_embeds\n",
    "\n",
    "        tiled_attention = tf.tile(attention, [1, 1, embed_size])\n",
    "\n",
    "        c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "        decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "\n",
    "        logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        next_id = tf.argmax(out, axis=1)\n",
    "        next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "        next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "        decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1)\n",
    "\n",
    "        next_ids.append(next_id)\n",
    "        outs.append(logits)\n",
    "\n",
    "    outs = tf.stack(outs, axis=1)\n",
    "    next_ids = tf.stack(next_ids, axis=1)\n",
    "    answer_length = tf.reduce_mean(max_length-features['answer_length'], axis=-1)\n",
    "    sequence_mask = tf.sequence_mask(answer_length, max_length)\n",
    "    sequence_mask = tf.logical_not(sequence_mask)\n",
    "    sequence_mask = tf.cast(sequence_mask, tf.float32)\n",
    "\n",
    "#\n",
    "#    sequence_mask = tf.Print(sequence_mask,\n",
    "#            [out, next_id], summarize=100)\n",
    "\n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(labels=label_onehot, logits=outs)\n",
    "        loss = tf.reduce_mean(losses * sequence_mask)\n",
    "\n",
    "        train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss, global_step)\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                train_op=train_op,\n",
    "                loss=loss)\n",
    "\n",
    "    elif EVAL:\n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(labels=label_onehot, logits=outs)\n",
    "        loss = tf.reduce_mean(losses * sequence_mask)\n",
    "\n",
    "        eval_metric_ops = {'acc': tf.metrics.accuracy(labels, next_ids)}\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=loss,\n",
    "                eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "    elif PREDICT:\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions={\n",
    "                    'prediction': next_ids,\n",
    "                    'contents': features['ids'],\n",
    "                    'answer': features['label'],\n",
    "                    'length': features['answer_length']})\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception('estiamtor spec is invalid')\n",
    "\n",
    "    return estimator_spec\n",
    "\n",
    "\n",
    "def training(est):\n",
    "    est.train(train_input_fn)\n",
    "\n",
    "\n",
    "def evaluate(est):\n",
    "    id_map, word_map = read_maps()\n",
    "    FLAGS = re.compile('[^PAD|<START>|<END>]')\n",
    "\n",
    "    pred = est.predict(test_input_fn)\n",
    "    for p in pred:\n",
    "        pred_start = -p['length'][0]\n",
    "        words_cont = map(lambda i: word_map[i], p['contents'])\n",
    "        words_pred = map(lambda i: word_map[i], p['prediction'][pred_start:])\n",
    "        words_ans = map(lambda i: word_map[i], p['answer'][pred_start:])\n",
    "\n",
    "        filtered_contents = ' '.join(filter(FLAGS.match, words_cont))\n",
    "        filtered_pred = ' '.join(filter(FLAGS.match, words_pred))\n",
    "        filtered_ans = ' '.join(filter(FLAGS.match, words_ans))\n",
    "\n",
    "        print('contents: ', filtered_contents)\n",
    "        print('pred ans: ', filtered_pred)\n",
    "        print('human ans: ', filtered_ans)\n",
    "        print()\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    config = tf.estimator.RunConfig()\n",
    "    config._save_checkpoints_steps = 1000\n",
    "    config._save_checkpoints_secs = None\n",
    "    fairseq = tf.estimator.Estimator(model_fn, model_dir='./checkpoint/model', config=config)\n",
    "    for i in range(999999):\n",
    "        training(fairseq)\n",
    "        evaluate(fairseq)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset():\n",
    "    with open(\"./dataset/test.in\", 'w') as f:\n",
    "        for i in range(len(df_data['user_ids'])):\n",
    "            f.write(str(df_data['user_ids'][i]) + '\\n')\n",
    "            f.write(str(df_data['bot_ids'][i]) + '\\n')        \n",
    "\n",
    "gen_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(\"./dataset/test.in\")\n",
    "tr_data = tf.data.Dataset.from_tensor_slices((user_query, answer_query))\n",
    "\n",
    "iterator = tf.data.Iterator.from_structure(tr_data.output_types,\n",
    "                                   tr_data.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "training_init_op = iterator.make_initializer(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/vahidk/EffectiveTensorflow\n",
    "dataset = tf.data.TextLineDataset(\"./dataset/test.in\")\n",
    "\n",
    "tr_data = tf.data.Dataset.from_tensor_slices((user_query, answer_query))\n",
    "\n",
    "iterator = tf.data.Iterator.from_structure(tr_data.output_types,\n",
    "                                   tr_data.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "training_init_op = iterator.make_initializer(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'\\xeb\\xa9\\x94\\xeb\\xa1\\xb1', b'\\xec\\xa0\\x80\\xeb\\x9e\\x91 \\xeb\\x86\\x80\\xea\\xb3\\xa0 \\xec\\x8b\\xb6\\xec\\x9c\\xbc\\xec\\x8b\\xa0\\xea\\xb1\\xb0\\xea\\xb5\\xb0\\xec\\x9a\\x94')\n",
      "(b'\\xec\\x95\\x84 \\xea\\xb7\\xb8\\xeb\\x9f\\xac\\xeb\\xa9\\xb4 \\xec\\x95\\x88\\xeb\\x90\\x98', b'\\xeb\\x84\\xa4? \\xec\\x96\\xb4\\xeb\\x96\\xa4 \\xeb\\xa7\\x90\\xec\\x94\\x80\\xec\\x9d\\xb4\\xec\\x8b\\xa0\\xec\\xa7\\x80 \\xea\\xb6\\x81\\xea\\xb8\\x88\\xed\\x95\\x98\\xeb\\x84\\xa4\\xec\\x9a\\x94')\n",
      "(b'\\xec\\x98\\xa4\\xeb\\x8a\\x98 \\xed\\x95\\x98\\xeb\\xa3\\xa8 \\xec\\x96\\xb4\\xeb\\x95\\x8c?', b'\\xeb\\x8b\\xb9\\xec\\x8b\\xa0\\xea\\xb3\\xbc \\xed\\x95\\xa8\\xea\\xbb\\x98\\xed\\x95\\xb4\\xec\\x84\\x9c \\xec\\xa2\\x8b\\xec\\x9d\\x80\\xea\\xb1\\xb0 \\xea\\xb0\\x99\\xec\\x95\\x84\\xec\\x9a\\x94')\n",
      "(b'\\xeb\\x84\\x88 \\xec\\xa7\\x80\\xea\\xb8\\x88 \\xec\\x96\\xbc\\xeb\\xa7\\x88 \\xec\\x9e\\x88\\xec\\x96\\xb4?', b'\\xec\\xa0\\x80\\xeb\\x8a\\x94 \\xeb\\xa7\\x88\\xec\\x9d\\x8c\\xeb\\xa7\\x8c\\xed\\x81\\xbc\\xec\\x9d\\x80 \\xeb\\xb6\\x80\\xec\\x9e\\x90\\xec\\x97\\x90\\xec\\x9a\\x94.')\n",
      "End of training dataset.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # initialize the iterator on the training data\n",
    "    sess.run(training_init_op)\n",
    "\n",
    "    # get each element of the training dataset until the end is reached\n",
    "    while True:\n",
    "        try:\n",
    "            elem = sess.run(next_element)\n",
    "            print(elem)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of training dataset.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real vocabulary size    8\n",
      "Limited vocabulary size 8\n"
     ]
    }
   ],
   "source": [
    "#Data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_data['user_ids'], df_data['bot_ids'], test_size=0.3,random_state=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
