{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Eager Tutorial (Define by Run)\n",
    "\n",
    "정리 및 요약 by Ryah Shin\n",
    "\n",
    "[참고1: 구글 블로그](https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html)\n",
    "\n",
    "[참고2: 구글 깃허브](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/g3doc/guide.md)\n",
    "\n",
    "[Eager 모드 설치방법(TF Nightly)](https://github.com/tensorflow/tensorflow#installation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Eager Execution\n",
    "\n",
    " Eager (이거)는 기존의 세션과 그래프 형식의 작동 방식에서 명령형 (imperative) 스타일로 텐서플로우를 활용할수 있는 기능이다.  기존에 텐서플로우 방식이 세션과 그래프 완성 후 디버깅을 해야하는 불편함이 있었다면, 이거 모드를 통해서 좀 더 직관적으로 접근 가능하다.\n",
    " \n",
    "텐서플로우 2018 Dev Summit에서 내용을 발췌하면 다음과 같다.\n",
    "\n",
    " - Eager 모드는 점점 contrib에서 정식으로 변하고 있고 session을 사용하지 않고 코드를 실행 가능하다.\n",
    " - 그래디언트 계산을 손쉽게 수정이 가능하다.\n",
    " - Dataset (텐서플로우 자료 구조 중 하나)를 통해서 sqlite database를 읽을 수 있다 (아직 실험적)\n",
    " - TensorRT를 지원함으로써 모델을 최적화 할 수 있다.\n",
    " \n",
    "이 장에서는 Eager 모드에 대한 간략한 소개, 기능 설명 및 예제를 제공하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3550a3535ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TensorFlow version: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVERSION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf18/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5466\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m     raise ValueError(\n\u001b[0;32m-> 5468\u001b[0;31m         \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5470\u001b[0m   \u001b[0;31m# Monkey patch to get rid of an unnecessary conditional since the context is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# eager 모드를 실행하는 방법\n",
    "# eager 모드는 프로그램 시작 때 실행을 해야하며, 재시작 까지는 eager 모드가 유지됩니다.\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad() function return the derivative of loss with respect to weight and bias. Then passing this to optimizer.apply_gradients() completes the process of apply gradient descent.\n",
    "\n",
    "Apart from the above changes, almost everything remains same. Here are some important points before using eager execution:\n",
    "\n",
    "· Data must be initialized using tensorflow.data.Dataset. (Use can use other ways but they are not recommended)\n",
    "\n",
    "· Eager execution runs by default on CPU, to use GPU include below code:\n",
    "with tf.device(‘/gpu:0’)\n",
    "\n",
    "· Eager execution doesn’t create Tensor Graph, to build graph just remove the tf.enable_eager_execution().\n",
    "\n",
    "· Eager execution is good for R&D but for production you should use graph execution.\n",
    "\n",
    "· You can save the model generated with eager execution and later load this model in graph or eager execution.\n",
    "\n",
    "Here is the link to the working code on Google Colaboratory which you can try and play with.\n",
    "\n",
    "Update: Definition of grad() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연산의 자유도\n",
    "\n",
    " - Eager 모드를 통해, 조금 더 직관적으로 연산이 가능하다. 2 * 2 매트릭스에서 연산을 하는 예제들을 확인하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[16 19]\n",
      " [36 43]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[17 20]\n",
      " [37 44]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.matmul([[1, 2],\n",
    "               [3, 4]],\n",
    "              [[4, 5],\n",
    "               [6, 7]])\n",
    "\n",
    "y = tf.add(x, 1)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 19],\n",
       "       [36, 43]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy() #넘파이 형태로 변환도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#반대로 tf.constant 활용하여 넘파이에서 텐서플로우 구조로도 변화가 가능합니다.\n",
    "\n",
    "np_val = np.array(10., dtype=np.float32)\n",
    "tf_val = tf.constant(np_val)\n",
    "\n",
    "print(np_val)\n",
    "print(tf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[16 19]\n",
      " [36 43]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[17 20]\n",
      " [37 44]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0.19489515 0.2844069  0.16411018]\n",
      " [0.8483808  0.06130743 0.88258183]\n",
      " [0.6253625  0.27058995 0.49275684]\n",
      " [0.8884959  0.9641547  0.14701462]\n",
      " [0.8995452  0.36912692 0.48457158]], shape=(5, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Multiply 2 * 2 matrix\n",
    "# Multiply two 2x2 matrices\n",
    "x = tf.matmul([[1, 2],\n",
    "               [3, 4]],\n",
    "              [[4, 5],\n",
    "               [6, 7]])\n",
    "\n",
    "# Add one to each element\n",
    "# (tf.add supports broadcasting)\n",
    "#broadcasting: 퍼트리다의 의미, 크기가 작은 행렬을 크기가 큰 행렬로 맞추어 주는 기능, 축소는 데이터 손실 때문에 불가\n",
    "#아래와 같은 경우는 1이 [[1 1], [1 1]], shape(2,2)로 변환\n",
    "#차원이 다를 경우, expand_dims() 함수에 맞게 넣어야 함. \n",
    "#설명 참조: http://excelsior-cjh.tistory.com/entry/Matrix-Broadcasting-%ED%96%89%EB%A0%AC%EC%9D%98-%EB%B8%8C%EB%A1%9C%EB%93%9C%EC%BA%90%EC%8A%A4%ED%8C%85\n",
    "\n",
    "y = tf.add(x, 1)\n",
    "\n",
    "# Create a random random 5x3 matrix\n",
    "z = tf.random_uniform([5, 3])\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 3.], shape=(1,), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# Tensor object를 활용하여, tf.add, tf.subtract, tf.multiply로 활용도 가능하다.\n",
    "x = (tf.ones([1], dtype=tf.float32) + 1) * 2 - 1\n",
    "print(x)\n",
    "\n",
    "# Numpy를 활용하여 값을 변환하기\n",
    "import numpy as np\n",
    "\n",
    "x = tf.add(1, 1)                     # tf.Tensor with a value of 2\n",
    "y = tf.add(np.array(1), np.array(1)) # tf.Tensor with a value of 2\n",
    "z = np.multiply(x, y)                # numpy.int64 with a value of \n",
    "\n",
    "#반대로 tf.constant 활용하여 numpy -> tf로 변화\n",
    "np_x = np.array(2., dtype=np.float32)\n",
    "x = tf.constant(np_x)\n",
    "\n",
    "py_y = 3.\n",
    "y = tf.constant(py_y)\n",
    "\n",
    "z = x + y + 1\n",
    "print(z)\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "\n",
    "- 데이터셋 만들기 (tf.data.Dataset 활용)-> Datasets API를 활용하면 성능이나 복잡한 파이프라인을 손쉽게 활용이 가능함.\n",
    "\n",
    "- eager모드가 켜진 상태에서 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1: 데이터셋 만들기\n",
    "#Dataset.from_tensors와 Dataset.from_tensor_slices를 사용하면 데이터 생성\n",
    "\n",
    "ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a CSV file\n",
    "import tempfile\n",
    "_, filename = tempfile.mkstemp()\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(\"\"\"Line 1\n",
    "Line 2\n",
    "Line 3\n",
    "  \"\"\")\n",
    "ds_file = tf.data.TextLineDataset(filename)\n",
    "\n",
    "#Step2: Apply transformations\n",
    "#map, batch, shuffle등을 활용하여 dataset records를 활용. tf.data.Dataset을 좀 더 자세히..\n",
    "\n",
    "ds_tensors = ds_tensors.map(tf.square).shuffle(2).batch(2)\n",
    "ds_file = ds_file.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element of ds_tensors\n",
      "tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor([16  9], shape=(2,), dtype=int32)\n",
      "tf.Tensor([25 36], shape=(2,), dtype=int32)\n",
      "\n",
      "Elements in ds_file:\n",
      "tf.Tensor([b'Line 1' b'Line 2'], shape=(2,), dtype=string)\n",
      "tf.Tensor([b'Line 3' b'  '], shape=(2,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#Step3: Iterate\n",
    "#Use tfe.Iterator on the Dataset obj. \n",
    "print('Element of ds_tensors')\n",
    "for x in tfe.Iterator(ds_tensors):\n",
    "    print(x)\n",
    "    \n",
    "print('\\nElements in ds_file:')\n",
    "for x in tfe.Iterator(ds_file):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Print Tensorflow Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'x:0' shape=() dtype=float32, numpy=45.0>\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'x:0' shape=() dtype=float32, numpy=0.0>\n",
      "0.0\n",
      "<bound method ResourceVariable.assign of <tf.Variable 'x:0' shape=() dtype=float32, numpy=42.0>>\n",
      "tf.Tensor(45.0, shape=(), dtype=float32)\n",
      "tf.Tensor(48.0, shape=(), dtype=float32)\n",
      "tf.Tensor([  45.   90.  180.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.get_variable(name=\"x\", shape=[], dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "print(x)\n",
    "\n",
    "#Tensorflow의 변수는 tensor로 나타냄으로, read_value()를 통해 현재 값으로 접근이 가능함.\n",
    "#Tensorflow의 함수는 자동으로 초기화\n",
    "\n",
    "#numpy를 통한 변환\n",
    "print(x.read_value().numpy())\n",
    "\n",
    "#Tensorflow변수의 값을 변경하기\n",
    "x.assign(42)\n",
    "print(x.assign)\n",
    "\n",
    "x.assign_add(3)\n",
    "print(x.read_value())\n",
    "\n",
    "#텐서 변수를 자유자제로 활용해보기\n",
    "print(x + 3)\n",
    "\n",
    "print(x * [1, 2, 4]) #자동으로 broadcasting해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Difference (Gradients)\n",
    "\n",
    " - tfe.gradients_function(f): 입력 f에 대해 arg 미분값을 돌려준다.\n",
    " - tfe.value_and_gradients_function(f): tfe.gradients_function(f)과 비슷하지만, 함수가 들어오면 이전 f값과 미분값에 대해 돌려줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "[<tf.Tensor: id=19321, shape=(), dtype=float32, numpy=6.0>]\n",
      "2nd grad: (<tf.Tensor: id=19330, shape=(), dtype=float32, numpy=6.0>, [<tf.Tensor: id=19337, shape=(), dtype=float32, numpy=2.0>])\n",
      "[<tf.Tensor: id=19362, shape=(), dtype=float32, numpy=2.0>]\n",
      "[<tf.Tensor: id=19369, shape=(), dtype=float32, numpy=1.0>]\n",
      "[<tf.Tensor: id=19378, shape=(), dtype=float32, numpy=-1.0>]\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return tf.multiply(x, x)\n",
    "assert 9 == square(3.).numpy()\n",
    "\n",
    "grad = tfe.gradients_function(square)\n",
    "assert 6 == grad(3.)[0].numpy()\n",
    "\n",
    "print(square(3.))\n",
    "print(grad(3.)) #x^2 -> 2x -> 6\n",
    "\n",
    "#2차 gradients_function\n",
    "grad2 = tfe.value_and_gradients_function(lambda x: grad(x)[0])\n",
    "#assert 2 == grad2(3.)[0].numpy()\n",
    "print(\"2nd grad: {}\".format(grad2(3.)))\n",
    "\n",
    "#3차 grad.\n",
    "grad3 = tfe.gradients_function(lambda x: grad2(x)[0])\n",
    "#assert 0 == grad3(3.)[0].numpy()\n",
    "print(grad3(3.))\n",
    "\n",
    "#absolute value\n",
    "def abs(x):\n",
    "    return x if x > 0. else -x\n",
    "\n",
    "grad = tfe.gradients_function(abs)\n",
    "\n",
    "print(grad(2.0))  # [1.]\n",
    "print(grad(-2.0)) # [-1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.072067\n",
      "Loss at step 0: 66.364388\n",
      "Loss at step 20: 30.010012\n",
      "Loss at step 40: 13.850556\n",
      "Loss at step 60: 6.667610\n",
      "Loss at step 80: 3.474723\n",
      "Loss at step 100: 2.055431\n",
      "Loss at step 120: 1.424524\n",
      "Loss at step 140: 1.144068\n",
      "Loss at step 160: 1.019394\n",
      "Loss at step 180: 0.963970\n",
      "Final loss: 0.940147\n",
      "W, B = 3.046539, 2.140244\n"
     ]
    }
   ],
   "source": [
    "#실제 linear regression을 통하여 활용해보자\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "    return input * weight + bias\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# A loss function: Mean-squared error\n",
    "def loss(weight, bias):\n",
    "    error = prediction(training_inputs, weight, bias) - training_outputs\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Function that returns the the derivative of loss with respect to\n",
    "# weight and bias\n",
    "grad = tfe.gradients_function(loss)\n",
    "\n",
    "# Train for 200 steps (starting from some random choice for W and B, on the same\n",
    "# batch of data).\n",
    "W = 5.\n",
    "B = 10.\n",
    "learning_rate = 0.01\n",
    "print(\"Initial loss: %f\" % loss(W, B).numpy())\n",
    "for i in range(200):\n",
    "    (dW, dB) = grad(W, B)\n",
    "    W -= dW * learning_rate\n",
    "    B -= dB * learning_rate\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step %d: %f\" % (i, loss(W, B).numpy()))\n",
    "print(\"Final loss: %f\" % loss(W, B).numpy())\n",
    "print(\"W, B = %f, %f\" % (W.numpy(), B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Grad\n",
    "\n",
    "Custom Gradient 제작하기. \n",
    "\n",
    "주로 cross entropy나 log likelyhood에 쓰이는 예제로 log(1 + e^x) 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=9890, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=9901, shape=(), dtype=float32, numpy=nan>]\n"
     ]
    }
   ],
   "source": [
    "def log1pexp(x):\n",
    "    return tf.log(1 + tf.exp(x))\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "print(grad_log1pexp(0.)) # [0.5]\n",
    "\n",
    "print(grad_log1pexp(100.)) # x = 100, nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=19202, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=19214, shape=(), dtype=float32, numpy=1.0>]\n"
     ]
    }
   ],
   "source": [
    "@tfe.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.log(1 + e), grad\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# Gradient at x = 0 works as before.\n",
    "print(grad_log1pexp(0.))\n",
    "# [0.5]\n",
    "# And now gradient computation at x=100 works as well.\n",
    "print(grad_log1pexp(100.))\n",
    "# [1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training models\n",
    "\n",
    " - eager에서는 특별히 수정해야 하지 않는 한, tf.layers와 같은 모듈을 사용을 권장함\n",
    " - Optimizer와 layer를 간단하게 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable & Optimization\n",
    "\n",
    " - tfe.Variable: 변형가능한 Tensor값을 저장하는 객체로써, 학습이나 미분을 할때 값에 대한 access가 가능함. 모델의 파라메터들이 python변수에 저장 될 수 있다는 이야기임\n",
    " - tfe.gradients_function(f): 쉬운 미분을 지원하지만, 모든 파라메터들이 f와 연동이 되어있어야 하여, 학습시 큰 파라메터에 대한 대응이 힘듬\n",
    " - tfe.implicit_gradients: 비슷한 기능이지만 몇가지 특수 기능이 있음?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.978195\n",
      "Loss at step 0: 67.215584\n",
      "Loss at step 20: 30.237247\n",
      "Loss at step 40: 13.908978\n",
      "Loss at step 60: 6.698107\n",
      "Loss at step 80: 3.513218\n",
      "Loss at step 100: 2.106319\n",
      "Loss at step 120: 1.484737\n",
      "Loss at step 140: 1.210071\n",
      "Loss at step 160: 1.088680\n",
      "Loss at step 180: 1.035020\n",
      "Loss at step 200: 1.011296\n",
      "Final loss: 1.011296\n",
      "W, B = 3.03347, 2.11813\n"
     ]
    }
   ],
   "source": [
    "#실제 linear regression을 통하여 활용해보자\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.W = tfe.Variable(5., name='weight')\n",
    "        self.B = tfe.Variable(10., name='bias')\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return inputs * self.W + self.B\n",
    "\n",
    "\n",
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "    error = model.predict(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# Define:\n",
    "# 1. A model\n",
    "# 2. Derivatives of a loss function with respect to model parameters\n",
    "# 3. A strategy for updating the variables based on the derivatives\n",
    "model = Model()\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# The training loop\n",
    "print(\"Initial loss: %f\" %\n",
    "      loss(model, training_inputs, training_outputs).numpy())\n",
    "for i in range(201):\n",
    "    optimizer.apply_gradients(grad(model, training_inputs, training_outputs))\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step %d: %f\" %\n",
    "              (i, loss(model, training_inputs, training_outputs).numpy()))\n",
    "print(\"Final loss: %f\" % loss(model, training_inputs, training_outputs).numpy())\n",
    "print(\"W, B = %s, %s\" % (model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models (개선필요)\n",
    "\n",
    "MNIST 2 Layer모델을 간단하게 Class로 만드는 예제\n",
    "\n",
    "tfe.Network: 기본적으로 layer의 Container역할을 하여, 다른 NW객체에 임비디드 되어 NW객체가 된다.\n",
    "\n",
    "추가로, inspection, saving, & restoring에 도움을 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tfe.Network):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.layer1 = self.track_layer(tf.layers.Dense(units=10))\n",
    "        self.layer2 = self.track_layer(tf.layers.Dense(units=10))\n",
    "    def call(self, input):\n",
    "        \"\"\"모델 실행\"\"\"\n",
    "        result = self.layer1(input)\n",
    "        result = self.layer2(result)\n",
    "        return result\n",
    "    \n",
    "#placeholder나 session에 대한 기능이 없고, input을 pass되면 자동으로 세팅 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 784)\n",
      "tf.Tensor([[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]], shape=(1, 1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터셋 생성하기\n",
    "model = MNISTModel()\n",
    "batch = tf.zeros([1, 1, 784])\n",
    "print(batch.shape)\n",
    "result = model(batch)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-13bd9ea72257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicit_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#학습을 위한 loss func, grad, 그리고 업데이트\n",
    "\n",
    "#1. loss func\n",
    "def loss_function(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_)\n",
    "\n",
    "#2. training loop\n",
    "#implicit_gradients(): 모든 TF 값에 대한 미분을 계산한다.\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate =0.001)\n",
    "for (x, y) in tfe.Iterator(dataset):\n",
    "    grads = tfe.implicit_gradients(loss_function)(model, x, y)\n",
    "    optimizer.apply_gradients(grads)\n",
    "\n",
    "#GPU사용하기\n",
    "#optimizer.min을 통해서, 짧게 작성하였지만, apply_gradients()기능을 써도 가능\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    for (x, y) in tfe.Iterator(dataset):\n",
    "        optimizer.minimize(lambda: loss_function(model, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Errors with Instant Feedback\n",
    "\n",
    "- runtime 이슈들을 디버깅하고, 코드에 관련된 사항을 interactive하게 볼 수 있음\n",
    "- 간단하게 4 vector와 2개의 tf.slice()를 활용하여, 정상 케이스와 에러 케이스를 분류 하겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tf.constant([10.0, 20.0, 30.0, 40.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 20.  30.  40.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#정상 케이스: 인덱스 값이 내부에 적용이 되기 때문\n",
    "print(tf.slice(vector, [1], [3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught error: Expected size[0] in [0, 3], but got 4 [Op:Slice]\n"
     ]
    }
   ],
   "source": [
    "#비정상 케이스: 인덱스 값이 데이터에서 넘가기 때문에\n",
    "try:\n",
    "    print(tf.slice(vector, [1], [4]))\n",
    "except tf.OpError as e:\n",
    "    print(\"Caught error: %s\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 사용하기\n",
    "\n",
    " - eager에서는 GPU가 자동으로 실행되지 않기 때문에, 지정해주고 사용해야 한다.\n",
    " - .gpu()를 활용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available.\n"
     ]
    }
   ],
   "source": [
    "# The example code from here on will work only if your notebook\n",
    "# is running on a machine with a functional CUDA GPU. The following\n",
    "# line checks that.\n",
    "is_gpu_available = tfe.num_gpus() > 0\n",
    "\n",
    "# Create some Tensors\n",
    "SIZE = 1000\n",
    "cpu_tensor = tf.random_normal([SIZE, SIZE])\n",
    "\n",
    "if is_gpu_available:\n",
    "    gpu_tensor = cpu_tensor.gpu()\n",
    "else:\n",
    "    print(\"GPU not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to conduct matmul on CPU:\n",
      "CPU times: user 162 ms, sys: 6.41 ms, total: 169 ms\n",
      "Wall time: 28.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=13, shape=(1000, 1000), dtype=float32, numpy=\n",
       "array([[-13.64949799, -31.02673149,  15.75299549, ..., -19.90308762,\n",
       "        -80.68657684,  19.10900116],\n",
       "       [-31.3809948 ,  17.13792038, -23.38959122, ...,   8.40496445,\n",
       "         -7.32779026, -19.07193184],\n",
       "       [-19.6288166 , -14.6174736 ,  -7.89929771, ...,  15.44713306,\n",
       "         15.5273037 ,  21.50070572],\n",
       "       ..., \n",
       "       [ 12.01198769,  -7.41007423,  26.26524734, ...,   4.17672539,\n",
       "          0.81272852, -18.06852722],\n",
       "       [  1.57177544,   5.99784756,  25.80046272, ...,  18.59385681,\n",
       "         22.02895355,  11.35874557],\n",
       "       [-13.62286949, -78.86634064,  -0.60588956, ...,  24.94280243,\n",
       "        -44.15736008, -60.31943512]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time a CPU-based matrix multiplication\n",
    "\n",
    "print(\"Time to conduct matmul on CPU:\")\n",
    "%time tf.matmul(cpu_tensor, cpu_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time GPU-based matrix multiplications.\n",
    "\n",
    "if is_gpu_available:\n",
    "    # First use of the GPU will be slow:\n",
    "    print(\"Time to conduct first matmul on GPU:\")\n",
    "    %time tf.matmul(gpu_tensor, gpu_tensor)\n",
    "    print()\n",
    "\n",
    "    # Subsequent uses are much faster:\n",
    "    print(\"Time to conduct second matmul on GPU:\")\n",
    "    %time tf.matmul(gpu_tensor, gpu_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to conduct CPU matmul:\n",
      "CPU times: user 161 ms, sys: 927 µs, total: 162 ms\n",
      "Wall time: 26 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second timing demo for GPUs, after it has been used once:\n",
    "\n",
    "cpu_tensor = tf.random_normal([SIZE, SIZE])\n",
    "print(\"Time to conduct CPU matmul:\")\n",
    "%time tf.matmul(cpu_tensor, cpu_tensor)\n",
    "print()\n",
    "\n",
    "if is_gpu_available:\n",
    "    gpu_tensor = cpu_tensor.gpu()\n",
    "    print(\"Time to conduct GPU matmul:\")\n",
    "    %time tf.matmul(gpu_tensor, gpu_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Took 0.26638007164001465 seconds to multiply a (1000, 1000) matrix by itself 10 times\n"
     ]
    }
   ],
   "source": [
    "#1000*1000 매트릭스 계산하기\n",
    "\n",
    "import time\n",
    "\n",
    "def measure(x):\n",
    "    # The very first time a GPU is used by TensorFlow, it is initialized.\n",
    "    # So exclude the first run from timing.\n",
    "    tf.matmul(x, x)\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(10):\n",
    "        tf.matmul(x, x)\n",
    "    end = time.time()\n",
    "\n",
    "    return \"Took %s seconds to multiply a %s matrix by itself 10 times\" % (end - start, x.shape)\n",
    "\n",
    "# Run on CPU:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    print(\"CPU: %s\" % measure(tf.random_normal([1000, 1000])))\n",
    "\n",
    "# If a GPU is available, run on GPU:\n",
    "if tfe.num_gpus() > 0:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        print(\"GPU: %s\" % measure(tf.random_normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error copying tensor to device: GPU:0. GPU:0 unknown device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-76e20da2e730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_gpu0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mgpu\u001b[0;34m(self, gpu_index)\u001b[0m\n\u001b[1;32m    756\u001b[0m       \u001b[0;32mas\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \"\"\"\n\u001b[0;32m--> 758\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GPU:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_copy\u001b[0;34m(self, ctx, device_name)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0mnew_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error copying tensor to device: GPU:0. GPU:0 unknown device."
     ]
    }
   ],
   "source": [
    "#Tensor 객체를 활용하여, 각 디바이스 별로 활용하는 것이 가능하다\n",
    "\n",
    "x = tf.random_normal([10, 10])\n",
    "\n",
    "x_gpu0 = x.gpu()\n",
    "x_cpu = x.cpu()\n",
    "\n",
    "_ = tf.matmul(x_cpu, x_cpu)  # Runs on CPU\n",
    "_ = tf.matmul(x_gpu0, x_gpu0)  # Runs on GPU:0\n",
    "\n",
    "if tfe.num_gpus() > 1:\n",
    "    x_gpu1 = x.gpu(1)\n",
    "    _ = tf.matmul(x_gpu1, x_gpu1)  # Runs on GPU:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Eagar with Graphs\n",
    "\n",
    "- Eagar 자체는 개발하고 디버깅 할 때 좋은 기능을 가지지만, Tensorflow graph형식이 분산 학습, 성능 최적화, 상용개발에 더 적합\n",
    "- 현재 모델을 graph형태로 변경하기 위해서는, eager를 disable하고 실행하면 됨\n",
    "- 관련 예제 코드: [MNIST with Eager](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/mnist)\n",
    "- 위의 예제 코드는 checkpoints를 저장하고 불러올 수 있기 때문에, 상용에도 적합하다.\n",
    "\n",
    "## 현재 활용하고 있는 코드의 변화\n",
    " \n",
    "- 현재 사용하고 있는 데이터에서, 형태를 tf.data로 변경하는것을 추천 드립니다.\n",
    "  - [참고링크 1](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html)\n",
    "  - [참고링크 2](https://www.tensorflow.org/programmers_guide/datasets)\n",
    "- tf.layer.Conv2D()와 같은 Object-oriented 기능 활용을 추천 (Explict storage for variables\n",
    "- 대부분의 모델이 eagar로 활용이 가능하지만, dynamic 모델에 대한 control flow같은 경우는 추가로 검토가 필요\n",
    "- tfe.enable_eagar_execution()을 활용하면, 끌수가 없기 때문에 Python 세션을 재시작 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 향후 To-Do\n",
    "\n",
    "- 11/29 블로그를 지속적으로 백업하면서, 추가 개선 방향을 고민 할 것\n",
    " 1. 데이터 전처리\n",
    " 2. 제 3의 모델 (Conv Seq2Seq) 테스트\n",
    " 3. 기존의 심심이 데이터에서 Generation 한 것이 의미가 있어 보임 -> 블로그도 Generation 후, 실제 의마기 있는 문장에 대한 데이터 추출로 보강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
