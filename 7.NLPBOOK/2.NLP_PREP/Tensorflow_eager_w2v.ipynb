{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.1.1 Eager Execution\n",
    " Eager (이거)는 기존의 세션과 그래프 형식의 작동 방식에서 명령형 (imperative) 스타일로 텐서플로우를 활용할수 있는 기능이다.  기존에 텐서플로우 방식이 세션과 그래프 완성 후 디버깅을 해야하는 불편함이 있었다면, 이거 모드를 통해서 좀 더 직관적으로 접근 가능하다.  따라서 연구자들이 조금 더 유동적으로 텐서플로우 프레임워크를 활용하거나 새롭게 텐서플로우를 입문하시는 분들에게 적합한 기능이다.\n",
    "텐서플로우 2018 Dev Summit에서 내용을 발췌하면 다음과 같다.\n",
    " - Eager 모드는 점점 contrib에서 정식으로 변하고 있고 session을 사용하지 않고 코드를 실행 가능하다.\n",
    " - 그래디언트 계산을 손쉽게 수정이 가능하다.\n",
    " - Dataset (텐서플로우 자료 구조 중 하나)를 통해서 sqlite database를 읽을 수 있다 (아직 실험적)\n",
    " - TensorRT를 지원함으로써 모델을 최적화 할 수 있다.\n",
    "이 장에서는 Eager 모드에 대한 간략한 소개, 기능 설명 및 예제를 제공하겠다. 우선 eager 모드를 실행하는 방법은 아래와 같다.\n",
    "import tensorflow as tf\n",
    "\n",
    "# eager 모드를 실행하는 방법\n",
    "# eager 모드는 프로그램 시작 때 실행을 해야하며, 재시작 까지는 eager 모드가 유지됩니다.\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "\n",
    "출력 결과:\n",
    "TensorFlow version: 1.8.0\n",
    "Eager execution: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "직관적인 연산\n",
    "  Eager 모드를 통해, 기존의 세션 방식과 비교하여 조금 더 직관적으로 연산이 가능하다. 2 * 2 매트릭스에서 연산을 하는 예를 들어 보자.\n",
    "x = tf.matmul([[1, 2],\n",
    "               [3, 4]],\n",
    "              [[4, 5],\n",
    "               [6, 7]])\n",
    "\n",
    "y = tf.add(x, 1)\n",
    "\n",
    "print(x)\n",
    "### 매트릭스 연산 ###\n",
    "tf.Tensor(\n",
    "[[16 19]\n",
    " [36 43]], shape=(2, 2), dtype=int32)\n",
    "print(y)\n",
    "### 매트릭스 덧샘 ###\n",
    "tf.Tensor(\n",
    "[[17 20]\n",
    " [37 44]], shape=(2, 2), dtype=int32)\n",
    "\n",
    "### 텐서플로우 자료형에서 넘파이 자료형태로 변환\n",
    "x.numpy()\n",
    "\n",
    "array([[16, 19],\n",
    "       [36, 43]], dtype=int32)\n",
    " \n",
    "x.numpy() #넘파이 형태로 변환도 가능합니다.\n",
    "\n",
    "\n",
    "#반대로 tf.constant 활용하여 넘파이에서 텐서플로우 구조로도 변화가 가능합니다.\n",
    "\n",
    "np_val = np.array(10., dtype=np.float32)\n",
    "tf_val = tf.constant(np_val) #tf.constant를 사용하여 넘파이 데이터 변환\n",
    "\n",
    "print(np_val)\n",
    "print(tf_val)\n",
    "10.0\n",
    "tf.Tensor(10.0, shape=(), dtype=float32)\n",
    "\n",
    "## Define and Print Tensorflow Variables\n",
    "\n",
    "x = tf.get_variable(name=\"x\", shape=[], dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "print(x)\n",
    "\n",
    "#Tensorflow의 변수는 tensor로 나타냄으로, read_value()를 통해 현재 값으로 접근이 가능함.\n",
    "#Tensorflow의 함수는 자동으로 초기화\n",
    "<tf.Variable 'x:0' shape=() dtype=float32, numpy=0.0>\n",
    "In [33]:\n",
    "#numpy를 통한 변환\n",
    "print(x.read_value().numpy())\n",
    "\n",
    "#Tensorflow변수의 값을 변경하기\n",
    "x.assign(42)\n",
    "print(x.assign)\n",
    "\n",
    "x.assign_add(3) #x 값에 더하기 적용\n",
    "print(x.read_value())\n",
    "\n",
    "print(x + 3) #텐서 변수를 자유자제로 활용해보기\n",
    "\n",
    "print(x * [1, 2, 4]) #자동으로 broadcasting도 가능함\n",
    "45.0\n",
    "<bound method ResourceVariable.assign of <tf.Variable 'x:0' shape=() dtype=float32, numpy=42.0>>\n",
    "tf.Tensor(45.0, shape=(), dtype=float32)\n",
    "tf.Tensor(48.0, shape=(), dtype=float32)\n",
    "tf.Tensor([ 45.  90. 180.], shape=(3,), dtype=float32)\n",
    "## Automatic Difference (Gradients)\n",
    "\n",
    " - tfe.gradients_function(f): 입력 f에 대해 arg 미분값을 돌려준다.\n",
    " - tfe.value_and_gradients_function(f): tfe.gradients_function(f)과 비슷하지만, 함수가 들어오면 이전 f값과 미분값에 대해 값을 출력한다.\n",
    "def square(x):\n",
    "    return tf.multiply(x, x)\n",
    "assert 9 == square(3.).numpy()\n",
    "\n",
    "grad = tfe.gradients_function(square)\n",
    "assert 6 == grad(3.)[0].numpy()\n",
    "\n",
    "print(square(3.))\n",
    "print(grad(3.)) #x^2 -> 2x -> 6\n",
    "tf.Tensor(9.0, shape=(), dtype=float32)\n",
    "[<tf.Tensor: id=14844, shape=(), dtype=float32, numpy=6.0>]\n",
    "#2차 gradients_function\n",
    "grad2 = tfe.value_and_gradients_function(lambda x: grad(x)[0])\n",
    "#assert 2 == grad2(3.)[0].numpy()\n",
    "print(\"2nd grad: {}\".format(grad2(3.)))\n",
    "\n",
    "#3차 grad.\n",
    "grad3 = tfe.gradients_function(lambda x: grad2(x)[0])\n",
    "#assert 0 == grad3(3.)[0].numpy()\n",
    "print(grad3(3.))\n",
    "\n",
    "#absolute value\n",
    "def abs(x):\n",
    "    return x if x > 0. else -x\n",
    "\n",
    "grad = tfe.gradients_function(abs)\n",
    "\n",
    "print(grad(2.0))  # [1.]\n",
    "print(grad(-2.0)) # [-1.]\n",
    "2nd grad: (<tf.Tensor: id=14851, shape=(), dtype=float32, numpy=6.0>, [<tf.Tensor: id=14856, shape=(), dtype=float32, numpy=2.0>])\n",
    "[<tf.Tensor: id=14875, shape=(), dtype=float32, numpy=2.0>]\n",
    "[<tf.Tensor: id=72, shape=(), dtype=float32, numpy=1.0>]\n",
    "[<tf.Tensor: id=14887, shape=(), dtype=float32, numpy=-1.0>]\n",
    "\n",
    "#실제 linear regression을 통하여 활용해보자\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "    return input * weight + bias\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# A loss function: Mean-squared error\n",
    "def loss(weight, bias):\n",
    "    error = prediction(training_inputs, weight, bias) - training_outputs\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Function that returns the the derivative of loss with respect to\n",
    "# weight and bias\n",
    "grad = tfe.gradients_function(loss)\n",
    "\n",
    "# Train for 200 steps (starting from some random choice for W and B, on the same\n",
    "# batch of data).\n",
    "W = 5.\n",
    "B = 10.\n",
    "learning_rate = 0.01\n",
    "print(\"Initial loss: %f\" % loss(W, B).numpy())\n",
    "for i in range(200):\n",
    "    (dW, dB) = grad(W, B)\n",
    "    W -= dW * learning_rate\n",
    "    B -= dB * learning_rate\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step %d: %f\" % (i, loss(W, B).numpy()))\n",
    "print(\"Final loss: %f\" % loss(W, B).numpy())\n",
    "print(\"W, B = %f, %f\" % (W.numpy(), B.numpy()))\n",
    "Initial loss: 69.151985\n",
    "Loss at step 0: 66.454216\n",
    "Loss at step 20: 30.185934\n",
    "Loss at step 40: 14.019278\n",
    "Loss at step 60: 6.812951\n",
    "Loss at step 80: 3.600699\n",
    "Loss at step 100: 2.168821\n",
    "Loss at step 120: 1.530553\n",
    "Loss at step 140: 1.246040\n",
    "Loss at step 160: 1.119215\n",
    "Loss at step 180: 1.062682\n",
    "Final loss: 1.038317\n",
    "W, B = 3.034794, 2.132014\n",
    "\n",
    "## Building and training models\n",
    "\n",
    " - eager에서는 특별히 수정해야 하지 않는 한, tf.layers와 같은 모듈을 사용을 권장함\n",
    " - Optimizer와 layer를 간단하게 정리\n",
    "## Variable & Optimization\n",
    "\n",
    " - tfe.Variable: 변형가능한 Tensor값을 저장하는 객체로써, 학습이나 미분을 할때 값에 대한 access가 가능함. 모델의 파라메터들이 python변수에 저장 될 수 있다는 이야기임\n",
    " - tfe.gradients_function(f): 쉬운 미분을 지원하지만, 모든 파라메터들이 f와 연동이 되어있어야 하여, 학습시 큰 파라메터에 대한 대응이 힘듬\n",
    " - tfe.implicit_gradients: 비슷한 기능이지만 몇가지 특수 기능이 있음?\n",
    "\n",
    "#실제 linear regression을 통하여 활용해보자\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.W = tfe.Variable(5., name='weight')\n",
    "        self.B = tfe.Variable(10., name='bias')\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return inputs * self.W + self.B\n",
    "\n",
    "\n",
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "    error = model.predict(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# Define:\n",
    "# 1. A model\n",
    "# 2. Derivatives of a loss function with respect to model parameters\n",
    "# 3. A strategy for updating the variables based on the derivatives\n",
    "model = Model()\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# The training loop\n",
    "print(\"Initial loss: %f\" %\n",
    "      loss(model, training_inputs, training_outputs).numpy())\n",
    "for i in range(201):\n",
    "    optimizer.apply_gradients(grad(model, training_inputs, training_outputs))\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step %d: %f\" %\n",
    "              (i, loss(model, training_inputs, training_outputs).numpy()))\n",
    "print(\"Final loss: %f\" % loss(model, training_inputs, training_outputs).numpy())\n",
    "print(\"W, B = %s, %s\" % (model.W.numpy(), model.B.numpy()))\n",
    "\n",
    "Initial loss: 67.940033\n",
    "Loss at step 0: 65.334366\n",
    "Loss at step 20: 30.092083\n",
    "Loss at step 40: 14.164181\n",
    "Loss at step 60: 6.957934\n",
    "Loss at step 80: 3.694184\n",
    "Loss at step 100: 2.214460\n",
    "Loss at step 120: 1.542879\n",
    "Loss at step 140: 1.237761\n",
    "Loss at step 160: 1.098992\n",
    "Loss at step 180: 1.035816\n",
    "Loss at step 200: 1.007025\n",
    "Final loss: 1.007025\n",
    "W, B = 3.0174496, 2.1403313\n",
    " \n",
    "Eager Gradients\n",
    "대부분의 TensorFlow 사용자는 자동 차별화에 관심이 있습니다.\n",
    "각 호출 중에 다른 작업이 발생할 수 있으므로 모든 포워드 작업을 테이프에 기록한 다음 그래디언트를 계산할 때 역방향으로 재생합니다.\n",
    "그라디언트를 계산 한 후에 테이프를 버립니다.\n",
    "autograd 패키지에 익숙하다면 API가 매우 유사합니다. \n",
    "def square(x):\n",
    "    return tf.multiply(x, x)\n",
    "​grad = tfe.gradients_function(square)\n",
    "​print(square(3.))    # [9.]\n",
    "print(grad(3.))      # [6.]        \n",
    " \n",
    "gradients_function 은 인자로 파이썬 함수 square()를 사용하고 그 입력에 대해 square()의 편도를 계산하는 파이썬 호출 가능 객체를 반환합니다. 따라서 square ()의 미분을 3.0으로 얻으려면 grad (3.0)을 호출합니다.\n",
    "이것은 6입니다.\n",
    "동일한 gradients_function 호출을 사용하여 square의 2 차 미분을 얻을 수 있습니다.\n",
    "gradgrad = tfe.gradients_function(lambda x: grad(x)[0])\n",
    "​print(gradgrad(3.))  # [2.]\n",
    "​def abs(x):\n",
    "     return x if x > 0. else -x\n",
    "​grad = tfe.gradients_function(abs)\n",
    "​print(grad(2.0))  # [1.]\n",
    "print(grad(-2.0)) # [-1.]\n",
    "​x = tfe.Variable(2.0)\n",
    "def loss(y):\n",
    "    return (y - x ** 2) ** 2\n",
    "​grad = tfe.implicit_gradients(loss)\n",
    "​print(loss(7.))  # tf.Tensor(9., shape=(), dtype=float32)\n",
    "print(grad(7.))  # [(<tf.Tensor: -24.0, shape=(), dtype=float32>,\n",
    "                    <tf.Variable 'Variable:0' shape=()               \n",
    "                     dtype=float32, numpy=2.0>)]\n",
    " \n",
    "why grad(7.)이 -24가 나오는지 아시나요?  (edit)\n",
    "eager 실행이 활성화되지 않은 경우에도 그라디언트  API가 작동합니다.\n",
    "tfe.gradients_function()\n",
    "tfe.value_and_gradients_function()\n",
    "tfe.implicit_gradients()\n",
    "tfe.implicit_value_and_gradients()\n",
    "​\n",
    "이 부분에 대해서 좀 더 보충 설명 하겠습니다.  (edit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
