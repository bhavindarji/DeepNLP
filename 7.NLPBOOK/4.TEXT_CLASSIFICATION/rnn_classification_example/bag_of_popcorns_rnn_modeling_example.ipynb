{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinseongjin/tf110/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 파일 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR_PATH = './data/'\n",
    "INPUT_TRAIN_DATA_FILE_NAME = 'input.npy'\n",
    "LABEL_TRAIN_DATA_FILE_NAME = 'label.npy'\n",
    "DATA_CONFIGS_FILE_NAME = 'data_configs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vocab', 'vocab_size'])\n"
     ]
    }
   ],
   "source": [
    "input_data = np.load(open(FILE_DIR_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'rb'))\n",
    "label_data = np.load(open(FILE_DIR_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'rb'))\n",
    "prepro_configs = None\n",
    "\n",
    "with open(FILE_DIR_PATH + DATA_CONFIGS_FILE_NAME, 'r') as f:\n",
    "    prepro_configs = json.load(f)\n",
    "    print(prepro_configs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447\n",
    "\n",
    "input_train, input_test, label_train, label_test = train_test_split(input_data, label_data, \n",
    "                                                                    test_size=TEST_SPLIT, random_state=RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "def mapping_fn(X, Y):\n",
    "    input, label = {'text': X}, Y\n",
    "    return input, label\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_test, label_test))\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './models/rnn_classifier'\n",
    "vocab_size = prepro_configs['vocab_size']\n",
    "embedding_size = 100\n",
    "hidden_state_dim = 150\n",
    "hidden_layer_dim = 150\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "                    features['text'],\n",
    "                    vocab_size,\n",
    "                    embedding_size)\n",
    "\n",
    "    training = False\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        training = True\n",
    "    \n",
    "    input_layer = tf.layers.dropout(inputs=input_layer,\n",
    "                                   rate=0.2,\n",
    "                                   training=training)\n",
    "    \n",
    "    rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [hidden_state_dim, hidden_state_dim]]\n",
    "    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                       inputs=input_layer,\n",
    "                                       dtype=tf.float32)\n",
    "    \n",
    "    hidden = tf.layers.dense(inputs=outputs[:,-1,:], units=hidden_layer_dim, activation=tf.nn.tanh)\n",
    "\n",
    "    hidden = tf.layers.dropout(inputs=hidden, rate=0.2, training=training)\n",
    "    logits = tf.layers.dense(inputs=hidden, units=2)\n",
    "    \n",
    "#     logits = tf.squeeze(logits)\n",
    "#     predicts = tf.nn.sigmoid(logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # TFServing을 하기 위한 준비를 위해서는 output에 대한 export 정의를 해줄 필요가 있다.\n",
    "        # 여기선 영화 평점 예측점수와 그 해당 영화 리뷰 텍스트에 대한 시멘틱 정보를 출력하고자 한다.\n",
    "        softmax_logits = tf.nn.softmax(logits)\n",
    "        \n",
    "        predictions = {'sentiment': tf.argmax(softmax_logits, 1),\n",
    "                       'confidence': tf.reduce_max(softmax_logits, axis=1)}\n",
    "        \n",
    "        # TFServing을 활용하여 예측값을 얻고자 한다면 예측 output에 대해서 프로토콜을 맞춰 export할 \n",
    "        # 준비를 해야 한다. 이를 estimator.export.PredictOutput을 통해 해결 할 수 있다.\n",
    "        export_outputs = {\n",
    "          'prediction': tf.estimator.export.PredictOutput(predictions),\n",
    "        }\n",
    "        \n",
    "        # TFServing 파일로 만들기 위해서는 export_outputs 파라메터를 반드시 지정해주어야 한다.\n",
    "        # 이 파라메터가 없이 serving 모델에 대한 저장을 하려하면 이 파라메터에 대한 부재가 있음을\n",
    "        # 에러를 통해 알게 될 것이다.\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  predictions=predictions,\n",
    "                  export_outputs=export_outputs)\n",
    "    \n",
    "    one_hot_labels = tf.one_hot(labels, 2)\n",
    "    loss = tf.losses.softmax_cross_entropy(one_hot_labels, logits)\n",
    "#     loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.argmax(logits, 1))\n",
    "        eval_metric_ops = {'acc': accuracy}\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "              mode=mode,\n",
    "              train_op=train_op,\n",
    "              loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'checkpoint/train_model4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0c0171e7b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f0c982b2598>) includes params argument, but params are not passed to Estimator.\n"
     ]
    }
   ],
   "source": [
    "est = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                             model_dir='checkpoint/train_model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into checkpoint/train_model4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.69325185, step = 0\n",
      "INFO:tensorflow:global_step/sec: 17.7991\n",
      "INFO:tensorflow:loss = 0.6039523, step = 100 (5.619 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.7816\n",
      "INFO:tensorflow:loss = 0.48150882, step = 200 (5.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.8831\n",
      "INFO:tensorflow:loss = 0.43082845, step = 300 (5.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.8392\n",
      "INFO:tensorflow:loss = 0.47063375, step = 400 (5.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.8304\n",
      "INFO:tensorflow:loss = 0.69558847, step = 500 (5.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.8302\n",
      "INFO:tensorflow:loss = 0.37839848, step = 600 (5.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.8267\n",
      "INFO:tensorflow:loss = 0.5603098, step = 700 (5.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.8398\n",
      "INFO:tensorflow:loss = 0.33230418, step = 800 (5.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.7963\n",
      "INFO:tensorflow:loss = 0.30760372, step = 900 (5.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.7183\n",
      "INFO:tensorflow:loss = 0.5153922, step = 1000 (5.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.7728\n",
      "INFO:tensorflow:loss = 0.25013587, step = 1100 (5.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.8814\n",
      "INFO:tensorflow:loss = 0.3035248, step = 1200 (5.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.8756\n",
      "INFO:tensorflow:loss = 0.24910203, step = 1300 (5.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 18.8202\n",
      "INFO:tensorflow:loss = 0.45102254, step = 1400 (5.313 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1407 into checkpoint/train_model4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.23434663.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f0c0171e4e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.train(train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-25-11:55:17\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/train_model4/model.ckpt-1407\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-25-11:55:20\n",
      "INFO:tensorflow:Saving dict for global step 1407: acc = 0.8344, global_step = 1407, loss = 0.37572578\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1407: checkpoint/train_model4/model.ckpt-1407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.8344, 'loss': 0.37572578, 'global_step': 1407}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 캐글 평가 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR_PATH = './data/'\n",
    "INPUT_TEST_DATA_FILE_NAME = 'test_input.npy'\n",
    "\n",
    "test_input_data = np.load(open(FILE_DIR_PATH + INPUT_TEST_DATA_FILE_NAME, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"text\":test_input_data}, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/train_model4/model.ckpt-1407\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array([p['sentiment'] for p in est.predict(input_fn=\n",
    "predict_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/train_model4/model.ckpt-1407\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "tuple_predictions = np.array([(p['sentiment'], p['confidence']) for p in est.predict(input_fn=\n",
    "predict_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, neg = list(), list()\n",
    "\n",
    "for s, c in tuple_predictions:\n",
    "    if s == 0:\n",
    "        neg.append(c)\n",
    "    else:\n",
    "        pos.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(pos, range=[0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(neg, range=[0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PATH='~/.kaggle/competitions/word2vec-nlp-tutorial/'\n",
    "\n",
    "test = pd.read_csv(DEFAULT_PATH+\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"2913_8\"</td>\n",
       "      <td>\"...as valuable as King Tut's tomb! (OK, maybe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"4396_1\"</td>\n",
       "      <td>\"This has to be one of the biggest misfires ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"395_2\"</td>\n",
       "      <td>\"This is one of those movies I watched, and wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"10616_1\"</td>\n",
       "      <td>\"The worst movie i've seen in years (and i've ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"9074_9\"</td>\n",
       "      <td>\"Five medical students (Kevin Bacon, David Lab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l...\n",
       "5    \"2913_8\"  \"...as valuable as King Tut's tomb! (OK, maybe...\n",
       "6    \"4396_1\"  \"This has to be one of the biggest misfires ev...\n",
       "7     \"395_2\"  \"This is one of those movies I watched, and wo...\n",
       "8   \"10616_1\"  \"The worst movie i've seen in years (and i've ...\n",
       "9    \"9074_9\"  \"Five medical students (Kevin Bacon, David Lab..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":list(predictions)} )\n",
    "output.to_csv( \"rnn_predict13.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFServing 파일로 저장하여 서비스 활용하기 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_SEQ_LEN = 100\n",
    "\n",
    "# Serving을 제공하기 위한 입력 리시버 함수를 선언해주어야 한다.\n",
    "def serving_input_receiver_fn():\n",
    "    # estimator에 입력하고자 하는 데이터를 dict 객체로 정의한다.\n",
    "    receiver_tensor = {\n",
    "        # 외부로부터 입력을 받는 프로토콜은 스트링이다. ServingInputReceiver 메뉴얼에도 언급되었다시피\n",
    "        # 이 방식은 TFRecord 파일 형태로 시리얼화한 데이터 형태로 전송을 받는다. (이를 tf.example 방식이라고도 하는 것 같다)\n",
    "        'text': tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    }\n",
    "    \n",
    "    # 다음은 TFRecord 방식으로 받은 데이터를 모델에 넣을 수 있게 처리를 하는 dict 객체라 보면 된다.\n",
    "    # 쉽게 말하면 앞서 estimator를 진행하기 위해 data_fn의 과정을 작성해두는데 이 과정을 여기서 거친다 보면 된다.\n",
    "    features = {\n",
    "        key: tensor\n",
    "        for key, tensor in receiver_tensor.items()\n",
    "    }\n",
    "    # TFRecord로 시리얼화 된 데이터를 integer tensor로 변환하기 위해서는 string to int로 decode를 해줘야 한다.\n",
    "    fn = lambda query: tf.decode_raw(query, tf.int64)\n",
    "    features['text'] = tf.map_fn(fn, features['text'], dtype=tf.int64)\n",
    "    # 받은 데이터에 대해 모델입력에 맞는 shape로 구성을 해주기 위해 reshape을 해준다.\n",
    "    features['text'] = tf.reshape(features['text'], [-1, LIMIT_SEQ_LEN])\n",
    "\n",
    "    # 위에 정의한 받을 데이터에 대한 프로토콜과 모델에 입력할 데이터 전처리를 다음 함수 파라메터에 입력해준다.\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['prediction', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/train_model4/model.ckpt-1407\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./served_model/new_staging/temp-b'1537876528'/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "export_dir_base = './served_model/new_staging'\n",
    "\n",
    "# 서빙에 대한 입력 리시버함수와 저장 위치를 파라메터로 지정한다면, 서빙 pb파일로 저장하여 간단하게 모델을 활용할 수 있게 된다.\n",
    "# 실행을 하게 되면 저장된 파일의 위치를 텍스트 출력을 통해 얻게된다.\n",
    "path = est.export_savedmodel(export_dir_base, serving_input_receiver_fn) #,\n",
    "#                       strip_default_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./served_model/new_staging/1537876528/variables/variables\n"
     ]
    }
   ],
   "source": [
    "# 모델이 저장된 경로 위치를 파라메터로 지정하여 함수를 부르면 간단하게 예측 모델을 활용할 수 있다.\n",
    "# 이 모델 예측을 간단하게 함수로 받게된다.\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "    export_dir = path,\n",
    "    # 옵션이지만 실행되는 모델에 대한 이름을 명시하고자 한다면 다음의 파라메터를 활용한다.\n",
    "    signature_def_key=\"serving_default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 array:  [   89    77    45    23    92   772   242     7   125   347     2   199\n",
      "   122     3  7572   729     2  3581  8961    12 37400  2004     8    10\n",
      "    17     6     3   236   465  1579     6   364    29     4     1    89\n",
      "   999    77   123     5  1625    10  1132    18     6    24  2473    71\n",
      "    16    30     1   676     9   137   509    10   849  7066    71     9\n",
      "    88    20   121    85    77    50    28   270   485  4535  3530     9\n",
      "   121    10    15     3   189    24     6   341    34   563   323    18\n",
      "   371   224    41    29     4     1    89 18838 18839     9   438    24\n",
      "     6    23     1  1493]\n",
      "\n",
      "모델에 입력하기 위한 array:  b'Y\\x00\\x00\\x00\\x00\\x00\\x00\\x00M\\x00\\x00\\x00\\x00\\x00\\x00\\x00-\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\\\\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\xf2\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x00\\x00\\x00\\x00\\x00\\x00\\x00[\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc7\\x00\\x00\\x00\\x00\\x00\\x00\\x00z\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x94\\x1d\\x00\\x00\\x00\\x00\\x00\\x00\\xd9\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xfd\\r\\x00\\x00\\x00\\x00\\x00\\x00\\x01#\\x00\\x00\\x00\\x00\\x00\\x00\\x0c\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x92\\x00\\x00\\x00\\x00\\x00\\x00\\xd4\\x07\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x11\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xec\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xd1\\x01\\x00\\x00\\x00\\x00\\x00\\x00+\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00l\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x1d\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00Y\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xe7\\x03\\x00\\x00\\x00\\x00\\x00\\x00M\\x00\\x00\\x00\\x00\\x00\\x00\\x00{\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x00\\x00\\x00\\x00Y\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00l\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x12\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa9\\t\\x00\\x00\\x00\\x00\\x00\\x00G\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1e\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa4\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x89\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xfd\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00Q\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x9a\\x1b\\x00\\x00\\x00\\x00\\x00\\x00G\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x00\\x00\\x00\\x00X\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x14\\x00\\x00\\x00\\x00\\x00\\x00\\x00y\\x00\\x00\\x00\\x00\\x00\\x00\\x00U\\x00\\x00\\x00\\x00\\x00\\x00\\x00M\\x00\\x00\\x00\\x00\\x00\\x00\\x002\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1c\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0e\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xe5\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xb7\\x11\\x00\\x00\\x00\\x00\\x00\\x00\\xca\\r\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x00\\x00\\x00\\x00y\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xbd\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00U\\x01\\x00\\x00\\x00\\x00\\x00\\x00\"\\x00\\x00\\x00\\x00\\x00\\x00\\x003\\x02\\x00\\x00\\x00\\x00\\x00\\x00C\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x12\\x00\\x00\\x00\\x00\\x00\\x00\\x00s\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xe0\\x00\\x00\\x00\\x00\\x00\\x00\\x00)\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1d\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00Y\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x96I\\x00\\x00\\x00\\x00\\x00\\x00\\x97I\\x00\\x00\\x00\\x00\\x00\\x00\\t\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xb6\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xd5\\x05\\x00\\x00\\x00\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터는 array 형태로 입력을 할 수가 있다. 이 때 주의할 점은 데이터 타입을 \n",
    "# 반드시 입력 리시버 함수에서 정의한 데이터 타입과 일치시켜야한다.\n",
    "# 보통 int32, int64 데이터에 대한 정의가 맞지않아 막상 데이터를 모델에 입력했을 때 \n",
    "# 바이너리 데이터 길이의 불일치 문제로 이어져 에러가 발생할 수 있다.\n",
    "\n",
    "d = np.array(input_data[0], dtype=np.int64)\n",
    "print('입력 데이터 array: ', d)\n",
    "\n",
    "d = d.tostring()\n",
    "print('\\n모델에 입력하기 위한 array: ', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단하게 입력 파라메터에 대한 프로토콜만 맞춰 입력한다면 예츨 모델을 함수를 통해 얻을 수 있다.\n",
    "output = predictor_fn({'text': [d]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confidence': array([0.6073587], dtype=float32), 'sentiment': array([1])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 서비스 테스트를 위한 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2vec(s, vocab, limit_len=100, PAD='pad', UNK='unk'):\n",
    "    s_tok = s.split()\n",
    "    pad_count = limit_len - len(s_tok)\n",
    "    indecied_list = [vocab[t] if t in vocab else 0 for t in s_tok]\n",
    "    if pad_count > 0:\n",
    "        indecied_list = indecied_list + ([0] * pad_count)\n",
    "    elif pad_count < 0:\n",
    "        indecied_list = indecied_list[:limit_len]\n",
    "    \n",
    "    np_index = np.array(indecied_list, dtype=np.int64)\n",
    "    np_index = np_index.tostring()\n",
    "    \n",
    "    return indecied_list, np_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 229, 10, 17, 6, 36, 49, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confidence': array([0.9480432], dtype=float32), 'sentiment': array([1])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = prepro_configs['vocab']\n",
    "\n",
    "test_sent = 'i feel this movie is so good'\n",
    "arr, input_arr = str2vec(test_sent, vocab)\n",
    "print(arr)\n",
    "predictor_fn({'text': [input_arr]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text : this movie is so boring\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confidence': array([0.94222295], dtype=float32), 'sentiment': array([0])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = input('input text : ')\n",
    "arr, input_arr = str2vec(input_str, vocab)\n",
    "predictor_fn({'text': [input_arr]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
