{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from tensorflow.python.keras.datasets import imdb\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "#전처리 lib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "\n",
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_path='~/.kaggle/competitions/word2vec-nlp-tutorial/'\n",
    "train = pd.read_csv(default_path+\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "#매번 위와 같은 작업을 반복할 수 없으니 함수로 만들어봅니다.\n",
    "def review_to_words( raw_review ):\n",
    "    # 1. HTML 태그 지우기\n",
    "    review_text = BeautifulSoup(raw_review, \"html5lib\").get_text()\n",
    "    \n",
    "    # 2. 영어가 아닌 특수문자들을 공백(\" \")으로 바꾸기\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 3. 대문자들을 소문자로 바꾸고 공백단위로 텍스트들을 나누기\n",
    "    words = letters_only.lower().split()\n",
    "\n",
    "    # 4. 파이썬에서 리스트보다 set에서의 찾기가 더 빠르다고 합니다.\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # 5. 불용어가 아닌 것만 남기기\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 6. 불용어가 아닌 텍스트를 공백 단위로 나누어서 리턴\n",
    "    return( \" \".join( meaningful_words ))\n",
    "\n",
    "#함수를 만들었으니 본격적으로 사용해봅시다.\n",
    "#전체 데이터의 라인 수를 얻습니다.\n",
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "#보통 엄청 크고 아름다운 PC를 가졌다면 위처럼 실행하면 됩니다.\n",
    "#아니라면....컴퓨터가 일을 제대로 하고 있는 지 확인이 필요할 겁니다.\n",
    "print(\"Cleaning and parsing the training set movie reviews...\\n\")\n",
    "clean_train_reviews = []\n",
    "for i in range( 0, num_reviews ):\n",
    "    #1000라인마다 잘하고 있는 지 보고서를 제출하게 합니다.\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print (\"Review %d of %d\\n\" % ( i+1, num_reviews ))\n",
    "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (20000, 200)\n",
      "x_test shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "# tensorflow내의 내장된 keras기능을 사용하여, 문장을 전처리를 진행하고, 단어 vocab을 dictionary화 합니다.\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(clean_train_reviews)\n",
    "variables = t.texts_to_sequences(clean_train_reviews)\n",
    "labels = np.array(train['sentiment'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_variable, x_test_variable, y_train, y_test = train_test_split(\n",
    "            variables, labels, test_size=0.2, random_state=10)\n",
    "            \n",
    "\n",
    "vocab_size = 150000\n",
    "sentence_size = 200\n",
    "embedding_size = 50\n",
    "model_dir = './checkpoint/cnn_classifier'\n",
    "\n",
    "# vocabulary셋을 만들때 padding이나 start, unknown과 같은 토큰 index들을 설정을 해줘야 합니다.\n",
    "# 여기의 경우는 padding은 start, unknown(oov)을 각각 0, 1, 2로 indexing해두었습니다.\n",
    "pad_id = 0\n",
    "start_id = 1\n",
    "oov_id = 2\n",
    "# 첫 시작 vocab은 3번부터 시작하기 위해 index_offset을 두어 원래 'vocabulary 토큰순서 + index offset'을 하도록 합니다.\n",
    "# 예를 들어 'apple'이 첫 단어라고 하면 '0 + index_offset'한 위치가 vocabulary index 위치 입니다.\n",
    "index_offset = 2\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(x_train_variable, \n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "x_test = sequence.pad_sequences(x_test_variable, \n",
    "                                maxlen=sentence_size,\n",
    "                                truncating='post',\n",
    "                                padding='post', \n",
    "                                value=pad_id)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <PAD>\n",
      "1 <START>\n",
      "2 <OOV>\n",
      "3 movie\n",
      "4 film\n",
      "5 one\n",
      "6 like\n",
      "7 good\n",
      "8 time\n",
      "9 even\n",
      "stories look age korda happy lonely done back manages taking together fanshawe together knoxville dvd us well form jungle music enough would hilarious uwe dvd made less oscar aliens painful knoxville find testament find actual <OOV> tell look story title constant scary touching monkey korda fight c brosnan bad james giving redeeming much im context solid although wanted fashion begin nazi film korda type look feature makes women thought turned korda kazakos aymeric script <START> korda based time end first based maybe would idea working actual <OOV> whole dialogue car far kristel thrills carry local killing lordi tension mukhsin casting mysterious extreme close korda help much sequence korda halleck cartoons baloo killing lordi think journeymen turn get best frewer redeeming gave dyer dvd rick korda done surprised hilarious quite film korda industry kill circumstances least favor mexican local bad years right underlying abused sex acts egotist much local cinematography meets feature fantastic movie directed ending told least points astaire kelly greatest still likely used <START> get film korda movie ridiculous director could get french kind kids series admits giving ticket korda giullia gangster <START> since wanted fashion camera movie rich get show favourite younger aliens really due run believable godard really best get place wants pokemon ambitious best got home happened idea next music movie wiz supplying mr race\n"
     ]
    }
   ],
   "source": [
    "# word indexing을 끝내고 vocabulary set을 생성합니다.\n",
    "word_index = t.word_index\n",
    "word_inverted_index = {v + index_offset: k for k, v in word_index.items()}\n",
    "\n",
    "# 보통, 첫 index 위치에 padding과 start, unknown 토큰이 들어가도록 합니다.\n",
    "word_inverted_index[pad_id] = '<PAD>'\n",
    "word_inverted_index[start_id] = '<START>'\n",
    "word_inverted_index[oov_id] = '<OOV>'\n",
    "\n",
    "######################\n",
    "# Test Vocabulary set#\n",
    "######################\n",
    "for i in range(0, 10):\n",
    "    print(i, word_inverted_index[i])\n",
    "\n",
    "def index_to_text(indexes):\n",
    "    return ' '.join([word_inverted_index[i] for i in indexes])\n",
    "\n",
    "print(index_to_text(variables[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 각 문장의 길이를 계산합니다. 이 경우엔 전처리 후에 문장 길이 정보를 보존하기 위해 사용합니다.\n",
    "# 또한, 경우에 따라 네트워크에 padding정보를 포함하지 않기 위해 문장 길이정보를 필요로 할 때가 있습니다.\n",
    "# 이 경우엔 문장길이는 최대 200으로 제한해두고 있습니다.\n",
    "x_len_train = np.array([min(len(x), sentence_size) for x in x_train_variable])\n",
    "x_len_test = np.array([min(len(x), sentence_size) for x in x_test_variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_filename = 'bag_of_popcorns_indexing.tfrecords'\n",
    "\n",
    "writer = tf.python_io.TFRecordWriter(tfrecords_filename)\n",
    "\n",
    "# Let's collect the real images to later on compare\n",
    "# to the reconstructed ones\n",
    "original_tables = []\n",
    "\n",
    "for review, label, length in zip(x_train, y_train, x_len_train):\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'review': _bytes_feature(review.tostring()),\n",
    "        'label': _int64_feature(label),\n",
    "        'length': _int64_feature(length)}))\n",
    "    \n",
    "    writer.write(example.SerializeToString())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1595,  5219,   312,  2004,  6412,    30,     3,    10,  3055,\n",
       "        3718,   114,     2,   166,   209,  3034,    14,  5177,   237,\n",
       "        6958,  1788, 19372,    17,  5219,     4,    30,    16,     4,\n",
       "        2127,  7241,    33,    37,  1744,  2978,    43,   130,   433,\n",
       "        3624, 13636,    16,   352,   327,    23,  2027,    36,  1686,\n",
       "         141, 21432,    13,    59,     2,   222,    61,  4456,  1960,\n",
       "          50,   671,   186,   854,  1653,  4071,    20,  7679,   649,\n",
       "         743,   236,  8560,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
