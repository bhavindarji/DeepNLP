{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras.datasets import imdb #imdb dataset\n",
    "from tensorflow.python.keras._impl.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras._impl.keras import utils\n",
    "from tensorboard import summary as summary_lib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape (25000,)\n",
      "x_test shape (25000,)\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb\n",
    "# https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/LICENSE\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples\n",
    "\n",
    "vocab_size = 5000\n",
    "sentence_size = 200\n",
    "embedding_size = 50\n",
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "pad_id = 0\n",
    "start_id = 1\n",
    "oov_id = 2\n",
    "index_offset = 2\n",
    "\n",
    "(x_train_variable, y_train), (x_test_variable, y_test) = imdb.load_data(num_words=vocab_size,\n",
    "                                                      start_char=start_id,\n",
    "                                                      oov_char=oov_id,\n",
    "                                                      index_from=index_offset)\n",
    "\n",
    "print(\"x_train shape\", x_train_variable.shape)\n",
    "print(\"x_test shape\", x_test_variable.shape)\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train_variable, \n",
    "                                 maxlen=sentence_size,\n",
    "                                 truncating='post',\n",
    "                                 padding='post',\n",
    "                                 value=pad_id)\n",
    "x_test = sequence.pad_sequences(x_test_variable, \n",
    "                                maxlen=sentence_size,\n",
    "                                truncating='post',\n",
    "                                padding='post', \n",
    "                                value=pad_id)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "\n",
    "x_len_train = np.array([min(len(x), sentence_size) for x in x_train_variable])\n",
    "x_len_test = np.array([min(len(x), sentence_size) for x in x_test_variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <PAD>\n",
      "1 <START>\n",
      "2 <OOV>\n",
      "3 the\n",
      "4 and\n",
      "5 a\n",
      "6 of\n",
      "7 to\n",
      "8 is\n",
      "9 br\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <OOV> is an amazing actor and now the same being director <OOV> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <OOV> and would recommend it to everyone to watch and the fly <OOV> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <OOV> to the two little <OOV> that played the <OOV> of norman and paul they were just brilliant children are often left out of the <OOV> list i think because the stars that play them all grown up are such a big <OOV> for the whole film but these children are amazing and should be <OOV> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <OOV> with us all\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "word_inverted_index = {v + index_offset: k for k, v in word_index.items()}\n",
    "\n",
    "# The first indexes in the map are reserved to represent things other than tokens\n",
    "word_inverted_index[pad_id] = '<PAD>'\n",
    "word_inverted_index[start_id] = '<START>'\n",
    "word_inverted_index[oov_id] = '<OOV>'\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(i, word_inverted_index[i])\n",
    "    \n",
    "def index_to_text(indexes):\n",
    "    return ' '.join([word_inverted_index[i] for i in indexes])\n",
    "\n",
    "print(index_to_text(x_train_variable[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(input_sent, length, label):\n",
    "    features = {\"input_sent\": input_sent, \"len\": length}\n",
    "    return features, label\n",
    "\n",
    "#the former (repeat before shuffle) provides better performance, \n",
    "# while the latter (shuffle before repeat) provides stronger ordering guarantees.\n",
    "\n",
    "def train_input_fn():\n",
    "    with tf.device('/cpu:0'):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))\n",
    "        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(10000, NUM_EPOCHS))\n",
    "        dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=parser, batch_size=BATCH_SIZE))\n",
    "        dataset = dataset.apply(tf.contrib.data.prefetch_to_device(\"/cpu:0\")) #디바이스에 미리 1개씩 대기\n",
    "        # dataset = dataset.prefetch(1) # prefetch가 오래 걸리면 10으로 세팅 하는것도 방법임\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        return iterator.get_next()\n",
    "    \n",
    "def eval_input_fn():\n",
    "    with tf.device('/cpu:0'):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))        \n",
    "        dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=parser, batch_size=BATCH_SIZE))\n",
    "        dataset = dataset.apply(tf.contrib.data.prefetch_to_device(\"/cpu:0\")) #디바이스에 미리 1개씩 대기\n",
    "        # dataset = dataset.prefetch(1) # prefetch가 오래 걸리면 10으로 세팅 하는것도 방법임\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def layer_test(features, labels, mode, params):\n",
    "    \n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "                                                features['input_sent'],\n",
    "                                                vocab_size,\n",
    "                                                embedding_size,\n",
    "                                                initializer=params['embed_init']\n",
    "                                                )\n",
    "        \n",
    "    conv_layer = tf.layers.conv1d(inputs=input_layer,\n",
    "                                 filters=32, # Filters는 얼마나 다른 windows를 보유하냐? 를 의미합니다.\n",
    "                                 kernel_size=3, # Kernel_size는 슬라이딩 윈도우의 사이즈를 결정합니다.3grams\n",
    "                                 padding=\"same\",\n",
    "                                 activation=tf.nn.relu)\n",
    "    \n",
    "    ## filters = 100이고 kernel_size = 4이면, 길이가 4인 100개의 다른 필터를 생성합니다. 100개의 컨볼루션들을 생성합니다.\n",
    "    ## conv1d는 (배치사이즈, 길이, 채널)로 입력값을 받는데, 배치사이즈: 문장 숫자 | 길이: 각 문장의 단어의 개수 | 채널: 임베딩 출력 차원수\n",
    "    \n",
    "    pool_layer = tf.reduce_max(input_tensor=conv_layer, axis=1)\n",
    "    \n",
    "    dense_layer = tf.layers.dense(inputs=pool_layer, units=128, activation=tf.nn.relu)\n",
    "    \n",
    "    output_layer = tf.layers.dense(inputs=dense_layer, units=1)\n",
    "    \n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    def _train_op(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "    \n",
    "    return head.create_estimator_spec(\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            mode=mode,\n",
    "            logits=output_layer,\n",
    "            train_op_fn=_train_op)\n",
    "    \n",
    "#         global_step = tf.train.get_global_step(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#                         logits=output_layer, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './data/cnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11f116390>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "params = {'embed_init': tf.random_uniform_initializer(-1.0, 1.0)}\n",
    "\n",
    "cnn_classifier = tf.estimator.Estimator(model_fn = layer_test,\n",
    "                                       model_dir='./data/cnn',\n",
    "                                       params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./data/cnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.73147726, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into ./data/cnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5987458.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-25-14:51:24\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./data/cnn/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-25-14:51:27\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.66772, accuracy_baseline = 0.5, auc = 0.7398737, auc_precision_recall = 0.7246644, average_loss = 0.62808615, global_step = 100, label/mean = 0.5, loss = 0.6279778, precision = 0.632313, prediction/mean = 0.5289521, recall = 0.80152\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(classifier):\n",
    "    classifier.train(input_fn=train_input_fn, steps=100)\n",
    "    eval_result = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    \n",
    "# #     # Reset the graph to be able to reuse name scopes\n",
    "# #     tf.reset_default_graph() \n",
    "# #     # Add a PR summary in addition to the summaries that the classifier writes\n",
    "# #     pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool), num_thresholds=21)\n",
    "# #     with tf.Session() as sess:\n",
    "# #         writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "# #         writer.add_summary(sess.run(pr), global_step=0)\n",
    "# #         writer.close()\n",
    "\n",
    "train_and_evaluate(cnn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
